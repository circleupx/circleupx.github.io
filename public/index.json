[{"content":"Introduction It has been over a year since I last wrote about JSON:API, since then the team behind JSON:API has published version 1.1 of the JSON:API specification. I would like to continue my journey of documenting JOSN:API in .NET by introducing a really cool feature to my Chinook JSON:API project, filtering.\nThe first thing to know about filtering in JSON:API is that the spec itself is agnostic to any filtering strategies. Meaning it is up to you to define how filtering should be handled by your API. In my opinion, this has always been a drawback of the JSON:API spec, I believe in that it would have been a better choice for the spec if it had decided on a filtering strategy, but that is discussion for another day. While the spec does not favor any filtering strategy it does have some recommendations.\nThe spec recommends using the LHS bracket syntax to denote which resource the filtering should be applied to. For example, imagine you are dealing with a post resource and each post resource can expose a relationship to an author resource collection, that is to say, each post has a 1 to many relationship with with the authors resource.\nAs a client of the API if you may want to find out which post has an author with a firstName of \u0026ldquo;Dan\u0026rdquo;, typically what you see in most APIs is that you would first need to filter the authors resource to only those that have \u0026ldquo;Dan\u0026rdquo; as a first name, then once you have those resources, you can filters the posts resource to only post that have a matching author\u0026rsquo;s resource. This type of filtering is not ideal even though it is how many REST APIs out in the wild are implemented, this is the well-know problem of over-fetching and under-fetching, a selling point of GraphQL.\nWe can do better, by properly defining the relationships between our resources a client application should be able to make the following request to handle the scenario I just described.\n1 GET /posts?filter[post]=published eq true\u0026amp;filter[author]=firstName eq \u0026#39;Dan\u0026#39;\u0026amp;include=author HTTP/1.1 Note the usage of LSH brackets and ODATA syntax, we\u0026rsquo;ll talk about that later on this post. If the request above is valid, then it should in theory yield the following JSON:API response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 { \u0026#34;data\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;post\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;JSON:API paints my bikeshed!\u0026#34;, \u0026#34;published\u0026#34; : true }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;http://example.com/post/1\u0026#34; }, \u0026#34;relationships\u0026#34;: { \u0026#34;author\u0026#34;: { \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;http://example.com/post/1/relationships/author\u0026#34;, \u0026#34;related\u0026#34;: \u0026#34;http://example.com/post/1/author\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;author\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;9\u0026#34; } } } } ], \u0026#34;included\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;author\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;9\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;Dan\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Gebhardt\u0026#34;, \u0026#34;twitter\u0026#34;: \u0026#34;dgeb\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;http://example.com/author/9\u0026#34; } } ] } In a single request, the client has requested that the API should get all posts where the published field is true and to include the all the related authors, the request also states that out of that list of posts, the API should only return posts where the author is name Dan. Using a nested filtering allows a client of the API to overcome the over-fetching and under-fetching problems that many REST APIs have.\nSo, how can we implement this in feature in .NET? Let\u0026rsquo;s take a look.\nThe first step in implementing filtering will be to take the filter query parameter from the incoming HTTP request URL and tokenize them. You have the option to implement a custom tokenizer, see my Parsing in C# blog post, or use one of the many awesome parsing libraries that exit in .NET. Personally, I have always relied on SuperPower as it easily allows you define a tokenizer and parsers. Once we have a tokenizer and a parser, the next step is to build an Abstract Syntax Tree the generate runtime expression that can be then be given to an ORM system like EF Core or Dapper.\nBy the way, if you are interesting in learning more about parsers then you can enroll in Building a Parser from scratch by Dmitry Soshnikov.\nLet\u0026rsquo;s review the Chinook project I have been building for the last two years, as I mentioned before, it is a level 3 REST API that implements the JSON:API specification. The API exposes a number of resources, one of them, the invoices resource, has a one to one relationship to the customer resource, a single customer resource is represented with the following JSON payload.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:5001/invoices/98/customer\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:5001/invoices/98\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;Luís\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Gonçalves\u0026#34;, \u0026#34;company\u0026#34;: \u0026#34;Embraer - Empresa Brasileira de Aeronáutica S.A.\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;Av. Brigadeiro Faria Lima, 2170\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;São José dos Campos\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;SP\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;Brazil\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12227-000\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;+55 (12) 3923-5555\u0026#34;, \u0026#34;fax\u0026#34;: \u0026#34;+55 (12) 3923-5566\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;luisg@embraer.com.br\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:5001/customers/1\u0026#34; } } } The Problem Back to the Chinook project, imagine the following scenario, a client of the Chinook API would like to query the API to find all invoices where the billing country is Germany and the customer, which is a related resource of invoices has a first name equal to Leonie, as it stands, a client of the Chinook API today could do it with the following actions.\nNavigating to the invoice resource collection, pulling all records in-memory, about 412 in total. Loop through the invoice collection to find any invoice where the billing country is Germany. For any invoice where the billing country is Germany, navigate to the related customer. Check if the customer\u0026rsquo;s first name is Leonie. What I have just described is totally inefficient, as I mention before this is a well-known problem, Over Fetching And Under Fetching and it often refer as the main reason to use GraphQL.\nIn order to provide better usability and developer experience I will introduce resource filtering to the Chinook project, the invoice resource collection will now allow a client to specify a filtering criteria. This filtering criteria can work against the invoice resource collection as well as any related resources but for the purposes of this demo I will only add filtering support for the related customer resource.\nAdding filtering to an API means that you will need to come up with a filtering language, I am going to stick to OData, why? Simple, it is well known standard that many developers are already familiar with and comes with well defined operators, you can of course come up with your own if desired or follow the ones recommended by the JSON:API community\nOData Operators Let\u0026rsquo;s take a look at the operators offered by OData as defined in Built-in Filter Operations\nThe filter operators offered by OData are as follows.\nComparison Operators eq Equal Address/City eq \u0026lsquo;Redmond\u0026rsquo; ne Not equal Address/City ne \u0026lsquo;London\u0026rsquo; gt Greater than Price gt 20 ge Greater than or equal Price ge 10 lt Less than Price lt 20 le Less than or equal Price le 100 has Has flags Style has Sales.Color\u0026rsquo;Yellow' in Is a member of Address/City in (\u0026lsquo;Redmond\u0026rsquo;, \u0026lsquo;London\u0026rsquo;) Logical Operators and Logical and Price le 200 and Price gt 3.5 or Logical or Price le 3.5 or Price gt 200 not Logical negation not endswith(Description,\u0026lsquo;milk\u0026rsquo;) Arithmetic Operators add Addition Price add 5 gt 10 sub Subtraction Price sub 5 gt 10 mul Multiplication Price mul 2 gt 2000 div Division Price div 2 gt 4 divby Decimal Division Price divby 2 gt 3.5 mod Modulo Price mod 2 eq 0 Grouping Operators ( ) Precedence grouping (Price sub 5) gt 10 OData also offers Built-in Query Functions but those functions are beyond the scope of this blog post.\nIn order for the a client of the Chinook API to find an invoice where the billing country is Germany and the related customer\u0026rsquo;s first name is Leonie the client would have to use the following query string.\n1 GET /invoices?filter[invoices]=billingCountry eq \u0026#39;Germany\u0026#39;\u0026amp;filter[customers]=firstName eq \u0026#39;Leonie\u0026#39;\u0026amp;include=invoices HTTP/1.1 Let\u0026rsquo;s break down the request above, /invoices is the resource collection we are dealing with, then we have the query string parameters, the first one is the JSON:API keyword filter and it is used by the client to inform the server that the resource should be filtered. The first LHS bracket with invoices is used to inform the server that filtering will be done against the resource invoices, remember JSON:API has compound documents, a single JSON:API document can have multiple resources. The next part, =billingCountry eq \u0026lsquo;Germany\u0026rsquo; is used to inform the server that the filter should be against the property billingCountry on the source invoices, with OData syntax, eq as mention above being used as the equals operators, since invoice billingCountry is a string type we use single quotes to specify the value. The second filter is against the customers resource, in this filter the client tells the server to use the firstName property to and to filter the customer resource where that name is Leonie.\nMeaning the request above, when executed correctly by the server, will only return invoices where the customer\u0026rsquo;s first name is Leonie and the invoice was in Germany.\nA quick aside, JSON is case sensitive, you can encounter APIs in different case formats, i.e. snake vs camel, therefore, when doing filtering, take into consideration the casing being used on the field you plan to filter one.\nNow that I know what the HTTP request will look like let\u0026rsquo;s switch to the Chinook API and the code required to support filtering.\nFirst thing I am going to do is to install Superpower on the Chinook Core Project by running the following dotnet command.\n1 dotnet add package Superpower --version 3.0.0 Now that Superpower is installed I will modify the existing UriKeyWords class that was added in JSON:API - Pagination Links.\n1 2 3 4 5 6 7 public static class UriKeyWords { public static string PageNumber = $\u0026#34;page[{number}]\u0026#34;; public static string PageSize = $\u0026#34;page[{size}]\u0026#34;; public const string size = nameof(size); public const string number = nameof(number); } Here is what the class looks like after adding the OData operators.\n1 2 3 4 5 6 7 8 9 10 11 12 public static class UriKeyWords { public static string PageNumber = $\u0026#34;page[{number}]\u0026#34;; public static string PageSize = $\u0026#34;page[{size}]\u0026#34;; public const string size = nameof(size); public const string number = nameof(number); public const string And = \u0026#34;and\u0026#34;; public const string Or = \u0026#34;or\u0026#34;; public const string Eq = \u0026#34;eq\u0026#34;; public const string Nq = \u0026#34;nq\u0026#34;; } Note the addition of some of the OData operators we previously defined, I kept the list small since we don\u0026rsquo;t need to support all OData operators at the moment. Next, for each operator type that you plan to support you will need to add a corresponding token, represent as an enum value. For example, for the \u0026ldquo;eq\u0026rdquo; operator types which falls under the equality operator you would have the following enum.\n1 2 3 4 5 6 [Flags] public enum ODataTokens { [Token(Category = \u0026#34;OData Equality Operator\u0026#34;, Example = \u0026#34;Eq, Nq, Gt Lt\u0026#34;, Description = \u0026#34;Equality operators supported by API.\u0026#34;)] EqualityOperator = 0, } Adding the rest of the support operators yields the following ODataTokens class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [Flags] public enum ODataTokens { [Token(Category = \u0026#34;Empty value\u0026#34;, Example = \u0026#34;Empty string or null\u0026#34;, Description = \u0026#34;Represents no value.\u0026#34;)] None = 0, [Token(Category = \u0026#34;String value\u0026#34;, Example = \u0026#34;\u0026#39;Hello World\u0026#39;\u0026#34;, Description = \u0026#34;Parsed string value.\u0026#34;)] StringValue = 1, [Token(Category = \u0026#34;Boolean Expression\u0026#34;, Example = \u0026#34;True\u0026#34;)] BooleanValue = 2, [Token(Category = \u0026#34;Object Field\u0026#34;, Example = \u0026#34;payment.CreditCardNumber\u0026#34;, Description = \u0026#34;An object property\u0026#34;)] ObjectField = 4, [Token(Category = \u0026#34;Logical Operator\u0026#34;, Example = \u0026#34;And, Or\u0026#34;)] LogicalOperator = 8, [Token(Category = \u0026#34;Equality Operator\u0026#34;, Example = \u0026#34;Eq, Nq, Gt Lt\u0026#34;, Description = \u0026#34;All equality operators supported\u0026#34;)] EqualityOperator = 16, [Token(Category = \u0026#34;Number value\u0026#34;, Example = \u0026#34;1\u0026#34;, Description = \u0026#34;Parsed number value.\u0026#34;)] IntegerValue = 32 } Up next, we need to build our OData parsers with Superpower, let\u0026rsquo;s gets started with the most basic, an OData string. As shown before, an OData request may look like the following HTTP GET request.\n1 GET /posts?filter[post]=published eq true\u0026amp;filter[author]=firstName eq \u0026#39;Dan\u0026#39;\u0026amp;include=author HTTP/1.1 OData Parser Note the usage of \u0026rsquo; to denote when a string starts and ends, this is what the we need to parse and Superpower allows us to build a parser with LINQ, the following LINQ expression can be used by Superpower to parse an OData string.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public static class ODataParser { private static char SingleQuote =\u0026gt; \u0026#39;\\\u0026#39;\u0026#39;; private static readonly char[] InvalidStringCharacters = {\u0026#39;*\u0026#39;, \u0026#39;=\u0026#39;, \u0026#39;+\u0026#39;, \u0026#39;$\u0026#39;, \u0026#39;#\u0026#39;, \u0026#39;~\u0026#39;, \u0026#39;`\u0026#39;, \u0026#39;\\\u0026#39;\u0026#39;, \u0026#39; \u0026#39;}; public static TextParser\u0026lt;string\u0026gt; ODataString =\u0026gt; from start in Character.EqualTo(SingleQuote) from chars in Content from end in Character.EqualTo(SingleQuote) select chars; public static TextParser\u0026lt;char\u0026gt; Characters =\u0026gt; from c in Character.ExceptIn(InvalidStringCharacters) select c; public static TextParser\u0026lt;string\u0026gt; Content =\u0026gt; from content in Characters.Many() select new string(content); } Here you start to see the beauty of parser combinators like Superpower, you can compose parsers out of other parsers. In the code above, the ODataString parser is composed by the Content parser and the Characters parser.\nNow that I have the tokens and parser I need to create the Tokenizer. For the tokenizer we can use the built in TokenizerBuilder provided by Superpower, along with their helpers functions, like Match, Ignore, Build and so on. Below is how the Tokenizer looks so far, note the usage of the key words defined in ODataParser and the Enum ODataTokens.\nTokenizer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public static class Tokenizers { public static TokenizerBuilder\u0026lt;ODataTokens\u0026gt; GetODataTokenizer() { var tokenizerBuilder = new TokenizerBuilder\u0026lt;ODataTokens\u0026gt;() .Ignore(Span.WhiteSpace) .Match(ODataParser.ODataEqualOperator, ODataTokens.ComparisonOperator) .Match(ODataParser.ODataNotEqualOperator, ODataTokens.ComparisonOperator) .Match(ODataParser.ODataGreaterThanOperator, ODataTokens.ComparisonOperator) .Match(ODataParser.ODataLessThanOperator, ODataTokens.ComparisonOperator) .Match(ODataParser.ODataGreaterThanOrEqualOperator, ODataTokens.ComparisonOperator) .Match(ODataParser.ODataLessThanOrEqualOperator, ODataTokens.ComparisonOperator) .Match(ODataParser.ContainsOperator, ODataTokens.ComparisonOperator) .Match(ODataParser.ODataString, ODataTokens.StringValue) .Match(ODataParser.ODataLogicalAnd, ODataTokens.LogicalOperator) .Match(ODataParser.ODataLogicalOr, ODataTokens.LogicalOperator) .Match(ODataParser.ODataFalse, ODataTokens.BooleanValue) .Match(ODataParser.ODataTrue, ODataTokens.BooleanValue) .Match(Numerics.IntegerInt32, ODataTokens.IntegerValue); return tokenizerBuilder; } } Something worth mentioning here, Superpower has a good support for error handling, when an error is encountered Superpower will report that error and you can set how to handle it, I won\u0026rsquo;t cover error handling of the tokenizer on this blog post as I feel it is beyond the scope of this post, but essentially what you would do is take the error reported by Superpower and convert it into an Errors Document.\nNext, I will add a Tokenizer Builder, the builder will take expose method that take in a generic type, TObject, inspect the properties in this generic type and tokenize them along with the OData tokenizer above. Here is the tokenizer builder.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 internal class TokenizerBuilder { private static readonly BindingFlags Flags = BindingFlags.Public | BindingFlags.NonPublic | BindingFlags.Instance; /// \u0026lt;summary\u0026gt;Tokenize an array of properties using reflection. Ignores casing\u0026lt;/summary\u0026gt; /// \u0026lt;typeparam name=\u0026#34;TObject\u0026#34;\u0026gt;Object from which the properties will be extracted.\u0026lt;/typeparam\u0026gt; /// \u0026lt;returns\u0026gt;Tokenize list of \u0026lt;see cref=\u0026#34;TObject\u0026#34;/\u0026gt; properties \u0026lt;/returns\u0026gt; internal static Tokenizer\u0026lt;ODataTokens\u0026gt; TokenizeObjectProperties\u0026lt;TObject\u0026gt;() { var tokenizer = Tokenizers.GetODataTokenizer(); var mappedPropertiesToToken = MapPropertiesToToken\u0026lt;TObject\u0026gt;(tokenizer); return mappedPropertiesToToken.Build(); } private static TokenizerBuilder\u0026lt;ODataTokens\u0026gt; MapPropertiesToToken\u0026lt;TObject\u0026gt;(TokenizerBuilder\u0026lt;ODataTokens\u0026gt; tokenizer) { IEnumerable\u0026lt;string\u0026gt; properties = typeof(TObject) .GetProperties(Flags) .Select(propertyInfo =\u0026gt; propertyInfo.Name) .Distinct(); foreach (var propertyName in properties) { tokenizer.Match(Span.EqualToIgnoreCase(propertyName), ODataTokens.ObjectField); } return tokenizer; } } With the code above, when I call the method TokenizeObjectProperties and use Invoices as the generic type, all the properties that currently exist in the Invoices class will get tokenized.\nTaking an incoming query string, tokenize it, parsing it, building an AST, then finally getting an expression to pass to an ORM like EF Core or Dapper takes a look of work, whenever I have face this before I have relied on the builder pattern, and since what I am building is rather complex structure, the builder pattern will allow me delegate different parts of the process to small parts of the builder.\nBuilding a DSL The one thing I want to ensure is that on top of the builder pattern there needs to be a DSL to ensure proper usage of the builder pattern. For example, I want to expose the following DSL.\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class DSL { public static string BuildDSL() { var displayUrl = _httpContextAccessor.HttpContext.GetRequestUri(); var resource = ResourceQueryBuilder .NewResourceQuerySpecification(displayUrl) .StartFilter() .AddFilter\u0026lt;Invoice\u0026gt;() .EndFilter() .BuildSpecification(); } } In the DSL above, ordering of operations can be controlled, I want to shield the any consumers from incorrectly tokenizing, parsing, building the AST and finally the expression, a DSL works create for this as the fluent style API controls the way the final result can be assembled. How to create a fluent interface in C# by Scott Lilly can probably explain the benefits of this type of code better than I can, if you can, I do recommend reading it.\nThe DSL I built has support for pagination, ordering, but for now we are only interesting in filtering. As you can see there is a AddFilter method that accepts a generic type, the Invoice class in this case, this filtering method is responsible for a good chuck of the work I have described so far. Let\u0026rsquo;s take a deeper look at this method.\n1 2 3 4 5 6 7 8 9 public sealed class ResourceQueryBuilder { public IAddResourceFiltering AddFilter\u0026lt;TResource\u0026gt;() { var tokenizer = TokenizerBuilder.TokenizeObjectProperties\u0026lt;TResource\u0026gt;(); var parsedQueryStringDictionary = _queryParameterService.ParseFilterQueryString(); var resourceType = typeof(TResource); } } The first part of the AddFilter method in the ResourceQueryBuilder class is to call the tokenizer as shown in the code above to tokenize the properties on the resource, the next step is to call ParseFilterQueryString in the QueryParameterService class, this class was introduce in JSON:API - Pagination Links as UriQueryParametersReader I just consolidated the reader and writer into a service. The new method, ParseFilterQueryString, was added in to the class due to a limitation in JsonApiFramework, which powers the Chinook project, JsonApiFramework was never built to handle multiple query filters.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 public class QueryParameterSerive { public Dictionary\u0026lt;string, string\u0026gt; ParseFilterQueryString() { var filterDictionary = new Dictionary\u0026lt;string, string\u0026gt;(); if (string.IsNullOrWhiteSpace(_requestUri.Query)) { return filterDictionary; } var requestUriQuery = _requestUri.Query; var startIndex = requestUriQuery[(requestUriQuery.IndexOf(UriKeyWords.QuerySeparator) + 1)..]; var startIndexCopy = startIndex; foreach (var filter in startIndexCopy.Split(UriKeyWords.Ampersand)) { var array = filter.Split(UriKeyWords.Equal); var filterKey = Uri.UnescapeDataString(array[0]); var filterValue = Uri.UnescapeDataString(array[1]); var filterKeySplit = filterKey.Split(UriKeyWords.LeftBracket, UriKeyWords.RightBracket); if (filterKeySplit.Length == 3 \u0026amp;\u0026amp; filterKeySplit[2] == string.Empty) { var filterQueryString = filterKeySplit[0]; var filterQueryStringValue = filterKeySplit[1]; if (string.Compare(filterQueryString, UriKeyWords.Filter, StringComparison.OrdinalIgnoreCase) == 0) { filterDictionary[filterQueryStringValue] = filterValue; } } } return filterDictionary; } } With the code above the API can now handle multiple query parameters being used in the URL query string. Back to the ResourceQueryBuilder class, the next step is to get the resource type that was used to call AddFilter. This is essential as we need to know which service model we need to use when building our expressions.\n1 2 3 4 5 6 7 8 9 10 11 12 public sealed class ResourceQueryBuilder { public IAddResourceFiltering AddFilter\u0026lt;TResource\u0026gt;() { var resource = parsedQueryStringDictionary.FirstOrDefault(x =\u0026gt; x.Key.Camelize() == resourceType.Name.Camelize().Pluralize()); if (resource.Key is null) { keyValuePairs.Add(resourceType.Name, PredicateBuilder.New\u0026lt;TResource\u0026gt;()); return this; } } } The next part of the AddFilter method is looking at the parsed query strings values return from ParseFilterQueryString to see if the current resource matches up with a filter used in the URL query string. If no matched then we add a default expression to a global dictionary that is keeping track of all the filters being applied.\nPredicateBuilder Notice the usage of the PredicateBuilder class, this is a helper class that allows us to work with expressions, the new method creates a starting expression that evaluates to true, essentially it creates the following code.\n1 public Expression\u0026lt;Func\u0026lt;TResource, bool\u0026gt;\u0026gt; FilterExpression { get; } = entity =\u0026gt; true; As mentioned in JSON:API - Pagination Links, the code above is great because the Linq Provide will see this code and do nothing, in other words, this is a good default value to have when the client doesn\u0026rsquo;t specify any filters.\nSpeaking of the PredicateBuilder, this is a well-known class that has been used over the year by many developers working with C# expression, most notably, this is what powers the LinqKit Project.\nThe final part of the AddFilter method to loop through each node that was tokenized, then to use the visitor patter to visit each node grabbing the value and composing an expression, here is the code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public sealed class ResourceQueryBuilder { public IAddResourceFiltering AddFilter\u0026lt;TResource\u0026gt;() { var result = tokenizer.TryTokenize(resource.Value); var tokenList = result.Value; var visitor = new ExpressionTreeVisitor\u0026lt;TResource\u0026gt;(); foreach (var token in tokenList) { switch (token.Kind) { case ODataTokens.None: break; case ODataTokens.StringValue: VisitConstantNode(token, visitor); break; case ODataTokens.BooleanValue: VisitConstantNode(token, visitor); break; case ODataTokens.ObjectField: VisitObjectField(token, visitor, resourceType); break; case ODataTokens.LogicalOperator: VisitLogicalNode(token, visitor); break; case ODataTokens.ComparisonOperator: VisitComparisonNode(token, visitor); break; case ODataTokens.IntegerValue: VisitConstantNode(token, visitor); break; } } var filter = visitor.GetFilterExpression(); keyValuePairs.Add(resourceType.Name, filter); return this; } } In the code above we call the TryTokenize method in the tokenizer to tokenize the query parameter string, if successful, we loop through the list then begin to visit each known using the Visitor Pattern. After each node is visited an expression factory is responsible for putting together a complete C# expression which is returned by the method GetFilterExpression.\nNow that we have an expression that can be given to EF Core all that I need to do is modify the InvoiceResource class and the GetInvoiceResourceCollectionHandler to accept the output of the ResourceQueryBuilder which is a specification.\nHere is the structure of the specification that is generated by the ResourceQueryBuilder class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class ResoureQuerySpecification { private Dictionary\u0026lt;string, dynamic\u0026gt; FilterDictionary { get; } = new Dictionary\u0026lt;string, dynamic\u0026gt;(); private List\u0026lt;dynamic\u0026gt; Includes { get; } = new List\u0026lt;dynamic\u0026gt;(); private dynamic OrderBy { get; set; } private dynamic OrderByDescending { get; set; } private dynamic GroupBy { get; set; } public dynamic GetFilter\u0026lt;T\u0026gt;(string key) { var hasValue = FilterDictionary.TryGetValue(key, out dynamic filter); if (hasValue) { return filter; } else { return PredicateBuilder.New\u0026lt;T\u0026gt;(); } } public void AddFilter(string key, dynamic value) { FilterDictionary.TryAdd(key, value); } } And here is the code required to add filtering support to the Invoice resource.\n1 2 3 4 5 6 7 8 var displayUrl = _httpContextAccessor.HttpContext.GetRequestUri(); var resource = ResourceQueryBuilder .NewResourceQuerySpecification(displayUrl) .StartFilter() .AddFilter\u0026lt;Invoice\u0026gt;() .AddFilter\u0026lt;Customer\u0026gt;() .EndFilter() .BuildSpecification(); As I said, a DSL is super useful here, ideally, this is the only code developer of the API would use to add filtering, and in the future pagination, includes and ordering.\nBack to the modified GetInvoiceResourceCollectionHandler, the code below is all that is needed to retrieve the filters from the ResoureQuerySpecification class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 public class GetInvoiceResourceCollectionHandler : IRequestHandler\u0026lt;GetInvoiceResourceCollectionCommand, IEnumerable\u0026lt;Invoice\u0026gt;\u0026gt; { private readonly ChinookDbContext _chinookDbContext; public GetInvoiceResourceCollectionHandler(ChinookDbContext chinookDbContext) { _chinookDbContext = chinookDbContext; } public async Task\u0026lt;IEnumerable\u0026lt;Invoice\u0026gt;\u0026gt; Handle(GetInvoiceResourceCollectionCommand request, CancellationToken cancellationToken) { Expression\u0026lt;Func\u0026lt;Invoice, bool\u0026gt;\u0026gt; invoiceFilter = request.querySpecification.GetFilter\u0026lt;Invoice\u0026gt;(nameof(Invoice)); Expression\u0026lt;Func\u0026lt;Customer, bool\u0026gt;\u0026gt; customerFilter = request.querySpecification.GetFilter\u0026lt;Customer\u0026gt;(nameof(Customer)); var invoiceQuery = _chinookDbContext.Invoices.Where(invoiceFilter); var customerQuery = _chinookDbContext.Customers.Where(customerFilter); var query = from fi in invoiceQuery join fc in customerQuery on fi.CustomerId equals fc.CustomerId select fi; var result = await query .TagWithSource() .ToListAsync(); return result; } } Let\u0026rsquo;s break down the code, the GetInvoiceResourceCollectionCommand exposes now the specification we need to retrieve the filter expression, here the GetFilter is used to retrieve the generated filter expression, as mentioned before, if the filter doesn\u0026rsquo;t exist we default to the expression created by the PredicateBuilder class. Once we have the expression, a query is created for each resource we want to support in this case there is query for the Invoice and Customer resource. The two queries are then combined in a joining query, the joining query is executed, here EF Core will take the expression and translate it to the proper SQL code.\nResults Let\u0026rsquo;s run a few examples. If I navigate to the invoices resource collection without any query parameters I get back 412 records in a single call, remember we haven\u0026rsquo;t added pagination.\n1 GET /invoices HTTP/1.1 The database was queried using the following SQL query.\n1 2 3 SELECT \u0026#34;i\u0026#34;.\u0026#34;InvoiceId\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingAddress\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingCity\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingCountry\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingPostalCode\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingState\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;CustomerId\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;InvoiceDate\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;Total\u0026#34; FROM \u0026#34;invoices\u0026#34; AS \u0026#34;i\u0026#34; INNER JOIN \u0026#34;customers\u0026#34; AS \u0026#34;c\u0026#34; ON \u0026#34;i\u0026#34;.\u0026#34;CustomerId\u0026#34; = \u0026#34;c\u0026#34;.\u0026#34;CustomerId\u0026#34; So far so good, let\u0026rsquo;s run another test.\n1 GET /invoices?filter[invoices]=billingCountry eq \u0026#39;Brazil\u0026#39; HTTP/1.1 The API returns now only 35 records and not the usual 412, so some type of filtering appears to be happening. Let\u0026rsquo;s confirm by looking at the SQL generated.\n1 2 3 4 SELECT \u0026#34;i\u0026#34;.\u0026#34;InvoiceId\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingAddress\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingCity\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingCountry\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingPostalCode\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingState\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;CustomerId\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;InvoiceDate\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;Total\u0026#34; FROM \u0026#34;invoices\u0026#34; AS \u0026#34;i\u0026#34; INNER JOIN \u0026#34;customers\u0026#34; AS \u0026#34;c\u0026#34; ON \u0026#34;i\u0026#34;.\u0026#34;CustomerId\u0026#34; = \u0026#34;c\u0026#34;.\u0026#34;CustomerId\u0026#34; WHERE \u0026#34;i\u0026#34;.\u0026#34;BillingCountry\u0026#34; = \u0026#39;Brazil\u0026#39; That looks right.\nOne more test, filtering a related resource which is what started this blog post.\n1 GET /invoices?filter[invoices]=billingCountry eq \u0026#39;Brazil\u0026#39;\u0026amp;filter[customers]=firstName eq \u0026#39;Eduardo\u0026#39; Returns only 7 records, so now we have further filtering, let\u0026rsquo;s look at the generated SQL.\n1 2 3 4 5 6 7 8 SELECT \u0026#34;i\u0026#34;.\u0026#34;InvoiceId\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingAddress\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingCity\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingCountry\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingPostalCode\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;BillingState\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;CustomerId\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;InvoiceDate\u0026#34;, \u0026#34;i\u0026#34;.\u0026#34;Total\u0026#34; FROM \u0026#34;invoices\u0026#34; AS \u0026#34;i\u0026#34; INNER JOIN ( SELECT \u0026#34;c\u0026#34;.\u0026#34;CustomerId\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Address\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;City\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Company\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Country\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Email\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Fax\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;FirstName\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;LastName\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Phone\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;PostalCode\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;State\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;SupportRepId\u0026#34; FROM \u0026#34;customers\u0026#34; AS \u0026#34;c\u0026#34; WHERE \u0026#34;c\u0026#34;.\u0026#34;FirstName\u0026#34; = \u0026#39;Eduardo\u0026#39; ) AS \u0026#34;t\u0026#34; ON \u0026#34;i\u0026#34;.\u0026#34;CustomerId\u0026#34; = \u0026#34;t\u0026#34;.\u0026#34;CustomerId\u0026#34; WHERE \u0026#34;i\u0026#34;.\u0026#34;BillingCountry\u0026#34; = \u0026#39;Brazil\u0026#39; Oh yeah, that looks right, this is perfect, filtering appears to be working as expected and now our clients can query not just top level resource but also related resource in a single HTTP request.\nGoal achived.\nFurther Reading Here are a list of resource related to everything that I just talked about, these resource will come in handy if run into any issues.\nEntity Framework Core 5 – Pitfalls To Avoid and Ideas to Try Dynamically Build LINQ Expressions Expression Trees Giving Clarity to LINQ Queries by Extending Expressions How Do I Create an Expression\u0026lt;Func\u0026lt;\u0026raquo; with Type Parameters from a Type Variable Dynamically Composing Expression Predicates ","permalink":"http://localhost:1313/post/2023/json-api-implementing-filtering/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eIt has been over \u003ca href=\"/post/2022/json-api-pagination-links/\"\u003ea year since I last wrote\u003c/a\u003e about JSON:API, since then the \u003ca href=\"https://jsonapi.org/about/#editors\"\u003eteam\u003c/a\u003e behind JSON:API has published version 1.1 of the JSON:API specification. I would like to continue my journey of documenting JOSN:API in .NET by introducing a really cool feature to my \u003ca href=\"https://github.com/circleupx/Chinook\"\u003eChinook JSON:API\u003c/a\u003e project, filtering.\u003c/p\u003e\n\u003cp\u003eThe first thing to know about filtering in JSON:API is that the spec itself \u003ca href=\"https://jsonapi.org/format/#fetching-filtering\"\u003eis agnostic\u003c/a\u003e to any filtering strategies. Meaning it is up to you to define how filtering should be handled by your API. In my opinion, this has always been a drawback of the JSON:API spec, I believe in that it would have been a better choice for the spec if it had decided on a filtering strategy, but that is discussion for another day. While the spec does not favor any filtering strategy it does have some \u003ca href=\"https://jsonapi.org/recommendations/#filtering\"\u003erecommendations\u003c/a\u003e.\u003c/p\u003e","title":"JSON:API Implementing Filtering"},{"content":"Most developers are familiar with the modulo opertor, it is often presented in an example that determines if a number is odd or even as shown in the code below taken from Testing whether a value is odd or even.\n1 2 3 4 5 6 7 function isEven(n) { return n % 2 == 0; } function isOdd(n) { return Math.abs(n % 2) == 1; } Determining if a number is odd or even is just one use case for the modulo operator. Another use case that I learn a while back is that you can use the modulo operator to set a range, a boundary if you will, allowing you to rotate the array, this is because fundamentally A mod B is between 0 and B - 1 or another way to think about it, 0 \u0026lt;= A \u0026lt; B.\nFor example, imagine an array of length 4 and an indexer that is incremented as we loop through the array.\n1 2 3 4 5 6 7 8 9 0 % 4 = 0 1 % 4 = 1 2 % 4 = 2 3 % 4 = 3 4 % 4 = 0 // Return back to zero. 5 % 4 = 1 6 \u0026amp; 4 = 2 7 \u0026amp; 4 = 3 8 \u0026amp; 4 = 0 // Again, returns to zero. As you can see from the example above, the modulo operator created a boundary between 0 and 3, this technique is known as a circular array. You can use a circular array if you are implementing pagination and need to return to the first page when all the pages have been visited.\n","permalink":"http://localhost:1313/post/2023/circular-arrays/","summary":"\u003cp\u003eMost developers are familiar with the \u003ca href=\"https://en.wikipedia.org/wiki/Modulo\"\u003emodulo opertor\u003c/a\u003e, it is often presented in an example that determines if a number is odd or even as shown in the code below taken from \u003ca href=\"https://stackoverflow.com/questions/6211613/testing-whether-a-value-is-odd-or-even\"\u003eTesting whether a value is odd or even\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e7\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-JavaScript\" data-lang=\"JavaScript\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eisEven\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003en\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003en\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eisOdd\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003en\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e Math.\u003cspan style=\"color:#a6e22e\"\u003eabs\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003en\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eDetermining if a number is odd or even is just one use case for the modulo operator. Another use case that I learn a while back is that you can use the modulo operator to set a range, a boundary if you will, allowing you to rotate the array, this is because fundamentally A mod B is between 0 and B - 1 or another way to think about it, \u003cstrong\u003e0 \u0026lt;= A \u0026lt; B\u003c/strong\u003e.\u003c/p\u003e","title":"Circular Arrays"},{"content":"I\u0026rsquo;ve been spending the last few weeks learning Go by reading Learning Go by Jon Bodner, so far I\u0026rsquo;ve been enjoying learning about Go, though there is still one thing I keep tripping over, in Go, you can return one or more values, for me Go is the first language that I have worked with that does that, in every other language I had to introduce a custom discriminating union to achieve what Go does natively.\nTake the following Go code.\n1 2 3 4 5 6 7 8 9 10 11 func twoSum(nums []int, target int) []int { s := make(map[int]int) for idx, num := range nums { if pos, ok := s[target-num]; ok { return []int{pos, idx} } s[num] = idx } return []int{} } Most of it can be understood even by those that have never worked with Go, the part that I was having a rough time understanding was the if statement below.\n1 2 3 if pos, ok := s[target-num]; ok { return []int{pos, idx} } I wasn\u0026rsquo;t understanding how the variable \u0026ldquo;ok\u0026rdquo; could evaluate to true, I\u0026rsquo;ve worked so much with C# that my brain naturally tried to read the code as if it were C#, and in C# like many other objection-oriented languages you can only return one value. Looking at the official docs I found the following note under Index Expression.\nAn index expression on a map of type map[K]V used in an assignment statement or initialization of the special form yields an additional untyped boolean value. The value of ok is true if the key x is present in the map, and false otherwise.\nIn other words, in the example above if target minus sum yields a value, that value is assigned to the variable pos, and the untyped boolean value is assigned to the variable \u0026ldquo;ok\u0026rdquo;, then the variable \u0026ldquo;ok\u0026rdquo; is evaluated.\n","permalink":"http://localhost:1313/post/2023/go-multiple-return-values/","summary":"\u003cp\u003eI\u0026rsquo;ve been spending the last few weeks learning \u003ca href=\"https://go.dev/learn/\"\u003eGo\u003c/a\u003e by reading \u003ca href=\"https://a.co/d/dlJyukR\"\u003eLearning Go\u003c/a\u003e by \u003ca href=\"https://www.amazon.com/stores/Jon-Bodner/author/B08SWGN5NN\"\u003eJon Bodner\u003c/a\u003e, so far I\u0026rsquo;ve been enjoying learning about Go, though there is still one thing I keep tripping over, in Go, you can return one or more values, for me Go is the first language that I have worked with that does that, in every other language I had to introduce a custom discriminating union to achieve what Go does natively.\u003c/p\u003e","title":"Go - Multiple Return Values"},{"content":"Introduction I have been spending my last few weeks sharpening up my Kubernetes skills, one area that I focused on was how to enable and use a Service Mesh in Kubernetes. A service mesh is a layer in your infrastructure that enables control of inbound and outboard traffic. It controls the traffic of any app or service that uses the network.\nKubernetes offers a wide range of Service Meshes, in this blog post I am going to concentrate on HashiCorp\u0026rsquo;s service mesh offering, Consul, though you may see other refer to it as Consul Connect, Consul Connect is a set of features that were added to Consul was in 2018 to enable service mesh support.\nThese features consist of three parts, first, the Consul server component, the Consul client, and the sidecars that are deployed. The server component is responsible for persisting data, i.e. configurations, thus requiring high availability, don\u0026rsquo;t run a single Consul Server in a Production environment. The client component resides in your node, it is responsible for reporting the health of each service running on your node as well as keeping track of the health of other services in other nodes. The client sends this information back to the Consul server component. Another responsibility of the client is to configure and manage all sidecars. The sidecars are responsible for intercepting inbound and outbound network traffic through a proxy, Consul leverages Envoy to achieve this feature.\nI have prepared two sample applications to demo how you can configure and use Consul as a Service Mesh in Kubernetes. The first app will be a frontend application written in Blazor, the second app will be a Web API written in .NET 6 using minimal APIs. The Blazor application will call the Web API to get a weather forecast, there will be no data validation, authentication or authorization in the apps since the main focus of this post is to show how to use Consul.\nWeb API The Web API is written using the new Minimal APIs feature offered by .NET. The API generates an in-memory collection of WeatherForecast, the entire API can be configured as shown below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 namespace BackendApp { public class Program { public static void Main(string[] args) { var builder = WebApplication.CreateBuilder(args); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); var app = builder.Build(); app.UseHttpsRedirection(); var summaries = new[] { \u0026#34;Freezing\u0026#34;, \u0026#34;Bracing\u0026#34;, \u0026#34;Chilly\u0026#34;, \u0026#34;Cool\u0026#34;, \u0026#34;Mild\u0026#34;, \u0026#34;Warm\u0026#34;, \u0026#34;Balmy\u0026#34;, \u0026#34;Hot\u0026#34;, \u0026#34;Sweltering\u0026#34;, \u0026#34;Scorching\u0026#34; }; app.MapGet(\u0026#34;/weatherforecast\u0026#34;, (HttpContext httpContext) =\u0026gt; { var forecast = Enumerable.Range(1, 5).Select(index =\u0026gt; new WeatherForecast { Date = DateOnly.FromDateTime(DateTime.Now.AddDays(index)), TemperatureC = Random.Shared.Next(-20, 55), Summary = summaries[Random.Shared.Next(summaries.Length)] }) .ToArray(); return forecast; }) .WithName(\u0026#34;GetWeatherForecast\u0026#34;) .WithOpenApi(); app.Run(); } } } In the code above, the router handler MapGet is used to map incoming HTTP requests whenever the incoming router matches \u0026ldquo;/weatherforecast\u0026rdquo;.\nAPI Docker File To be able to run the API in Kubernetes I will need to containerize the app, the following Docker file should do the trick.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM mcr.microsoft.com/dotnet/aspnet:7.0 AS base WORKDIR /app EXPOSE 80 FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build WORKDIR /src COPY [\u0026#34;App/BackendApp.csproj\u0026#34;, \u0026#34;App/\u0026#34;] RUN dotnet restore \u0026#34;App/BackendApp.csproj\u0026#34; COPY . . WORKDIR \u0026#34;/src/App\u0026#34; RUN dotnet build \u0026#34;BackendApp.csproj\u0026#34; -c Release -o /app/build FROM build AS publish RUN dotnet publish \u0026#34;BackendApp.csproj\u0026#34; -c Release -o /app/publish /p:UseAppHost=false FROM base AS final WORKDIR /app COPY --from=publish /app/publish . ENTRYPOINT [\u0026#34;dotnet\u0026#34;, \u0026#34;BackendApp.dll\u0026#34;] Then to build the container image with Docker execute the following command.\n1 docker build . -f App/Dockerfile -t backend:1.0.0 The dot after the build command refers to the current working directory, the -f is the path to the Docker file and the -t command sets a tag, in my case I decided to append version 1.0.0 to my app. After executing the command above, run the following command to confirm the image was created.\n1 2 3 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE backend 1.0.0 f20ce50a5b86 39 seconds ago 216MB Blazor App Next up, I have created a frontend application base on Blazor, the app uses the default Blazor template, and as part of the template, Blazor adds a weather forecast page. The data displayed is loaded from an in-memory collection. I am going to change the application to load the data from the Web API created above.\nThis is the WeatherForecastService that comes from the Blazor template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class WeatherForecastService { private static readonly string[] Summaries = new[] { \u0026#34;Freezing\u0026#34;, \u0026#34;Bracing\u0026#34;, \u0026#34;Chilly\u0026#34;, \u0026#34;Cool\u0026#34;, \u0026#34;Mild\u0026#34;, \u0026#34;Warm\u0026#34;, \u0026#34;Balmy\u0026#34;, \u0026#34;Hot\u0026#34;, \u0026#34;Sweltering\u0026#34;, \u0026#34;Scorching\u0026#34; }; public Task\u0026lt;WeatherForecast[]\u0026gt; GetForecastAsync(DateOnly startDate) { return Task.FromResult(Enumerable.Range(1, 5).Select(index =\u0026gt; new WeatherForecast { Date = startDate.AddDays(index), TemperatureC = Random.Shared.Next(-20, 55), Summary = Summaries[Random.Shared.Next(Summaries.Length)] }).ToArray()); } } If you were to run the app and navigate to the forecast page you would see something similar to what is being shown on the screenshot below.\nI\u0026rsquo;ll need to refactor the GetForecastAsync method to use an HTTP client to make an API request to the Web API endpoint, https://localhost:7043/weatherforecast, to retrieve the WeatherForecast data from the API\n1 2 3 4 5 6 7 8 9 10 11 12 namespace FrontEndApp.Data { public class WeatherForecastService { public async Task\u0026lt;WeatherForecast[]\u0026gt; GetForecastAsync() { var httpClient = new HttpClient(); var weatherForecastCollection = await httpClient.GetFromJsonAsync\u0026lt;WeatherForecast[]\u0026gt;(\u0026#34;https://localhost:7043/weatherforecast\u0026#34;); return weatherForecastCollection; } } } The code above will establish a dependency between the Web API and the Blazor App, the two apps now communicate through the network, once I introduce Consul into the mix, I will show how the network communication between these two apps can be secured by simply installing a Service Mesh.\nBlazor Docker File Just like the Web API, I will need to containerize the Blazor app, the following Docker file should do the trick.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM mcr.microsoft.com/dotnet/aspnet:7.0 AS base WORKDIR /app EXPOSE 80 FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build WORKDIR /src COPY [\u0026#34;App/FrontEndApp.csproj\u0026#34;, \u0026#34;App/\u0026#34;] RUN dotnet restore \u0026#34;App/FrontEndApp.csproj\u0026#34; COPY . . WORKDIR \u0026#34;/src/App\u0026#34; RUN dotnet build \u0026#34;FrontEndApp.csproj\u0026#34; -c Release -o /app/build FROM build AS publish RUN dotnet publish \u0026#34;FrontEndApp.csproj\u0026#34; -c Release -o /app/publish /p:UseAppHost=false FROM base AS final WORKDIR /app COPY --from=publish /app/publish . ENTRYPOINT [\u0026#34;dotnet\u0026#34;, \u0026#34;FrontEndApp.dll\u0026#34;] Then to build the container image with Docker execute the following command.\n1 docker build . -f App/Dockerfile -t frontend:1.0.0 Again, the dot after the build command refers to the current working directory, the -f is the path to the Docker file and the -t command sets a tag, in my case I decided to append version 1.0.0 to my app. After executing the command above, run the following command to confirm the image was created.\n1 2 3 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE frontend 1.0.0 f20ce50a5b86 4 seconds ago 216MB Configuring Helm I plan to use Helm to deploy my applications, Helm is the package manager for Kubernetes, it is a great tool to manage Kubernetes applications, you can use Helm to define, install and upgrade any Kubernetes application.\nTo use Helm I will need to create a Helm Chart for the frontend and backend application. Executing the following command will create a new helm chart.\n1 helm create backendapp Executing the command above will create a new chart titled \u0026ldquo;backendapp\u0026rdquo;, the chart is composed of a few files as seen on the folder tree below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Root ├── .vscode │ └── settings.json ├── App │ ├── appsettings.json │ ├── BackendApp.csproj │ ├── Dockerfile │ ├── Program.cs │ ├── Properties │ │ └── launchSettings.json │ └── WeatherForecast.cs ├── BackendApp.sln └── Deploy └── backendapp ├── .helmignore ├── Chart.yaml ├── charts ├── templates │ ├── deployment.yaml │ ├── hpa.yaml │ ├── ingress.yaml │ ├── NOTES.txt │ ├── service.yaml │ ├── serviceaccount.yaml │ ├── tests │ │ └── test-connection.yaml │ └── _helpers.tpl └── values.yaml #The file you provide to consumers of your chart. The deploy folder is the root directory for all helm charts. Within that folder, you will find the chart.yaml file, this is where all chart metadata is placed i.e. chart name, the Charts folder is where you place any Charts that your Chart depends on. The template folder is the directory containing the files used to manifest your chart. It utilizes Go\u0026rsquo;s template language.\nThe chart needs a few updates, first in the Chart.yaml file, I will set the appVersion to 1.0.0, this is because the Docker image was tagged with 1.0.0 and in my values.yaml I left the tag value empty.\n1 2 3 4 5 6 apiVersion: v2 name: backendapp description: A Helm chart for Kubernetes type: application version: 0.1.0 appVersion: \u0026#34;1.0.0\u0026#34; The next change required is to the values.yaml file, I need to change the repository as newly created Helm Charts look for Nginx by default. The name of my container image is backend, so that is the value I will use, see below.\n1 2 3 4 5 image: repository: backendapp pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVekubecrsion. tag: \u0026#34;\u0026#34; The \u0026ldquo;pullPolicy\u0026rdquo; being set to \u0026ldquo;IfNotPresent\u0026rdquo; is important as it allows Helm to pull images from the local container registry.\nNo more changes are required for the backend Helm chart, next, I will create the Helm chart for the frontend using the following command.\n1 helm create frontendapp Just like the Web API, this will create a chart named \u0026ldquo;frontendapp under my deploy folder within the FrontendApp and just like the Web API I will need to update the values.yaml and chart.yaml files as shown below.\n1 2 3 4 5 6 apiVersion: v2 name: frontendapp description: A Helm chart for Kubernetes type: application version: 0.1.0 appVersion: \u0026#34;1.0.0\u0026#34; 1 2 3 4 5 image: repository: frontendapp pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVekubecrsion. tag: \u0026#34;\u0026#34; I\u0026rsquo;ll need to make an additional update to the Helm chart by changing the port forward from port 8080, the two apps could potentially run on the same node and run into a port collision, therefore, I will change the UI to port forward on 8081, you can do it manually on the terminal using the kubectl port forward but I am going to go ahead and modify the command on the notes.txt file.\nDeploying App to Kubernetes Time to use Helm to deploy both apps. First I am going to deploy the backend app using the following command.\n1 helm install backendapp ./ The helm install command takes the name for the install, the chart, this can be a path and a set of flags, in the command above I am not providing any flags, just the name and the path to the chart, which is the current directory.\nThe output of that command should be the values found in notes.txt, as shown below.\n1 2 3 4 export POD_NAME=$(kubectl get pods --namespace default -l \u0026#34;app.kubernetes.io/name=backendapp,app.kubernetes.io/instance=backendapp\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath=\u0026#34;{.spec.containers[0].ports[0].containerPort}\u0026#34;) echo \u0026#34;Visit http://127.0.0.1:8080 to use your application\u0026#34; kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT Run each of the following commands to being port forwarding from port 8080 on your machine to port 80 on the container.\n1 2 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 Great, I should be able to send the following curl command and get a successful response from the API.\n1 2 3 curl -X \u0026#39;GET\u0026#39; \\ \u0026#39;http://localhost:8080/weatherforecast\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; The command above yields the following response from the API.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [ { \u0026#34;date\u0026#34;: \u0026#34;2023-06-22\u0026#34;, \u0026#34;temperatureC\u0026#34;: -5, \u0026#34;temperatureF\u0026#34;: 24, \u0026#34;summary\u0026#34;: \u0026#34;Bracing\u0026#34; }, { \u0026#34;date\u0026#34;: \u0026#34;2023-06-23\u0026#34;, \u0026#34;temperatureC\u0026#34;: 14, \u0026#34;temperatureF\u0026#34;: 57, \u0026#34;summary\u0026#34;: \u0026#34;Scorching\u0026#34; }, { \u0026#34;date\u0026#34;: \u0026#34;2023-06-24\u0026#34;, \u0026#34;temperatureC\u0026#34;: 15, \u0026#34;temperatureF\u0026#34;: 58, \u0026#34;summary\u0026#34;: \u0026#34;Mild\u0026#34; }, { \u0026#34;date\u0026#34;: \u0026#34;2023-06-25\u0026#34;, \u0026#34;temperatureC\u0026#34;: 27, \u0026#34;temperatureF\u0026#34;: 80, \u0026#34;summary\u0026#34;: \u0026#34;Chilly\u0026#34; }, { \u0026#34;date\u0026#34;: \u0026#34;2023-06-26\u0026#34;, \u0026#34;temperatureC\u0026#34;: 46, \u0026#34;temperatureF\u0026#34;: 114, \u0026#34;summary\u0026#34;: \u0026#34;Bracing\u0026#34; } ] Excellent, the API was deployed successfully and it is listening to the incoming requests, I don\u0026rsquo;t need to keep the port-forward proxy open so I\u0026rsquo;ll use CTRL+C to end the proxy.\nTime to deploy the frontend app.\nJust like before, execute the Helm install command.\n1 helm install frontendapp ./ The command should yield something similar to the following.\n1 2 3 4 5 6 7 8 9 10 11 NAME: frontendapp LAST DEPLOYED: Wed Jun 21 20:54:33 2023 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace default -l \u0026#34;app.kubernetes.io/name=frontendapp,app.kubernetes.io/instance=frontendapp\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath=\u0026#34;{.spec.containers[0].ports[0].containerPort}\u0026#34;) echo \u0026#34;Visit http://127.0.0.1:8081 to use your application\u0026#34; kubectl --namespace default port-forward $POD_NAME 8081:$CONTAINER_PORT Execute each of the commands from the notes to get the port-forwarding proxy going.\n1 2 Forwarding from 127.0.0.1:8081 -\u0026gt; 80 Forwarding from [::1]:8081 -\u0026gt; 80 Great, let\u0026rsquo;s test it out, I should be able to open my browser to http://localhost:8081, then navigate to the Fetch Data page and see the randomly generate weather forecast create by the API.\nAs expected, it works, the Blazor App is running on Kubernetes and it is able to communicate with the Web API.\nNow that we have the apps deployed and exchanging dates I\u0026rsquo;ll introduce Consul and configure it to control how the app communicate with each other.\nNote: Not shown here, but the frontend required an additional update, the URL change from https://localhost:7043/weatherforecast to http://backendapp:80/weatherforecast that is because the frontend Pod talks to the backend pod though a Kubernetes Service name \u0026ldquo;backendapp\u0026rdquo;. This service was generated when the app was installed with Helm, see the service.yaml file under the templates folder.\nInstall Consul You can install Consul using the Consul K8s CLI, the CLI makes it easy to get up and running, another option is to install Consul using Helm. That is the option that I will show here.\nTo install Consul using Helm you will need to add the HashiCorp Helm repository by running on the following command.\n1 helm repo add hashicorp https://helm.releases.hashicorp.com You should see the following as a confirmation.\n1 \u0026#34;hashicorp\u0026#34; has been added to your repositories With the HashiCorp Helm repo installed you can use the following command to install Consul, just don\u0026rsquo;t execute it yet.\n1 helm install consul hashicorp/consul --set global.name=consul --create-namespace --namespace consul The command above will install Consul but it does so with the default configurations, you can provide your own configurations by creating your own values.yaml. In a text editor, create a values.yaml file and add the following content to it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # Contains values that affect multiple components of the chart. global: # The main enabled/disabled setting. # If true, servers, clients, Consul DNS and the Consul UI will be enabled. enabled: true # The prefix used for all resources created in the Helm chart. name: consul # The consul image version. image: hashicorp/consul:1.15.3 # The name of the datacenter that the agents should register as. datacenter: dc1 # Enables TLS across the cluster to verify authenticity of the Consul servers and clients. tls: enabled: true # Enables ACLs across the cluster to secure access to data and APIs. acls: # If true, automatically manage ACL tokens and policies for all Consul components. manageSystemACLs: true # Exposes Prometheus metrics for the Consul service mesh and sidecars. metrics: enabled: true # Enables Consul servers and clients metrics. enableAgentMetrics: true # Configures the retention time for metrics in Consul servers and clients. agentMetricsRetentionTime: \u0026#34;1m\u0026#34; # Configures values that configure the Consul server cluster. server: enabled: true # The number of server agents to run. This determines the fault tolerance of the cluster. replicas: 1 # Contains values that configure the Consul UI. ui: enabled: true # Defines the type of service created for the Consul UI (e.g. LoadBalancer, ClusterIP, NodePort). # NodePort is primarily used for local deployments. service: type: NodePort # Enables displaying metrics in the Consul UI. metrics: enabled: true # The metrics provider specification. provider: \u0026#34;prometheus\u0026#34; # Configures and installs the automatic Consul Connect sidecar injector. connectInject: enabled: true # Enables metrics for Consul Connect sidecars. metrics: defaultEnabled: true You can see the default values on the Consul chart by inspecting the Consul chart using the following command.\n1 helm inspect values hashicorp/consul Or you can visit the official Helm Chart Reference docs to see all the values that can be overwritten on your values.yaml file.\nNow you can install Consul by executing the following command.\n1 helm install consul hashicorp/consul --create-namespace --namespace consul --values values.yaml Successful execution of the command above yields the following.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 NAME: consul LAST DEPLOYED: Wed Jun 21 22:40:35 2023 NAMESPACE: consul STATUS: deployed REVISION: 1 NOTES: Thank you for installing HashiCorp Consul! Your release is named consul. To learn more about the release, run: $ helm status consul --namespace consul $ helm get all consul --namespace consul Consul on Kubernetes Documentation: https://www.consul.io/docs/platform/k8s Consul on Kubernetes CLI Reference: https://www.consul.io/docs/k8s/k8s-cli NOTE The version installed in this example is Consul 1.15.3 which is the most recent version of Consul as of June 2023.\nNow run the following command to see the status of the newly created resources.\n1 kubectl get statefulset,deployment -n consul 1 2 3 4 5 6 7 NAME READY AGE statefulset.apps/consul-server 1/1 10m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/consul-connect-injector 1/1 1 1 10m deployment.apps/consul-webhook-cert-manager 1/1 1 1 10m deployment.apps/prometheus-server 1/1 1 1 10m The deployment consul-connect-injector is responsible for injecting services mesh sidecars as well as keeping the Kubernetes probes in sync with Consul. The consul-webhook-cert-manager deployment is responsible for creating certificates. The prometheus-server deployment runs Prometheus, see Prometheus for more details, and the stateful set consul-server manages persistence claim volumes for the Consul server, remember this should be highly available, losing the claim volume could result in total loss of data.\nNote that these resources could take up to a minute to be created, though normally you should expect them to be created within a couple of seconds.\nAn additional confirmation to Consul being successfully installed is to connect to the UI using a port-forwarding proxy, you can do so by executing the following command.\n1 kubectl port-forward service/consul-server --namespace consul 8500:8500 Then on a web browser navigate to http://localhost:8500/ui/, you should see the Consul UI as shown in the screenshot below.\nNext, let me confirm that the Blazor app is still up and running.\nAnd it is, great, while Consul has been installed successfully it is still not handling the network communication between the two pods. In order for Consul to handle the network communication between pods, the pods need to be added to the service mesh via pod annotation, see Annotations for more details. In Consul, the annotation required is consul.hashicorp.com/connect-inject:\u0026ldquo;true\u0026rdquo;.\nAdding Services to the Mesh The annotation, consul.hashicorp.com/connect-inject:\u0026ldquo;true\u0026rdquo;, needs to be added to each pod running under the frontend app deployment, to add the annotation I will need to modify the values.yaml file from\n1 podAnnotations: {} to the following.\n1 2 podAnnotations: \u0026#34;consul.hashicorp.com/connect-inject\u0026#34;: \u0026#34;true\u0026#34; With the annotation now added to the deployment.yaml file, the app can be redeployed. After redeploying the app, and returning to the Consul UI, the frontend app should now be registered under Services as shown in the screenshot below.\nConsul uses a mutating webhook, that is, an HTTP callback that allows third-party applications to modify Kubernetes Resources. When the frontend pod was scheduled, Kubernetes called Consul\u0026rsquo;s mutating webhook, which allows Consul to look at the pod definition to see if it has the consul.hashicorp.com/connect-inject:\u0026ldquo;true\u0026rdquo; annotation, if it does, Consul modifies the pod to add a sidecar proxy.\nTime to add the \u0026ldquo;backendapp\u0026rdquo; to the mesh, as before, I will update the values.yaml file to include the required annotation on any backend pod.\n1 2 podAnnotations: consul.hashicorp.com/connect-inject: \u0026#34;true\u0026#34; Redeploying the backend app and visiting the Consul UI shows the app was successfully registered, see the screenshot below.\nThis means both apps are now secure by the Consul Service Mesh since Consul is secure by default, this also means that no traffic outside of the mesh may reach the apps, not very useful, but I\u0026rsquo;ll change that in part 2 of this blog series.\nFirst, let\u0026rsquo;s prove that the apps are secure by Consul and that only authenticated and authorized traffic is allowed to reach the applications. In my current cluster, aside from the frontend and backend app I have a few other pods, one of them being the consul server itself, if I have configure everything correctly, then the consul server pod should not be able to communicate with the frontend app, this can be proven by executing the following command.\n1 kubectl exec consul-server-0 -n consul -- curl -sS http://frontend-frontendapp:80 I get this as a response.\n1 2 curl: (52) Empty reply from server command terminated with exit code 52 This means the frontend app is secure, containers that are not in the Service Mesh cannot talk to applications that are within the mesh, essentially the Service Mesh rejectes any unauthorized traffic, I can further prove that by removing the Consul annotation from the frontendapp, then redeploying the frontend app, and executing the same command, though this time I get a different response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34; /\u0026gt; \u0026lt;base href=\u0026#34;/\u0026#34; /\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;css/bootstrap/bootstrap.min.css\u0026#34; /\u0026gt; \u0026lt;link href=\u0026#34;css/site.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;link href=\u0026#34;FrontEndApp.styles.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; href=\u0026#34;favicon.png\u0026#34;/\u0026gt; \u0026lt;!--Blazor:{\u0026#34;sequence\u0026#34;:0,\u0026#34;type\u0026#34;:\u0026#34;server\u0026#34;,\u0026#34;prerenderId\u0026#34;:\u0026#34;ee0f04c3fa464763b57c57330cede53f\u0026#34;,\u0026#34;descriptor\u0026#34;:\u0026#34;CfDJ8JJV\\u002BT0V21FCl\\u002Bk1k\\u002BLUY\\u002BBnKRRxy3paaV9R7emQy2NWR\\u002BXDx\\u002BwW0sLcOLDKemVI1mSpmb4/qiN4kzx9P0xqw94CAz/bk/llLJNEV7BeQm5ywTpAXn09jXJKz7LDw1rs9MUSeZ6ncDwOZdFFkmuEu/NwIei0XOdnwzxSvEjtvrs0rSIUeZWIWLsRFuMwZX/tcppkDSBJCQNSV9dL0aXaNWQVVKURgi\\u002BBXUpy/hL7b8lozx3WJ\\u002BktZX2AqNE6DSbtzsXp2i8JpOBLhT3GS2Pu\\u002BGWXX\\u002B5tPVgQsFvxVYDlRG\\u002Bnkbc3FrSaluGIhgmZMoG9901C747UrYYTwj0Fy4qaQ4i/\\u002BtGMvwZPWkM9O5A8TWSBQ6\\u002Bvrgc7FKXLkF0MwcKJuCVlH0ZCK6oqbUfVJoRk1MNJQTLL9sKcHQc//eQJb\\u002BY\\u002B\u0026#34;}--\u0026gt;\u0026lt;title\u0026gt;Index\u0026lt;/title\u0026gt;\u0026lt;!--Blazor:{\u0026#34;prerenderId\u0026#34;:\u0026#34;ee0f04c3fa464763b57c57330cede53f\u0026#34;}--\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!--Blazor:{\u0026#34;sequence\u0026#34;:1,\u0026#34;type\u0026#34;:\u0026#34;server\u0026#34;,\u0026#34;prerenderId\u0026#34;:\u0026#34;a9e663ffbd414a9c82f7811056718548\u0026#34;,\u0026#34;descriptor\u0026#34;:\u0026#34;CfDJ8JJV\\u002BT0V21FCl\\u002Bk1k\\u002BLUY\\u002BAIAzoXlkJews\\u002BivCBmR0zSEmSPge5jSzQzOvKwgPLyF1HVzKYJkbCOsjNqKHjf6njkosboRc/F6zkql4GFmkDxrvC8B2\\u002BsyGz7DUoxO2IAPeY1SQJB7YMMzKuQf/ZoIQrk6BKE3EyjcKPq\\u002BWFuSaoP6clUsAErNrZF1o35qh/yo5Aokhsstpf6ypffABkYczzpMXoyH963a\\u002BwAWwM4QMYXtwux8bQAONLSoYXnP7juOqzWzjo9nUnM2Zo46TyxoPPa9627V2BS4eRWG07EcljuIMjuS6rGmDZ3u\\u002BNfprAO8DppGJ1EgDZg3XXiguNVCQdAQC0TeG9B4GEslITYFIJB\u0026#34;}--\u0026gt; \u0026lt;div class=\u0026#34;page\u0026#34; b-u0fy2w1pvo\u0026gt;\u0026lt;div class=\u0026#34;sidebar\u0026#34; b-u0fy2w1pvo\u0026gt;\u0026lt;div class=\u0026#34;top-row ps-3 navbar navbar-dark\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;div class=\u0026#34;container-fluid\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;a class=\u0026#34;navbar-brand\u0026#34; href b-125dvlpeqw\u0026gt;FrontEndApp\u0026lt;/a\u0026gt; \u0026lt;button title=\u0026#34;Navigation menu\u0026#34; class=\u0026#34;navbar-toggler\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;span class=\u0026#34;navbar-toggler-icon\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/button\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;collapse nav-scrollable\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;nav class=\u0026#34;flex-column\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;div class=\u0026#34;nav-item px-3\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;a href=\u0026#34;\u0026#34; class=\u0026#34;nav-link active\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;oi oi-home\u0026#34; aria-hidden=\u0026#34;true\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;/span\u0026gt; Home \u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;nav-item px-3\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;a href=\u0026#34;counter\u0026#34; class=\u0026#34;nav-link\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;oi oi-plus\u0026#34; aria-hidden=\u0026#34;true\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;/span\u0026gt; Counter \u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;nav-item px-3\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;a href=\u0026#34;fetchdata\u0026#34; class=\u0026#34;nav-link\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;oi oi-list-rich\u0026#34; aria-hidden=\u0026#34;true\u0026#34; b-125dvlpeqw\u0026gt;\u0026lt;/span\u0026gt; Fetch data \u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/nav\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;main b-u0fy2w1pvo\u0026gt;\u0026lt;div class=\u0026#34;top-row px-4\u0026#34; b-u0fy2w1pvo\u0026gt;\u0026lt;a href=\u0026#34;https://docs.microsoft.com/aspnet/\u0026#34; target=\u0026#34;_blank\u0026#34; b-u0fy2w1pvo\u0026gt;About\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;article class=\u0026#34;content px-4\u0026#34; b-u0fy2w1pvo\u0026gt; \u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt; Welcome to your new app. \u0026lt;div class=\u0026#34;alert alert-secondary mt-4\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;oi oi-pencil me-2\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;strong\u0026gt;How is Blazor working for you?\u0026lt;/strong\u0026gt; \u0026lt;span class=\u0026#34;text-nowrap\u0026#34;\u0026gt; Please take our \u0026lt;a target=\u0026#34;_blank\u0026#34; class=\u0026#34;font-weight-bold link-dark\u0026#34; href=\u0026#34;https://go.microsoft.com/fwlink/?linkid=2186158\u0026#34;\u0026gt;brief survey\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt; and tell us what you think. \u0026lt;/div\u0026gt;\u0026lt;/article\u0026gt;\u0026lt;/main\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!--Blazor:{\u0026#34;prerenderId\u0026#34;:\u0026#34;a9e663ffbd414a9c82f7811056718548\u0026#34;}--\u0026gt; \u0026lt;div id=\u0026#34;blazor-error-ui\u0026#34;\u0026gt; An error has occurred. This application may no longer respond until reloaded. \u0026lt;a href=\u0026#34;\u0026#34; class=\u0026#34;reload\u0026#34;\u0026gt;Reload\u0026lt;/a\u0026gt; \u0026lt;a class=\u0026#34;dismiss\u0026#34;\u0026gt;🗙\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;_framework/blazor.server.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; The response is the HTML of the main landing page of the Blazor application, which means that the consul-server pod is now allowed to communicate with the frontend app, due to the removal of the frontend app from the Service Mesh. This is great, but we need to allow traffic from outside the mesh to reach the apps that live within the Service Mesh, we also need to control how apps communicate with each other inside the service. Allowing external traffic into the Service Mesh in a secure fashion will be the responsibility of the Ingress gateways, which I\u0026rsquo;ll cover in part 2 of this series along with how to establish strong ACLS within the Service Mesh.\nConclusion Using Consul as a Service Mesh in Kubernetes turned out to be easier than expected, the documentation provided by HashiCorp was super useful and pointed me in the right direction whenever I felt lost. I did encounter a weird behavior with the Web API, the liveness probe was pointing to /swagger, just like the readiness probe, and while the readiness probe was succeeding, the liveness probe was failing, so I had to take off the liveness probe from the Helm chart, I\u0026rsquo;m not sure what the issue was, once I discover the problem I will share it here.\nPart two of this series will be on the Consul Ingress Gateway, after that, I plan to move to Istio and Linkerd, two other popular Service Mesh tools in Kubernetes.\nTill then, cheerio.\nResources Helm Chart Reference\nHelm Uninstall Commands\nDeploy Consul datacenter\n","permalink":"http://localhost:1313/post/2023/consul-service-mesh-in-kubernetes-part-1/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI have been spending my last few weeks sharpening up my Kubernetes skills, one area that I focused on was how to enable and use a Service Mesh in Kubernetes. A service mesh is a layer in your infrastructure that enables control of inbound and outboard traffic. It controls the traffic of any app or service that uses the network.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://thechief.io/c/editorial/top-14-kubernetes-service-meshes/\"\u003eKubernetes offers a wide range of Service Meshes\u003c/a\u003e, in this blog post I am going to concentrate on HashiCorp\u0026rsquo;s service mesh offering, Consul, though you may see other refer to it as Consul Connect, Consul Connect is a set of features that were added to Consul was in 2018 to enable service mesh support.\u003c/p\u003e","title":"Consul Service Mesh in Kubernetes - Part 1"},{"content":"Introduction I am working on a project that requires the usage of rules engines to enforce business rules, I am unsure if I should roll out my own custom implementation, probably a bad idea, or if I should use an existing project.\nTo help me make a decision I will need to look at the current options for rules engines available in .NET, I need to understand their capabilities and limitations. In .NET the most well-known rules engine is probably NRules, the project has been around for some years and has good documentation. I also know that Microsoft created its own rules engine, RulesEngine back in 2019. Then per awesome-dotnet I could use Plastic or Peasy.NET but I opted out on looking at those projects, for my use case I think NRules or RulesEngine will do.\nTo determine the final solution I will create a proof of concept project that uses each project, plus my own implementation.\nThe first step here is to come up with a business and some business rules. My fictional business will be a pizza store, called Glorious Pizza, as the owner of Glorious Pizza I want to enforce the following business rule.\nOn Saturdays, all pizzas should have a 10 percent discount. NRules The first project I am going to test will be NRules. I have created an API project called GloriousPizza, the API will expose an endpoint to create an order. The create order request only contains a single field, total.\n1 2 3 { \u0026#34;total\u0026#34;: 9.99 } When the request is received by the API, NRules will be invoked and the rules I have defined above will be executed against the incoming request. Now I need to translate my rules into NRules using the NRules DSL. The rule can be translated using the following DiscountRule class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class DiscountRule: Rule { public override void Define() { Order order = default; When() .Match(() =\u0026gt; order) // Bind this rule to an NRule fact of type Orders .Match\u0026lt;Order\u0026gt;(x=\u0026gt;x.CreatedDate.DayOfWeek == DayOfWeek.Sunday); Then() .Do(ctx =\u0026gt; ApplyDiscount(order)); } private static void ApplyDiscount(Order order) { var discount = (order.Total / 100) * 10; order.Total = order.Total - discount; } } The class implements a Rule from NRules, Rule is the base class exposed by NRule it offers helper functions such as When And Then, When is used to wire up a rule to a class. In the case above the When method is used to tell NRules to first bind any matching facts, your domain class or data model, in my case that being Orders. It is very important to bind a rule to a fact, if not done correctly then when the rule evaluation kicks in then the order variable will be null, meaning that a null exception would be thrown within the ApplyDiscount. Additionally, the parameter passed in the method ApplyDiscount must have a matching name, if instead, I were to rename the parameter orders to plural, then a null reference exception would occur. The second match invocation tells NRules to only apply this rule to orders that have a created date of Sunday.\nI can test the rule by modifying the API resource controller to include the following code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [Route(\u0026#34;api/[controller]\u0026#34;)] [ApiController] public class OrdersController : ControllerBase { [HttpPost] public IActionResult CreateResource(Order order) { var repository = new RuleRepository(); repository.Load(x =\u0026gt; x.From(typeof(Program).Assembly)); //Compile rules var factory = repository.Compile(); //Create a working session var session = factory.CreateSession(); session.Insert(order); // Execute all rules in session. session.Fire(); return Created(\u0026#34;\u0026#34;, order); } } The first thing this code is doing is creating an empty Rules repository, by default the repository created by NRules acts against an in-memory database that holds all the rules. After the repository is instantiated, we load all rules in the current assembly and load them into the repository. NRule is able to add any rule that implements the Rule class. After the rules have been added you will need to compile them, all rules defined in NRules are nothing more than expression trees, expression trees are a representation of a code, and like all frameworks that deal with expression trees compilation is required to convert the expression trees into executable delegates.\nAfter compiling all the rules, you will need to create a session, sessions are used to ensure that rules are only applied to facts that are added to the session, and as you can tell from the code, an order fact is added to the session. Once you have finished adding all facts, you are free to fire the rules engine and execute all rules.\nLet\u0026rsquo;s test it by sending the following HTTP request\n1 2 3 4 5 6 7 8 curl -X \u0026#39;POST\u0026#39; \\ \u0026#39;https://localhost:7139/api/Orders\u0026#39; \\ -H \u0026#39;accept: */*\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;total\u0026#34;: 10, \u0026#34;createdDate\u0026#34;: \u0026#34;2023-05-21T21:11:29.616Z\u0026#34; }\u0026#39; As a client of the API, I am creating an order with a grand total of $10, if this order is created on a Sunday, then I should get a 10 percent discount, $1, which means my grand total should actually be 9, well May 21st, 2023 happens to be a Sunday thus Sending the HTTP request to the API on Sunday yields the following response.\n1 2 3 4 { \u0026#34;total\u0026#34;: 9, \u0026#34;createdDate\u0026#34;: \u0026#34;2023-05-21T21:11:29.616Z\u0026#34; } My total is now 9 instead of 10. Perfect, this is great, NRules was easy to configure and easy to work with, there is one flaw with this approach, the rules and the rules engine exist together in the same code, meaning that if I wanted to change the rule to give discounts on Saturdays instead of Sunday, I would need to modify the code and redeploy the API.\nNRules does offer the ability for you to create your own custom rules repository, allowing you to load rules from outside the app, the trick here is that you would be responsible for translating those rules into expressions tree which involves parsing, tokenizing the rules and then finally generating the expression tree. Not an easy task, I\u0026rsquo;ve done something similar a few times with URL parameters, taking the incoming request, parsing it, tokenizing it then creating an expression tree to then give to an ORM framework like EF Core to do dynamic queries and even after doing it a few times I still make mistakes.\nRulesEngine Let\u0026rsquo;s now take a look at Microsoft\u0026rsquo;s rules engine, RulesEngine, here is the official overview of the project directly from their Wiki.\nRules Engine is a library/NuGet package for abstracting business logic/rules/policies out of a system. It provides a simple way of giving you the ability to put your rules in a store outside \u0026gt; the core logic of the system, thus ensuring that any change in rules doesn\u0026rsquo;t affect the core system.\nGreat, that is the exact limitation I just identified with NRules. To get started with RulesEngine install the library using Nuget.\n1 dotnet add package RulesEngine --version 4.0.0 With the RulesEngine package installed, I am going to migrate the rule I defined in NRules over to RulesEngine, here is what the DiscountRule class looks like after the migration.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public class DiscountRule { public static List\u0026lt;Rule\u0026gt; GetSundayDiscountRules() { var rules = new List\u0026lt;Rule\u0026gt;(); Rule sundayDiscountRule = new Rule { RuleName = \u0026#34;Discount Rule\u0026#34;, SuccessEvent = \u0026#34;Discount given on a Sunday\u0026#34;, ErrorMessage = \u0026#34;Discounts are only available on Sundays\u0026#34;, Expression = \u0026#34;CreatedDate.DayOfWeek == Sunday\u0026#34;, RuleExpressionType = RuleExpressionType.LambdaExpression }; rules.Add(sundayDiscountRule); return rules; } } The first thing to do is to create a rules container, this list will contain all the rules associated with giving out discounts to represent this rule a Rule is instantiated, a name, and message on success and error is given, then the expression and Lambda expression type, which as of May 2023 the only options is LambdaExpression. Next, the controller will be migrated as shown below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [Route(\u0026#34;api/[controller]\u0026#34;)] [ApiController] public class OrdersController : ControllerBase { [HttpPost] public async Task\u0026lt;IActionResult\u0026gt; CreateResourceAsync(Order order) { var discountWorkflows = new List\u0026lt;Workflow\u0026gt;(); Workflow discountWorkFlow = new Workflow(); discountWorkFlow.WorkflowName = \u0026#34;Sunday Discounts\u0026#34;; discountWorkFlow.Rules = DiscountRule.GetSundayDiscountRules(); discountWorkflows.Add(discountWorkFlow); var bre = new RulesEngine.RulesEngine(discountWorkflows.ToArray()); var rulesResult = await bre.ExecuteAllRulesAsync(discountWorkFlow.WorkflowName, order); return Created(\u0026#34;\u0026#34;, order); } } The first step is to create a container to hold all the workflows, workflows in RulesEngine are how rules are stored, it is essentially a JSON representation of the rule, for example, the Sunday discount rule we have been working with would be serialized and deserialized into the following JSON document.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \u0026#34;WorkflowName\u0026#34;: \u0026#34;Sunday Discounts\u0026#34;, \u0026#34;WorkflowsToInject\u0026#34;: null, \u0026#34;GlobalParams\u0026#34;: null, \u0026#34;Rules\u0026#34;: [ { \u0026#34;RuleName\u0026#34;: \u0026#34;Discount Rule\u0026#34;, \u0026#34;Properties\u0026#34;: null, \u0026#34;Operator\u0026#34;: null, \u0026#34;ErrorMessage\u0026#34;: \u0026#34;Discounts are only available on Sundays\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;RuleExpressionType\u0026#34;: \u0026#34;LambdaExpression\u0026#34;, \u0026#34;WorkflowsToInject\u0026#34;: null, \u0026#34;Rules\u0026#34;: null, \u0026#34;LocalParams\u0026#34;: null, \u0026#34;Expression\u0026#34;: \u0026#34;CreatedDate.DayOfWeek == Sunday\u0026#34;, \u0026#34;Actions\u0026#34;: null, \u0026#34;SuccessEvent\u0026#34;: \u0026#34;Discount is given on a Sunday.\u0026#34; } ] } If I were using an external storage system like a database, this is how the data would be saved to the database for this particular rule, since this example doesn\u0026rsquo;t deal with external storage, the rule is represented in the code shown above. So, we have a workflow, we give it a name, then the rules from the class DiscountRule are added to it, and the workflow is then passed to the engine for evaluation.\nNotice that, unlike NRules, RulesEngine only deals with rule validation, there is no logic here that says what to do after a rule is successfully executed. I will need to add additional logic to handle updating the order total. To get the result of a rule you will need to listen to the OnSuccess event which is fire for all successful events, meaning that in your event listener, you will need additional logic to filter on a specific event, currently the only option is on the name defined in the Rule itself, in my case my OnSuccess event name is Discount given on a Sunday.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [HttpPost] public async Task\u0026lt;IActionResult\u0026gt; CreateResourceAsync(Order order) { var discountWorkflows = new List\u0026lt;Workflow\u0026gt;() Workflow discountWorkFlow = new Workflow(); discountWorkFlow.WorkflowName = \u0026#34;Sunday Discounts\u0026#34;; discountWorkFlow.Rules = DiscountRule.GetSundayDiscountRules(); discountWorkflows.Add(discountWorkFlow) var bre = new RulesEngine.RulesEngine(discountWorkflows.ToArray()); var rulesResult = bre.ExecuteAllRulesAsync(discountWorkFlow.WorkflowName, order).Result rulesResult.OnSuccess((eventName) =\u0026gt; { if (eventName == \u0026#34;Discount given on a Sunday\u0026#34;) { var discount = (order.Total / 100) * 10; order.Total = order.Total - discount; }; }) return Created(\u0026#34;\u0026#34;, order); } Now that I am listening to the success event I can update the order total and apply my discount. Let\u0026rsquo;s see if the code works, I\u0026rsquo;ll run the API, and as done before I will send the following HTTP request.\n1 2 3 4 5 6 7 8 curl -X \u0026#39;POST\u0026#39; \\ \u0026#39;https://localhost:7139/api/Orders\u0026#39; \\ -H \u0026#39;accept: */*\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;total\u0026#34;: 0, \u0026#34;createdDate\u0026#34;: \u0026#34;2023-05-21T22:47:20.415Z\u0026#34; }\u0026#39; The API returned the following response.\n1 2 3 4 { \u0026#34;total\u0026#34;: 10, \u0026#34;createdDate\u0026#34;: \u0026#34;2023-05-21T23:05:58.914Z\u0026#34; } I got 10 instead of 9, upon checking and debugging the code the rule was never executed, tried it again a few times with the same result. As far I could tell I had the right code and rule structure, but nothing worked, I confirmed I had the right structure by changing the rule from \u0026ldquo;CreatedDate.DayOfWeek == Sunday\u0026rdquo; to \u0026ldquo;Total == 10\u0026rdquo;, I figure that maybe RulesEngine can handle nested structure or requires additional configuration to support nested structure. Sending the same HTTP request with the new rule worked, making me confident that I had written the right code, therefore, the problem was with the expression \u0026ldquo;CreatedDate.DayOfWeek == Sunday\u0026rdquo;.\nThat\u0026rsquo;s when it dawned on me, RulesEngine works with an expression tree so the expression \u0026ldquo;CreatedDate.DayOfWeek == Sunday\u0026rdquo; needs to be in an expression format, but what is the correct format to represent the expression? Luckily I picked up a trick from Shay Rojansky a while back on how to do this, when in doubt about what an expression should look like use the .NET compiler, how? Simply create the expression yourself as shown in the following code.\n1 Expression\u0026lt;Func\u0026lt;Order, bool\u0026gt;\u0026gt; x = d =\u0026gt; d.CreatedDate.DayOfWeek == DayOfWeek.Sunday; If I run the expression on the debugger and look at the DebugView I get the following code.\n1 2 3 .Lambda #Lambda1\u0026lt;System.Func`2[Shared.Order,System.Boolean]\u0026gt;(Shared.Order $d) { (System.Int32)($d.CreatedDate).DayOfWeek == 0 } Right, that\u0026rsquo;s when I realized I am an idiot, my expression should have been \u0026ldquo;CreatedDate.DayOfWeek == DayOfWeek.Sunday\u0026rdquo; or \u0026ldquo;CreatedDate.DayOfWeek == 0\u0026rdquo; not \u0026ldquo;CreatedDate.DayOfWeek == Sunday\u0026rdquo;. With the correct expression now in place I get the following response.\n1 2 3 4 { \u0026#34;total\u0026#34;: 9, \u0026#34;createdDate\u0026#34;: \u0026#34;2023-05-21T23:31:46.928Z\u0026#34; } Great, I got the expected result. So what have we learned? Well, RulesEngine solves the main problem identified with NRules, but there are three flaws in its approach, as I just show it is very easy for someone managing the rules to put the wrong expression on a rule. This flaw leads to the second flaw, in my experience, the people that manage business rules aren\u0026rsquo;t developers, they are tech-savvy, yes, but not expression trees savvy, I mean, someone managing rules with RulesEngine would need to know how to create expression tree and have an understanding in what the following means and does.\n1 2 3 { \u0026#34;Expression\u0026#34;: \u0026#34;input1.country == \\\u0026#34;india\\\u0026#34; AND input1.loyaltyFactor \u0026lt;= 2 AND input1.totalPurchasesToDate \u0026gt;= 5000 AND input2.totalOrders \u0026gt; 2 AND input3.noOfVisitsPerMonth \u0026gt; 2\u0026#34; } I wish RulesEngine has chosen a higher level syntax grammar something close enough to the following\n1 When an order is placed on Sunday, give a 10 percent discount. High-level grammar would allow almost anyone to be able to work add, and update rules. The third flaw I see has to do with how the OnSuccess and OnFail events are raised, I think it would have been better if instead of having an OnSuccess or an OnFail, RulesEngine allowed you to register an event type and then on OnSuccess or Failure the event type gets invoked, think of Mediatr and its notification implementation, something to that style would definitively enhance RulesEngine.\nCustom Rules Engine Caution, this is my own custom-hand-rolled rules engine, should you use it? No, you should go with NRules or RulesEngine or something else in another language, after all, why limit yourself to just dotnet? I\u0026rsquo;ve included my own version here for educational purposes.\nI have decided to create my custom rules engine based on How to Design Software — Rules Engines by Joseph Gefroh. Though I would have liked to have supported having a high-level language and having the rules decoupled from the engine, the task proved to be great for this blog post. Instead, I will attempt to tackle those features in a future post.\nNow let\u0026rsquo;s get into the code, here is my very simple rules engine.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class Rules \u0026lt;T\u0026gt; { private readonly T _type; public Rules(T type) { _type = type; } internal void Validate(params (dynamic Rule, string Parameter)[] validations) { foreach ((dynamic rule, string parameter) in validations) { if (rule.Condition) { rule.Effect(_type); } } } } My rules engine has a single method that accepts a collection of tuples, the first parameter being the rule and the second parameter is the property this rule applies to. Here is how to use it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class CustomOrderRules : Rules\u0026lt;Order\u0026gt; { public CustomOrderRules(Order order) : base(order) { Validate( (Rule: OrderCreatedOnASunday(order, applyDiscountAction), Parameter: nameof(order.CreatedDate)) ); } private static dynamic OrderCreatedOnASunday(Order order) =\u0026gt; new { Condition = order.CreatedDate.DayOfWeek == DayOfWeek.Wednesday, }; } The above code defines a validation rule, OrderCreatedOnASunday, for now, it only has the condition that determines if this rule should be applied, it is the same condition we have been using so far, the day of the week has to be Sunday. Next, I will need to define my Effect, that is the function that will be invoked as a result of this rule. The question is now how to best implement effects, for simplicity, I\u0026rsquo;ll use a callback function as shown below.\n1 2 3 4 5 private void ApplyDiscount(Order order) { var discount = (order.Total / 100) * 10; order.Total = order.Total - discount; } Next, I will update the OrderCreatedOnASunday method to accept an Action as a parameter.\n1 2 3 4 5 private static dynamic OrderCreatedOnASunday(Order order, Action\u0026lt;Order\u0026gt; action) =\u0026gt; new { Condition = order.CreatedDate.DayOfWeek == DayOfWeek.Wednesday, Effect = action, }; Finally, the constructor needs to handle the Action assignment.\n1 2 3 4 5 6 7 8 9 public CustomOrderRules(Order order) : base(order) { Action\u0026lt;Order\u0026gt; applyDiscountAction = ApplyDiscount; Validate( (Rule: OrderCreatedOnASunday(order, applyDiscountAction), Parameter: nameof(order.CreatedDate)) ); } Here is the complete code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class CustomOrderRules : Rules\u0026lt;Order\u0026gt; { public CustomOrderRules(Order order) : base(order) { Action\u0026lt;Order\u0026gt; applyDiscountAction = ApplyDiscount; Validate( (Rule: OrderCreatedOnASunday(order, applyDiscountAction), Parameter: nameof(order.CreatedDate)) ); } private static dynamic OrderCreatedOnASunday(Order order, Action\u0026lt;Order\u0026gt; action) =\u0026gt; new { Condition = order.CreatedDate.DayOfWeek == DayOfWeek.Wednesday, Effect = action, }; private void ApplyDiscount(Order order) { var discount = (order.Total / 100) * 10; order.Total = order.Total - discount; } } Great, time to test my custom rules engine, I\u0026rsquo;ve modified the controller to now use the CustomOrderRules class. Sending the same HTTP request as before.\n1 2 3 4 5 6 7 8 curl -X \u0026#39;POST\u0026#39; \\ \u0026#39;https://localhost:7139/api/Orders\u0026#39; \\ -H \u0026#39;accept: */*\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;total\u0026#34;: 10, \u0026#34;createdDate\u0026#34;: \u0026#34;2023-05-21T01:39:33.272Z\u0026#34; }\u0026#39; Yields the following response.\n1 2 3 4 { \u0026#34;total\u0026#34;: 9, \u0026#34;createdDate\u0026#34;: \u0026#34;2023-05-21T01:39:33.272Z\u0026#34; } Perfect, that is the expected result. The custom rules engine works, and even though it suffers from the same limitations as NRules and Rules Engine, I like its simplicity and flexibility. Adding a new rule is as simple as defining a new Effect, for example.\n1 2 3 4 5 private void OrderIsMoreThan100OfferDiscount(Order order) { var discount = (order.Total / 100) * 10; order.Total = order.Total - discount; } Then the new rule will use that effect.\n1 2 3 4 5 private static dynamic OrderIsMoreThan100(Order order, Action\u0026lt;Order\u0026gt; action) =\u0026gt; new { Condition = order.Total \u0026gt; 100, Effect = action, }; And finally using the rule.\n1 2 3 4 5 6 7 8 9 10 public CustomOrderRules(Order order) : base(order) { Action\u0026lt;Order\u0026gt; applyDiscountAction = ApplyDiscount; Action\u0026lt;Order\u0026gt; orderIsMoreThan100Action = OrderIsMoreThan100OfferDiscount; Validate( (Rule: OrderCreatedOnASunday(order, applyDiscountAction), Parameter: nameof(order.CreatedDate)), (Rule: OrderCreatedOnASunday(order, orderIsMoreThan100Action), Parameter: nameof(order.Total)) ); } Though it suffers from the same issues as NRule and Rules Engine, I do hope to come back to this project to make the following improvements.\nDecouple the rules from the engine. Facilitating storing the rules in an external system. Supporting high-level grammar, something along the lines of \u0026lsquo;When X is true then do Y\u0026rsquo;. Rules should support types, using dynamic as a POC works, but in reality, I want to offer type safety. The engine should offer a fluent style API, similar to NRules. The engine should support rule composition, for example, in FluentValidation you can create a validation rule that is composed of two other validation rules. I think such flexibility offers an amazing developer experience. Anyways, while I like NRules more, I have decided to go with Microsoft\u0026rsquo;s Rule Engine simply for the fact that the rules can live independently. Thus allowing an external app like an Admin tool to manage them.\n","permalink":"http://localhost:1313/post/2023/rule-engines-in-dotnet/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI am working on a project that requires the usage of rules engines to enforce business rules, I am unsure if I should roll out my own custom implementation, probably a bad idea, or if I should use an existing project.\u003c/p\u003e\n\u003cp\u003eTo help me make a decision I will need to look at the current options for rules engines available in .NET, I need to understand their capabilities and limitations. In .NET the most well-known rules engine is probably \u003ca href=\"https://nrules.net/\"\u003eNRules\u003c/a\u003e, the project has been around for some years and has good documentation. I also know that Microsoft created its own rules engine, \u003ca href=\"https://microsoft.github.io/RulesEngine/\"\u003eRulesEngine\u003c/a\u003e back in 2019. Then per \u003ca href=\"https://github.com/quozd/awesome-dotnet\"\u003eawesome-dotnet\u003c/a\u003e I could use \u003ca href=\"https://github.com/sang-hyeon/Plastic\"\u003ePlastic\u003c/a\u003e or \u003ca href=\"https://github.com/peasy/Peasy.NET\"\u003ePeasy.NET\u003c/a\u003e but I opted out on looking at those projects, for my use case I think NRules or RulesEngine will do.\u003c/p\u003e","title":"Rule Engines In .NET"},{"content":".NET has the ability to auto-generated OpenAPI specifications based on your code. You can use different decorators on your code like ProducesResponseTypeAttribute or ConsumesAttribute to produce more descriptive response details for web API help pages generated by tools like Swagger.\nWhat if you didn\u0026rsquo;t want to use the autogenerated spec, what if you instead wanted to expose an already written spec, perhaps because you follow an API-first approach to building an API instead of a Code-First approach. How would you expose that OpenAPI file?\nTurns out the answer is pretty straightforward. .NET already supports the ability to expose static files via the wwwroot folder, unlike an MVC project an API project does not come with the wwwroot folder, but that doesn\u0026rsquo;t mean you can\u0026rsquo;t add it yourself.\nSo if you wanted to expose an OpenAPI spec file or any other kind of file via an API add a new folder on under the Web API project and name the folder wwwroot.\nSimply putting the file there is not enough, .NET doesn\u0026rsquo;t expose files simply because they are located in the wwwroot folder, you have to enable file browsing by using the UseStaticFiles middleware in code, for example.\n1 2 3 4 5 6 7 8 9 var builder = WebApplication.CreateBuilder(args); var app = builder.Build(); app.UseHttpsRedirection(); app.UseStaticFiles(); // Enables file browsing on wwwroot. app.UseAuthorization(); app.Run(); With the middleware enabled you should be able to retrieve any files under wwwroot. Say that I place the OpenAPI file in the following file directory.\n1 wwwroot/Swagger/v1/swagger.json To retrieve the file by HTTP I would need to submit a request to the following URL.\n1 https://\u0026lt;hostname\u0026gt;/swagger/v1/swagger.json Something worth mentioning, .NET by default will only serve files that have a media type as defined in Media Types. If the file you are attempting to serve does not have a supported mime type, the API will return a 404. To bypass this feature you will need to use the FileExtensionContentTypeProvider class to add additional media type mappings.\nFor example, as of June 2023, YAML is not part of the media types supported by .NET because it does not have an official media type even though you may see apps like Google Chrome accept application/yaml and text/yaml, see this IANA Proposal to learn more. If attempt to expose a YAML file through we .NET Web API, the API will only serve a 404 NOT FOUND response. You would need to use the following code to expose any media type not currently supported by .NET.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 var builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddControllers(); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); var app = builder.Build(); var provider = new FileExtensionContentTypeProvider(); // Add new mappings provider.Mappings[\u0026#34;.yaml\u0026#34;] = \u0026#34;text/yaml\u0026#34;; // Allow YAML files to be served via HTTP request. app.UseStaticFiles(new StaticFileOptions { ContentTypeProvider = provider }); app.UseSwagger(); app.UseSwaggerUI(c=\u0026gt;c.SwaggerEndpoint(\u0026#34;/swagger/v1/swagger.yaml\u0026#34;, \u0026#34;v1\u0026#34;)); // Path to file location. app.UseHttpsRedirection(); app.UseAuthorization(); app.MapControllers(); app.Run(); With the code above you can now expose any file over HTTP, you can learn more about how .NET deals with files by reading the Static files in ASP.NET Core doc.\nQuick note, as of June 2023 .NET only supports OpenAPI version 3.0.n, anything higher will result in an error, see 795 for more details.\n","permalink":"http://localhost:1313/post/2023/use-custom-openapi-specification-file-in-.net/","summary":"\u003cp\u003e.NET has the ability to auto-generated \u003ca href=\"https://swagger.io/specification/\"\u003eOpenAPI specifications\u003c/a\u003e based on your code. You can use different decorators on your code like \u003ca href=\"https://learn.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.mvc.producesresponsetypeattribute?view=aspnetcore-7.0\"\u003eProducesResponseTypeAttribute\u003c/a\u003e or \u003ca href=\"https://learn.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.mvc.consumesattribute?view=aspnetcore-7.0\"\u003eConsumesAttribute\u003c/a\u003e to produce more descriptive response details for web API help pages generated by tools like Swagger.\u003c/p\u003e\n\u003cp\u003eWhat if you didn\u0026rsquo;t want to use the autogenerated spec, what if you instead wanted to expose an already written spec, perhaps because you follow an \u003ca href=\"https://blog.postman.com/what-is-api-first-design/\"\u003eAPI-first\u003c/a\u003e approach to building an API instead of a Code-First approach. How would you expose that OpenAPI file?\u003c/p\u003e","title":"Use Custom OpenAPI Specification File In .NET"},{"content":"Introduction In my blog post Integration Testing Using WebApplicationFactory I spoke about the benefits of testing a .NET Core Web API using WebApplicationFactory. The idea is that WebApplicationFactory creates a local HTTP server in-memory, meaning that when using WebApplicationFactory you are not mocking the HTTP request made to your API, you are actually using the API as if it were hosted in a live environment.\nThe benefit here is that your test code seats in the middle of the Web API and the client code calling the API, meaning you can now test how the API behaves under certain requests from the client. One drawback of using WebApplicationFactory would be having to mock API dependencies, for example, the database. A common option for .NET developers using a relational database like SQL Server is to use SQLite in the integration tests, however, even that solution suffers from other drawbacks, our friend Jimmy Bogard goes into more detail in his blog Avoid In-Memory Databases for Tests. What if instead of faking the database we actually used a real live database in our integration tests? There is a way, how? Well, with Docker.\nTestContainers Docker is an amazing tool, it has facilitated the rapid growth of many modern-day apps due to its ability to quickly provision up an application along with any dependents in a reliable, repeatable way. In our case, we can use Docker and a library known as testcontainers to create and run Docker containers within our integration tests. The Testcontainers for .NET acts as a wrapper around the existing .NET Docker APIs providing a lightweight implementation to support tests environments. Essentially, when we run the integration tests defined in our custom WebApplicationFactory we can spin up an external dependency like a SQL Server instance that can then be used by the API.\nHere is how it would work.\nI am going to take another project of mine, the Chinook Web API that implements the JSON:API specification, which already has integration tests that use WebApplicationFactory, I am going to change the project to now include a reference to the library testcontainers-dotnet. I\u0026rsquo;ll switch to the directory of the project, Chinook.Web.UnitTest and run the following command from the terminal.\n1 dotnet add package Testcontainers --version 3.0.0 With testcontainers-dotnet now installed in my test project I am going to write up a Docker file that creates a one-node SQLServer instance. The file will live in the same directory as the Test Project. Here is the file content.\nMSSQL Dockerfile 1 2 3 4 5 6 7 8 9 10 FROM mcr.microsoft.com/mssql/server:2019-latest ENV ACCEPT_EULA=Y ENV MSSQL_SA_PASSWORD=Test@12345 ENV MSSQL_PID=Express ENV MSSQL_TCP_PORT=1433 EXPOSE 1433 # Default port of SQL Server CMD [\u0026#34;/opt/mssql/bin/sqlservr\u0026#34;] #Start SQL Server Note that the above Dockerfile only sets the password, the user will be the default SQL Server user SA. The next step is to bootstrap the Docker build and run processes into the integration tests, this is where I originally went down the wrong path. What I did was to add the code that builds and runs docker into my custom WebApplicationFactory since that class is responsible for bootstrapping my service, I ended up with this code.\nWrong WebApplicationFactory 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 public class CustomWebApplicationFactory : WebApplicationFactory\u0026lt;Program\u0026gt; { protected override void ConfigureWebHost(IWebHostBuilder builder) { builder.ConfigureServices(AddServices); } private void AddServices(IServiceCollection serviceCollection) { try { var projectDirectory = CommonDirectoryPath.GetProjectDirectory(); var containerImage = new ImageFromDockerfileBuilder() .WithDockerfileDirectory(projectDirectory, string.Empty) .WithDockerfile(\u0026#34;Dockerfile\u0026#34;) .WithName(\u0026#34;mssql-application-factory\u0026#34;) .Build(); containerImage .CreateAsync() .ConfigureAwait(false); const int mssqlBindPort = 1433; var sqlServerContainer = new ContainerBuilder() .WithImage(\u0026#34;mssql-application-factory\u0026#34;) .WithPortBinding(mssqlBindPort, mssqlBindPort) .Build(); sqlServerContainer .StartAsync() .ConfigureAwait(false); var dbContextService = serviceCollection.SingleOrDefault(d =\u0026gt; d.ServiceType == typeof(DbContextOptions\u0026lt;ChinookDbContext\u0026gt;)); if (dbContextService != null) { // remove the DbContext that is registered on StartUp.cs serviceCollection.Remove(dbContextService); } // register the new DbContext, .NET Core dependency injection framework will now use the in-memory SQLite instance instead of whatever configuration was used to register the DbContext on the StartUp class. var sqliteInMemoryConnectionString = \u0026#34;Server=localhost;User Id=SA;Password=Test@12345;\u0026#34;; serviceCollection.AddDbContext\u0026lt;ChinookDbContext\u0026gt;(contextOptions =\u0026gt; contextOptions.UseSqlServer(sqliteInMemoryConnectionString)); var builtServiceProvider = serviceCollection.BuildServiceProvider(); using var scopedProvider = builtServiceProvider.CreateScope(); var scopedServiceProvider = scopedProvider.ServiceProvider; // private field omitted for brevity var chinookDbContext = scopedServiceProvider.GetRequiredService\u0026lt;ChinookDbContext\u0026gt;(); // these two lines are important, they ensure the in-memory database is created now. chinookDbContext.Database.OpenConnection(); chinookDbContext.Database.EnsureCreated(); // database is now ready to be seeded through the DbContext. The data will be available in each of your integration test due to the scope of the DbContext. } catch (Exception) { throw; } } } The code above is flawed and never worked, can you spot my mistake? The build interface exposed by the TestContainers library is all asynchronous while the AddServices is not, and while yes, I can change the interface of my AddServices from void to async Task and the project will build and run, but by changing the interface, the method will not be called by ConfigureServices, which makes that solution not viable.\nI needed another way to bootstrap my containers, and that\u0026rsquo;s when I realized I\u0026rsquo;m using XUnit as my testing framework, I can use the IAsyncLifetime interface to bootstrap the Docker container, using IAsyncLifetime, you can define code that XUNit will execute as soon as your test class is created, I do want to make whatever I define reusable, therefore, instead of having my test class implement IAsyncLifetime I will create a SqlServerContainerImage class that implements the IAsyncLifetime, then whenever a test class needs a SQL Server it can inherit from SqlServerContainerImage.\nSqlServerContainerImage Class 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 public class SqlServerContainerImage : IAsyncLifetime { private readonly IFutureDockerImage _sqlServerContainerImage; private readonly IContainer _sqlServerContainer; public SqlServerContainerImage() { var projectDirectory = CommonDirectoryPath.GetProjectDirectory(); _sqlServerContainerImage = new ImageFromDockerfileBuilder() .WithDockerfileDirectory(projectDirectory, string.Empty) .WithDockerfile(\u0026#34;Dockerfile\u0026#34;) .WithName(\u0026#34;mssql-application-factory\u0026#34;) .Build(); const int mssqlBindPort = 1433; _sqlServerContainer = new ContainerBuilder() .WithImage(\u0026#34;mssql-application-factory\u0026#34;) .WithPortBinding(mssqlBindPort, mssqlBindPort) .WithWaitStrategy(Wait.ForUnixContainer().UntilPortIsAvailable(mssqlBindPort)) // Don\u0026#39;t forget this line, it ensure the container is running before the test is executed. .Build(); } /// \u0026lt;summary\u0026gt; /// Clean up process, delete images and containers /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; public async Task DisposeAsync() { await _sqlServerContainerImage .DeleteAsync() .ConfigureAwait(false); await _sqlServerContainer .DisposeAsync() .ConfigureAwait(false); } /// \u0026lt;summary\u0026gt; /// Bootstrap process, create images and containers. /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; public async Task InitializeAsync() { await _sqlServerContainerImage .CreateAsync() .ConfigureAwait(false); await _sqlServerContainer .StartAsync() .ConfigureAwait(false); } } Now I will need to modify my HomeControllerIntegrationTest originally created in Integration Testing Using WebApplicationFactory. Here is the updated class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public class HomeControllerIntegrationTest : SqlServerContainerImage, IClassFixture\u0026lt;CustomWebApplicationFactory\u0026gt; { private readonly CustomWebApplicationFactory _customWebApplicationFactory; public HomeControllerIntegrationTest(CustomWebApplicationFactory customWebApplicationFactory) { _customWebApplicationFactory = customWebApplicationFactory; } [Fact] public async Task GetHomeResource_HttpResponse_ShouldReturn200OK() { // Arrange using var httpClient = _customWebApplicationFactory.CreateClient(); var requestUri = httpClient.BaseAddress.AbsoluteUri; // Act var sut = await httpClient.GetAsync(requestUri); // Assert var responseCode = sut.StatusCode; responseCode.Should().Be(HttpStatusCode.OK); } } As you can see the test class now inherits from SqlServerContainerImage, and here is the final class definition for my custom WebApplicationFactory.\nCorrect SqlServerContainerImage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 public class CustomWebApplicationFactory : WebApplicationFactory\u0026lt;Program\u0026gt; { protected override void ConfigureWebHost(IWebHostBuilder builder) { builder.ConfigureServices(AddServices); } private void AddServices(IServiceCollection serviceCollection) { try { var dbContextService = serviceCollection.SingleOrDefault(d =\u0026gt; d.ServiceType == typeof(DbContextOptions\u0026lt;ChinookDbContext\u0026gt;)); if (dbContextService != null) { // remove the DbContext that is registered on StartUp.cs serviceCollection.Remove(dbContextService); } // register the new DbContext, .NET Core dependency injection framework will now use the in-memory SQLite instance instead of whatever configuration was used to register the DbContext on the StartUp class. var sqliteInMemoryConnectionString = \u0026#34;Server=localhost;User Id=SA;Password=Test@12345;\u0026#34;; serviceCollection.AddDbContext\u0026lt;ChinookDbContext\u0026gt;(contextOptions =\u0026gt; contextOptions.UseSqlServer(sqliteInMemoryConnectionString)); var builtServiceProvider = serviceCollection.BuildServiceProvider(); using var scopedProvider = builtServiceProvider.CreateScope(); var scopedServiceProvider = scopedProvider.ServiceProvider; // private field omitted for brevity var chinookDbContext = scopedServiceProvider.GetRequiredService\u0026lt;ChinookDbContext\u0026gt;(); // these two lines are important, they ensure the in-memory database is created now. chinookDbContext.Database.OpenConnection(); chinookDbContext.Database.EnsureCreated(); // database is now ready to be seeded through the DbContext. The data will be available in each of your integration test due to the scope of the DbContext. } catch (Exception) { throw; } } } Time to run a few tests to make sure everything is configured correctly, I\u0026rsquo;m going to add a breakpoint in the DisposeAsync method that is located in SqlServerContainerImage to prevent the container from being destroyed, I want to connect to the container to confirm that everything was configured correctly. I will know that everything is configured correctly if I see the database has all the tables defined by the DbContext.\nI\u0026rsquo;ll run the test GetHomeResource_HttpResponse_ShouldReturn200OK() and hit the break now to prevent the container and image from being destroyed.\nNow I need to connect to the test container, first I\u0026rsquo;ll need to know the name, to do so, run the following command to get all running containers.\n1 docker ps -a In my case, it outputs the following.\n1 2 3 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f0ba216b827b mssql-application-factory:latest \u0026#34;/opt/mssql/bin/perm…\u0026#34; 2 minutes ago Up 2 minutes 0.0.0.0:1433-\u0026gt;1433/tcp objective_chandrasekhar 2fb31c1cbaac testcontainers/ryuk:0.3.4 \u0026#34;/app\u0026#34; 2 minutes ago Up 2 minutes 0.0.0.0:32768-\u0026gt;8080/tcp testcontainers-ryuk-3427352d-4a88-4b53-9a17-7c0008fa04fe The name of my test container is objective_chandrasekhar. Great, now run the following command.\n1 docker exec -it objective_chandrasekhar \u0026#34;bash\u0026#34; Connect to SQL Sever You should now be in the bash terminal of the container, run the following command in bash to get into sqlcmd.\n1 /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P \u0026#34;Test@12345\u0026#34; Note that the user is SA because I never defined a user in my Dockerfile, only a password which you can see it is part of the command.\nHere is what my shell looks like so far.\n1 2 3 $ docker exec -it objective_chandrasekhar \u0026#34;bash\u0026#34; mssql@f0ba216b827b:/$ /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P \u0026#34;Test@12345\u0026#34; 1\u0026gt; In the last line 1\u0026gt;, if you see that it means that you have successfully connected to the database and have sqlcmd running. Time to see if the databases were created, I can do that by running the following command.\n1 SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE=\u0026#39;BASE TABLE\u0026#39; You should now see a new line with 2\u0026gt;. Now run the following command.\n1 GO The command executes the first command which should output all the base tables that currently exist on the database.\n1 2 3 4 5 6 7 8 9 10 11 12 TABLE_CATALOG TABLE_SCHEMA TABLE_NAME TABLE_TYPE --------------- ----------------- ----------------- ---------- master dbo artists BASE TABLE master dbo employees BASE TABLE master dbo genres BASE TABLE master dbo media_types BASE TABLE master dbo playlists BASE TABLE master dbo customers BASE TABLE master dbo tracks BASE TABLE master dbo invoices BASE TABLE master dbo playlist_track BASE TABLE master dbo invoice_items BASE TABLE Great, I see all the entities defined in my Chinook DbContext. That means that everything is configured correctly and it also means this approach to using Docker to create external dependencies will work, the test passed and the container and the image were successfully deleted from Docker.\nConclusion Awesome, using test containers is going to be a game changer for integrations tests I encourage you to learn more over at the official site and to check out their best practices and examples their examples.\nUntil next time, cheerio!\n","permalink":"http://localhost:1313/post/2023/power-up-integration-tests-with-tests-containers/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eIn my blog post \u003ca href=\"/post/2021/integration-testing-using-webapplicationfactory/\"\u003eIntegration Testing Using WebApplicationFactory\u003c/a\u003e I spoke about the benefits of testing a .NET Core Web API using \u003ca href=\"https://learn.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.mvc.testing.webapplicationfactory-1?view=aspnetcore-7.0\"\u003eWebApplicationFactory\u003c/a\u003e. The idea is that WebApplicationFactory creates a local HTTP server in-memory, meaning that when using WebApplicationFactory you are not mocking the HTTP request made to your API, you are actually using the API as if it were hosted in a live environment.\u003c/p\u003e\n\u003cp\u003eThe benefit here is that your test code seats in the middle of the Web API and the client code calling the API, meaning you can now test how the API behaves under certain requests from the client. One drawback of using WebApplicationFactory would be having to mock API dependencies, for example, the database. A common option for .NET developers using a relational database like SQL Server is to use SQLite in the integration tests, however, even that solution suffers from other drawbacks, our friend Jimmy Bogard goes into more detail in his blog \u003ca href=\"https://jimmybogard.com/avoid-in-memory-databases-for-tests/\"\u003eAvoid In-Memory Databases for Tests\u003c/a\u003e. What if instead of faking the database we actually used a real live database in our integration tests? There is a way, how? Well, with Docker.\u003c/p\u003e","title":"Power Up Integration Tests With Test Containers"},{"content":"Over the last few months, I have been brushing up on API testing, specifically around contract testing.\nAccording to Postman, an API contract is a human- and machine-readable representation of an API\u0026rsquo;s intended functionality. It establishes a single source of truth for what each request and response should look like—and forms the basis of service-level agreements (SLAs) between producers and consumers. API contract testing helps ensure that new releases don\u0026rsquo;t violate the contract by checking the content and format of requests and responses.\nIf you begin to walk down the road of API contrast testing, one question that naturally comes up is, what tools can I use to identify breaking changes? After all, reviewing a large YAML file on a pull request is not necessarily the best way for humans to spot breaking changes to API contracts.\nWhile researching API Contract testing I came across two tools that help identify breaking changes, Akita and Optic. In today\u0026rsquo;s post, I would like to explore Optic and demonstrate some of its capabilities and how it can be used to identify breaking changes.\nAccording to their documentation, Optic is a version of control tool, just like Git. In fact, it relies on Git in order to find breaking changes, it invokes a git show command to be able to see a diff between OpenAPI spec files across branches, which means you can use Optic in your existing pull request and since Optic is an NPM package you can import it into any CI provider.\nTo get started, installed the Optic CLI using NPM.\n1 npm -g install @useoptic/optic As of April 2023, the command above will install version 0.42.13. With Optic now installed you can use the diff command to compare two OpenAPI specs. Below you will find an OpenAPI specification file that I have created for this demo.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 openapi: 3.1.0 x-stoplight: id: tdmzx0hqhylls info: title: Users version: \u0026#39;1.0\u0026#39; servers: - url: \u0026#39;http://localhost:3000\u0026#39; paths: \u0026#39;/users/{userId}\u0026#39;: parameters: - schema: type: integer name: userId in: path required: true description: Id of an existing user. get: summary: Get User Info by User ID tags: [] responses: \u0026#39;200\u0026#39;: description: User Found content: application/json: schema: $ref: \u0026#39;#/components/schemas/User\u0026#39; examples: Get User Alice Smith: value: id: 142 firstName: Alice lastName: Smith email: alice.smith@gmail.com dateOfBirth: \u0026#39;1997-10-31\u0026#39; emailVerified: true signUpDate: \u0026#39;2019-08-24\u0026#39; \u0026#39;404\u0026#39;: description: User Not Found operationId: get-users-userId description: Retrieve the information of the user with the matching user ID. components: schemas: User: title: User type: object description: \u0026#39;\u0026#39; examples: - id: 142 firstName: Alice lastName: Smith email: alice.smith@gmail.com dateOfBirth: \u0026#39;1997-10-31\u0026#39; emailVerified: true signUpDate: \u0026#39;2019-08-24\u0026#39; properties: id: type: integer description: Unique identifier for the given user. firstName: type: string lastName: type: string email: type: string format: email dateOfBirth: type: string format: date example: \u0026#39;1997-10-31\u0026#39; emailVerified: type: boolean description: Set to true if the user\u0026#39;s email has been verified. createDate: type: string format: date description: The date that the user was created. required: - id - firstName - lastName - email - emailVerified Might be hard to see from the YAML, but basically, the API defined in the YAML is an API with a /users endpoint, an HTTP client can send a GET request to retrieve a specific user. The users resource exposes the following model.\n1 2 3 4 5 6 { \u0026#34;firstName\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Fellow\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;bob.fellow@gmail.com\u0026#34;, \u0026#34;dateOfBirth\u0026#34;: \u0026#34;1996-08-24\u0026#34; } Let\u0026rsquo;s assume I want to change the API, say I want to rename the field dateOfBirth to dob, breaking the existing contract exposed by the API, I will modify the contract and save my changes in a change.yaml file and the existing contract will be saved to a main.yaml file. To have Optic identify breaking changes on the contract I can use the following command.\n1 optic diff main.yaml change.yaml The command above results in the following output.\n1 2 3 4 5 GET /users/{userId}: - response 200: - body application/json: - /schema/properties/dob added - /schema/properties/dateOfBirth removed Don\u0026rsquo;t know if you can tell from the output, but Optic has successfully identified the change, appending a \u0026ndash;web to the command we ran will output the results in a web browser for better visual aid as seen in the screenshot below.\nYou can also run Optic across git branches using the diff command, you simply would change to the branch containing the API changes and then run the command.\n1 optic diff openapi.yml --base main --check --web Where \u0026ndash;base is the name of the main branch, in this case, the main branch is called main. The \u0026ndash;check parameter tells Optic to run the default breaking changes. See optic also offers the ability to apply rules, allowing you to create API standards, for example, imagine that you wanted all properties to be written using snake casing. In optic, you would define the properties casing as snake casing then you would run the rules against all pull requests. Let\u0026rsquo;s see how that would work, you will find a custom rule definition, I want all my properties to follow snake casing because it is easier to read than camel casing.\n1 2 3 4 ruleset: # Enforce consistent cases in your API - naming: properties: snake_case Now that I have my rule I can apply it to the previous command.\n1 optic diff main.yaml change.yaml --check --ruleset ./ruleset.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 FAIL GET /users/{userId} requirement rule [error]: response property naming check x firstName is not snake_case at paths \u0026gt; /users/{userId} \u0026gt; get \u0026gt; responses \u0026gt; 200 \u0026gt; content \u0026gt; application/json \u0026gt; schema \u0026gt; properties \u0026gt; firstName requirement rule [error]: response property naming check x lastName is not snake_case at paths \u0026gt; /users/{userId} \u0026gt; get \u0026gt; responses \u0026gt; 200 \u0026gt; content \u0026gt; application/json \u0026gt; schema \u0026gt; properties \u0026gt; lastName requirement rule [error]: response property naming check x emailVerified is not snake_case at paths \u0026gt; /users/{userId} \u0026gt; get \u0026gt; responses \u0026gt; 200 \u0026gt; content \u0026gt; application/json \u0026gt; schema \u0026gt; properties \u0026gt; emailVerified requirement rule [error]: response property naming check x createDate is not snake_case at paths \u0026gt; /users/{userId} \u0026gt; get \u0026gt; responses \u0026gt; 200 \u0026gt; content \u0026gt; application/json \u0026gt; schema \u0026gt; properties \u0026gt; createDate 3 operations changed 0 passed 29 errors Optic has identified all properties that do not follow the naming convention defined by my rule. This is great, I can now enforce naming conventions across all resources on my users API.\nOn an upcoming blost I will explore Akita, the second tools I found for API contract testing, till next time.\nCheerio.\n","permalink":"http://localhost:1313/post/2023/optic/","summary":"\u003cp\u003eOver the last few months, I have been brushing up on \u003ca href=\"https://www.postman.com/api-platform/api-testing/\"\u003eAPI testing\u003c/a\u003e, specifically around contract testing.\u003c/p\u003e\n\u003cp\u003eAccording to Postman, an API contract is a human- and machine-readable representation of an API\u0026rsquo;s intended functionality. It establishes a single source of truth for what each request and response should look like—and forms the basis of service-level agreements (SLAs) between producers and consumers. API contract testing helps ensure that new releases don\u0026rsquo;t violate the contract by checking the content and format of requests and responses.\u003c/p\u003e","title":"Tracking API Changes With Optic"},{"content":"Recently TypeScript wizard Matt Pocock made a Twitter thread on branded types. At first, I did not know what he was talking about, I thought it was a new TypeScript feature being introduced in TypeScript 5 but upon closer look, I realized that it was not a new feature but rather a technique that I already knew, opaque types.\nI first learned about opaque types from Evert Pot in his blog post Implementing an opaque type in typescript, though I guess now the TypeScript community prefers to call them branded types, the name doesn\u0026rsquo;t matter, the problem being solved is the same, preventing types from being interchangeable.\nFor example, take a look at the following code.\n1 2 3 4 5 6 7 8 9 10 11 type DepositAmount = number; type WithdrawAmout = number; function deposit(amount: DepositAmount) { console.log(`Adding ${amount} money...`) } const depositAmount: DepositAmount = 100 deposit(depositAmount); In the code above a DepositAmount and WithdrawAmout type are declared, assume that the deposit function is part of some Web API run by a bank. In this scenario, we create a variable depositAmount, assign it a value of 100 and pass it as a parameter to the deposit function. Pretty standard stuff, nothing new here. The code does have one flaw, it is possible for a developer to accidentally do the following.\n1 2 3 const depositAmount: WithdrawAmout = 100 deposit(depositAmount); The variable depositAmount is now of type WithdrawAmout while the deposit function still expects a type of DepositAmount. Changing the variable depositAmount to be of type WithdrawAmout has no impact on the program, the result is the same, try it yourself. The reason why this change has no impact on the way the API runs is due to how TypeScript infers type, see to TypeScript the type DepositAmount and WithdrawAmout are the same. While the code continues to work as before, we as humans know that the functionality is wrong, a type WithdrawAmout being passed to a function that expects a type of DepositAmount doesn\u0026rsquo;t make a whole lot of sense.\nThis is where Branded/Opaque types come in handy, a branded type is not interchangeable. Here is how it would work in the example above. First, declare a brand as a unique symbol type.\n1 declare const brand: unique symbol; A symbol is a type that can never be created because it is a primitive type, just like numbers and strings. See Symbols\nNow I\u0026rsquo;ll declare my branded type and I\u0026rsquo;ll update DepositAmount and WithdrawAmout.\n1 2 3 4 type Brand\u0026lt;T, TBran extends string\u0026gt; = T \u0026amp; {[brand] : TBran}; type DepositAmount = Brand\u0026lt;number, \u0026#34;DepositAmount\u0026#34;\u0026gt;; type WithdrawAmout = Brand\u0026lt;number, \u0026#34;WithdrawAmout\u0026#34;\u0026gt;; The original DepositAmount \u0026amp; WithdrawAmout are now branded types. The rest of the code is updated as shown below.\n1 2 3 4 5 6 7 8 function deposit(amount: DepositAmount) { console.log(`Adding ${amount} money...`) } const depositAmount: DepositAmount = 100 as DepositAmount; deposit(depositAmount); Now if someone attempts to use the deposit function but with a variable of type WithdrawAmout, TypeScript will produce the following error.\n1 // Argument of type \u0026#39;WithdrawAmout\u0026#39; is not assignable to parameter of type \u0026#39;DepositAmount\u0026#39;. Thus ensuring that only a variable of type DepositAmount is ever passed to the deposit function.\nIf you prefer to not have to cast then use a type predicate function as shown below.\n1 2 const isValidDepositAmount = (amount: number) : amount is DepositAmount =\u0026gt; { return amount \u0026gt; 0 } const isValidWithdrawAmount = (amount: number) : amount is WithdrawAmount =\u0026gt; { return amount \u0026lt; 0} Then the code can be updated.\n1 2 3 4 5 const depositAmount = 100; if (isValidDepositAmount(depositAmount)){ deposit(depositAmount); // variable depositAmount is valid. } And if someone were to assign an invalid value like -100 to depositAmount then the deposit method will never be invoked.\n1 2 3 4 5 const depositAmount = -100; if (isValidDepositAmount(depositAmount)){ deposit(depositAmount); // will never be executed because -100 cannot be cast to DepositAmount in isValidDepositAmount. } If you are interested in learning more, I recommend becoming a TypeScript Wizard.\n","permalink":"http://localhost:1313/post/2023/stricter-types-in-typescript/","summary":"\u003cp\u003eRecently TypeScript \u003ca href=\"https://www.totaltypescript.com/\"\u003ewizard\u003c/a\u003e \u003ca href=\"https://twitter.com/mattpocockuk\"\u003eMatt Pocock\u003c/a\u003e made a \u003ca href=\"https://twitter.com/mattpocockuk/status/1625173884885401600\"\u003eTwitter thread\u003c/a\u003e on branded types. At first, I did not know what he was talking about, I thought it was a new TypeScript feature being introduced in \u003ca href=\"https://devblogs.microsoft.com/typescript/announcing-typescript-5-0-beta/\"\u003eTypeScript 5\u003c/a\u003e but upon closer look, I realized that it was not a new feature but rather a technique that I already knew, opaque types.\u003c/p\u003e\n\u003cp\u003eI first learned about opaque types from \u003ca href=\"https://evertpot.com/opaque-ts-types/\"\u003eEvert Pot\u003c/a\u003e in his blog post \u003ca href=\"https://evertpot.com/opaque-ts-types/\"\u003eImplementing an opaque type in typescript\u003c/a\u003e, though I guess now the TypeScript community prefers to call them branded types, the name doesn\u0026rsquo;t matter, the problem being solved is the same, preventing types from being interchangeable.\u003c/p\u003e","title":"Stricter Types In TypeScript"},{"content":"In the last few months, my Twitter feed has been dominated by one topic, tRPC. tRPC is a library that provides type-safety between your front end and backend, in theory, it allows you to quickly build applications.\nIn today\u0026rsquo;s post, I would like to explore tRPC, its capabilities and features, and how it could be used in a project. To get started, I will create a new Node.js app using Express. If you prefer to use React or Next.js see the official docs.\nPrerequisites To get started, I\u0026rsquo;m going to initiate a new Node project using the following command.\n1 npm init The command will create a package.json file for your application, the command will prompt a series of questions, like the package name, version, description, and so on. See package.json for more information. Feel free to stick with the default information.\nNext, I need to install express, it can be done using the following command.\n1 npm install express There are additional dependencies required for this project. I\u0026rsquo;m going to need ts-node, TypeScript, and nodemon to be able to build this app using TypeScript, to install these dependencies run the following commands.\n1 npm i typescript ts-node nodemon --save-dev Don\u0026rsquo;t forget to include @types/express.\n1 npm i @types/node @types/express Now I need to set my folder structure given that a client and server will be required for this demo, I am going to create a client and server folder to host the files corresponding to each app. I will also need a tsconfig.json to set tRPC to run in a strict mode as tRPC only support strict mode. The following tsconfig file should do the trick.\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es6\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;rootDir\u0026#34;: \u0026#34;./\u0026#34;, \u0026#34;outDir\u0026#34;: \u0026#34;./build\u0026#34;, \u0026#34;esModuleInterop\u0026#34;: true, \u0026#34;strict\u0026#34;: true } } I\u0026rsquo;ll run the npm init command again but this time within the client folder and I\u0026rsquo;ll add an index.ts file to the directory. Then I\u0026rsquo;ll switch over to the server directory, and add an index.ts here as well.\nHere is the project structure as of now.\n1 2 3 4 5 6 7 8 9 root client -index.ts -package.json -/node_modules server -index.ts -package.json -/node_modules Your server directory doesn\u0026rsquo;t need scripts, a package.json, or any node_modules as they will be available globally from the root directory.\nBack in the server directory, open the index.ts file and place the following code there.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import express from \u0026#39;express\u0026#39;; const app: express.Application = express(); const port: number = 3000; app.get(\u0026#39;/\u0026#39;, (_req, _res) =\u0026gt; { _res.send(\u0026#34;TypeScript With Express\u0026#34;); }); app.listen(port, () =\u0026gt; { console.log(`TypeScript with Express http://localhost:${port}/`); }); The code above is a sample express app, to run it open the package.json located in the root directory and add the following under the script.\n1 2 3 4 5 { \u0026#34;scripts\u0026#34;: { \u0026#34;server\u0026#34;: \u0026#34;nodemon --quiet ./server/index.ts\u0026#34;, } } Now to confirm that everything has been installed and configured correctly run the following command.\n1 npm run server The server is now listening for traffic on port 3000. To confirm, run the following command.\n1 curl http://localhost:3000 You should then see \u0026ldquo;TypeScript With Express\u0026rdquo; as the API response. If you did, then good, it means you have everything installed correctly.\ntRPC Time to configure tRPC.\nFirst, I\u0026rsquo;m going to need to define a router. What is a router? In tRPC a router acts like an API resource, something like \u0026ldquo;customers\u0026rdquo;, I come from a .NET background, and a router to me is a controller. In .NET you would have a \u0026ldquo;CustomerController\u0026rdquo; class.\nI\u0026rsquo;ll add a router.ts file under the server directory. Here is the content of that file.\n1 2 3 4 5 6 import { initTRPC } from \u0026#39;@trpc/server\u0026#39;; export const t = initTRPC.create(); export const appRouter = t.router({}); export type AppRouter = typeof appRouter; As you can see from the code above, I imported tRPC, initiated the library by calling the create() method, and then created a router with no procedures. A procedure is an endpoint exposed within our router. For example, in REST you may have a resource called orders and that resource might be exposed through an endpoint like /orders, the procedure is that endpoint.\nA quick note on tRPC, the library should be instantiated once per app having multiple instances will cause issues. So make sure to only call create() once.\nNext, I am going to add a context, a tRPC context to me feels like an abstraction over the concept of a request middleware, but at the same time it doesn\u0026rsquo;t feel like a request/response middleware. A context can be used to share data or logic with all the procedures, a common example is authentication. Before your procedure is invoked, the request would reach the context which can then validate that the user or client is authorized to make the request.\nSince I don\u0026rsquo;t want to deal with authentication in this example, I\u0026rsquo;m going to keep my context empty as shown in the following code snippet.\n1 2 3 4 5 6 7 8 9 import { inferAsyncReturnType } from \u0026#39;@trpc/server\u0026#39;; import * as trpcExpress from \u0026#39;@trpc/server/adapters/express\u0026#39;; export const createContext = ({ req, res, }: trpcExpress.CreateExpressContextOptions) =\u0026gt; ({}); // no context export type Context = inferAsyncReturnType\u0026lt;typeof createContext\u0026gt;; The last thing I need to do now is to modify the index.ts file, I need to replace the code that exists there with one that uses the context and router created above.\nHere is the updated index.ts file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import { initTRPC } from \u0026#39;@trpc/server\u0026#39;; import express from \u0026#39;express\u0026#39;; import { Context, createContext } from \u0026#39;./server/context\u0026#39;; import * as trpcExpress from \u0026#39;@trpc/server/adapters/express\u0026#39;; import { appRouter } from \u0026#39;./server/router\u0026#39;; const t = initTRPC.context\u0026lt;Context\u0026gt;().create(); const app = express(); const port: number = 3000; app.use( \u0026#39;/trpc\u0026#39;, trpcExpress.createExpressMiddleware({ router: appRouter, createContext, }), ); app.listen(port); In the code above we use a helper function to inject tRPC into the Express middleware pipelines. The helper function takes the router and context created above. Then the API listens for traffic in port 3000. The time has come to expose some procedures. I\u0026rsquo;m going to create a procedure that supports getting a list of cars and another procedure that supports adding a new car.\nBack in the router.ts file, I am going to add two new endpoints \u0026ldquo;getCars\u0026rdquo; and \u0026ldquo;createCar\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import { initTRPC } from \u0026#39;@trpc/server\u0026#39;; export const t = initTRPC.create(); export const appRouter = t.router({ getCars: t.procedure .query((req) =\u0026gt; { return [{ id: 1, name: \u0026#39;Toyota\u0026#39;, }, { id: 2, name: \u0026#39;Honda\u0026#39;, }]; }), createCar: t.procedure .mutation(async (req) =\u0026gt; { return { name: \u0026#39;Nissan\u0026#39; } }), }); export type AppRouter = typeof appRouter; Let\u0026rsquo;s examine the new endpoints, the first procedure exposed is \u0026ldquo;getCars\u0026rdquo;, it is then mapped to a query function. One thing to note about procedures in tRPC is that they are case-sensitive, meaning the following request will work.\n1 curl http://api.example.com/trpc/getCars but the following request will fail.\n1 curl http://api.example.com/trpc/getcars It will return the following JSON document.\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;error\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;No \\\u0026#34;query\\\u0026#34;-procedure on path \\\u0026#34;getcars\\\u0026#34;\u0026#34;, \u0026#34;code\u0026#34;: -32004, \u0026#34;data\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;NOT_FOUND\u0026#34;, \u0026#34;httpStatus\u0026#34;: 404, \u0026#34;stack\u0026#34;: \u0026#34;Full stack trace ommited for brevity\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;getcars\u0026#34; } } } The document doesn\u0026rsquo;t appear to be a Problem Details document, I would love it if as developers we stopped reinventing the wheel and used known standards.\nThe next thing to note is that tRPC uses query, mutation and subcriptions like GraphQL. In fact, by their own admittion, tRPC borrows a few ideas from GraphQL. Since our \u0026ldquo;getCars\u0026rdquo; is meant to retrieve data I mapped it to a query function. The query function itself doesn\u0026rsquo;t do much in my example, it simply returns an in-memory collection of cars. In a real-world application, the in-memory collection would be replaced with your data abstraction logic, i.e. repositories.\nThe second procedure exposes a \u0026ldquo;createCar\u0026rdquo; endpoint, this endpoint accepts an HTTP POST request, just like the \u0026ldquo;getCars\u0026rdquo; procedure, and it turns an in-memory object. Something else to note here is that like GraphQL, tRPC does not use the HTTP PUT, PATCH, or DELETE, all those operations in tRPC would be done via an HTTP POST.\nOur API is now ready to serve traffic, let\u0026rsquo;s create our client app, back in the client folder I\u0026rsquo;ll update the index.ts file with the following code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import { createTRPCProxyClient, httpBatchLink } from \u0026#39;@trpc/client\u0026#39;; import { AppRouter } from \u0026#39;../server/router\u0026#39;; import fetch from \u0026#39;node-fetch\u0026#39;; // polyfill const globalAny = global as any; globalAny.AbortController = AbortController; globalAny.fetch = fetch as any; async function main() { const trpc = createTRPCProxyClient\u0026lt;AppRouter\u0026gt;({ links: [ httpBatchLink({ url: \u0026#39;http://localhost:3000/trpc\u0026#39;, }), ], }); const cars = await trpc.getCars.query(); console.log(cars); } main(); To run the API server and client at the same time I will install\n1 npm i concurrently --save-dev With the package concurrently now installed I can now run the API server and client at the same time, just need to update the root package.json file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;name\u0026#34;: \u0026#34;fun-trpc\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;testing trpc\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;server\u0026#34;: \u0026#34;nodemon --quiet ./server/index.ts\u0026#34;, \u0026#34;client\u0026#34;: \u0026#34;npm start --prefix client\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;concurrently \\\u0026#34;npm run server\\\u0026#34; \\\u0026#34;npm run client\\\u0026#34;\u0026#34; }, \u0026#34;author\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;@trpc/server\u0026#34;: \u0026#34;^10.10.0\u0026#34;, \u0026#34;@types/express\u0026#34;: \u0026#34;^4.17.17\u0026#34;, \u0026#34;@types/node\u0026#34;: \u0026#34;^18.11.19\u0026#34;, \u0026#34;express\u0026#34;: \u0026#34;^4.18.2\u0026#34;, \u0026#34;zod\u0026#34;: \u0026#34;^3.20.2\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;concurrently\u0026#34;: \u0026#34;^7.6.0\u0026#34;, \u0026#34;nodemon\u0026#34;: \u0026#34;^2.0.20\u0026#34;, \u0026#34;ts-node\u0026#34;: \u0026#34;^10.9.1\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^4.9.5\u0026#34; } } I can now run the following command to confirm the API and client are working.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ npm run start \u0026gt; fun-trpc@1.0.0 start \u0026gt; concurrently \u0026#34;npm run server\u0026#34; \u0026#34;npm run client\u0026#34; [0] [0] \u0026gt; fun-trpc@1.0.0 server [0] \u0026gt; nodemon --quiet ./server/index.ts [0] [1] [1] \u0026gt; fun-trpc@1.0.0 client [1] \u0026gt; npm start --prefix client [1] [1] [1] \u0026gt; trpc-client@1.0.0 start [1] \u0026gt; nodemon client.ts [1] [1] [nodemon] 2.0.20 [1] [nodemon] to restart at any time, enter `rs` [1] [nodemon] watching path(s): *.* [1] [nodemon] watching extensions: ts,json [1] [nodemon] starting `ts-node client.ts` [1] [ { id: 1, name: \u0026#39;Toyota\u0026#39; }, { id: 2, name: \u0026#39;Honda\u0026#39; } ] [1] [nodemon] clean exit - waiting for changes before the restart As you can see from the console output above my client was able to use connect to the API server and pull data. Since I am working with a collection the console output had two cars, but let\u0026rsquo;s say that instead of I wanted the first car, and from the first car I wanted the name, the client\u0026rsquo;s main function could be updated as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 async function main() { const trpc = createTRPCProxyClient\u0026lt;AppRouter\u0026gt;({ links: [ httpBatchLink({ url: \u0026#39;http://localhost:3000/trpc\u0026#39;, }), ], }); const cars = await trpc.getCars.query(); console.log(cars[0].name); // type-safety here. I\u0026#39;m able to infer my API types from the client thanks to tRCP. } This is where tRPC shines, full type-safety in the front end using types defines in the back end. Now, this isn\u0026rsquo;t something new, type-safety in front-end code has been done before. For example, if you are not using tRPC you may build a copy of the API type in your front-end code then right after you call the API you would transform the API response into its type equivalent, for example, imagine you are using Axios to call an API as shown below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import axios from \u0026#39;axios\u0026#39;; async function getCars() { try { const { data, status } = await axios.get(\u0026#39;https://api.example.com/cars\u0026#39;); return data; } catch (error) { console.log(\u0026#39;error message: \u0026#39;, error.message); return error.message; } } } getUsers(); The code above doesn\u0026rsquo;t offer any type-safety, if have been programming long enough in TypeScript then you know that type-safety can be easily added by modifying the code as shown below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import axios from \u0026#39;axios\u0026#39;; type Car = { id: number; name: string; }; async function getCars() { try { const { data, status } = await axios.get\u0026lt;Cars[]\u0026gt;(\u0026#39;https://api.example.com/cars\u0026#39;); return data; } catch (error) { return error.message; } } getCars(); Now the data return from getCars is type-safe. Again nothing new here, pretty standard. I do think that while tRPC approach offers a better developer experience I am not too sure how many people can take advantage of tRPC. For once, your API code and client code must live together, additionally, newer frameworks like Remix offer the same type-safety out of the box without having to use tRPC.\nConclusion Still, I enjoyed doing this demo, it helped me understand why everyone has been hyping tRPC. I think the library can be beneficial to many out there, the only time I would not recommend using tRPC is if you are starting a new project on a framework like Remix or for obvious reasons, if your front-end and backend are written in different languages, but who knows, tRPC is very young, in the future, it may expand to other languages.\nI do encourage you to learn more by visiting the official website, the example written here just scratches the surface.\n","permalink":"http://localhost:1313/post/2023/trpc/","summary":"\u003cp\u003eIn the last few months, my Twitter feed has been dominated by one topic, tRPC. \u003ca href=\"https://trpc.io/\"\u003etRPC\u003c/a\u003e is a library that provides type-safety between your front end and backend, in theory, it allows you to quickly build applications.\u003c/p\u003e\n\u003cp\u003eIn today\u0026rsquo;s post, I would like to explore tRPC, its capabilities and features, and how it could be used in a project. To get started, I will create a new Node.js app using \u003ca href=\"https://expressjs.com/\"\u003eExpress\u003c/a\u003e. If you prefer to use React or Next.js see \u003ca href=\"https://trpc.io/docs/quickstart#next-steps\"\u003ethe official docs\u003c/a\u003e.\u003c/p\u003e","title":"tRPC"},{"content":"I enjoy writing unit tests and any tools that make writing tests easier are appreciated. For the last year, I have incorporated AutoFixture into all of my unit tests. I have found AutoFixture to be an excellent tool, it changed the way I approach the \u0026ldquo;Arrange\u0026rdquo; phase.\nPreviously, my arrange phase involved manually assigning values to properties, in a small class that is referenced by a few tests, you may tolerate manually assigning values. Once you start to deal with a class that has many properties such as nested properties as shown on the \u0026ldquo;Employee\u0026rdquo; class below, things get out of hand.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class Employee { public Employee() { Customers = new HashSet\u0026lt;Customer\u0026gt;(); InverseReportsToNavigation = new HashSet\u0026lt;Employee\u0026gt;(); } public long EmployeeId { get; set; } public string LastName { get; set; } public string FirstName { get; set; } public string Title { get; set; } public long? ReportsTo { get; set; } public byte[] BirthDate { get; set; } public byte[] HireDate { get; set; } public string Address { get; set; } public string City { get; set; } public string State { get; set; } public string Country { get; set; } public string PostalCode { get; set; } public string Phone { get; set; } public string Fax { get; set; } public string Email { get; set; } public virtual Employee ReportsToNavigation { get; set; } public virtual ICollection\u0026lt;Customer\u0026gt; Customers { get; set; } public virtual ICollection\u0026lt;Employee\u0026gt; InverseReportsToNavigation { get; set; } } Under the hood, AutoFixture uses a faking library that generates random values based on the property type.\nHere is a quick example.\n1 2 3 4 5 6 7 8 9 10 11 public class UnitTestingExample { [Fact] public void AutoFixtureExmaple() { var fixture = new Fixture(); var employeeFixture = fixture.Create\u0026lt;Employee\u0026gt;(); Console.WriteLine(employeeFixture.FirstName) // f17b08e4-efb0-40b8-9b6b-c937d8032c8c } } As shown above, AutoFixture will assign random values to each property, but in a unit test you would want to control the value, as shown in the following code snippet, you can use AutoFixture to assign a value to a property.\n1 2 3 4 5 6 7 8 9 10 11 12 [Fact] public void UnitTestingExample() { var fixture = new Fixture(); fixture.Customize\u0026lt;Employee\u0026gt;(e =\u0026gt; e.With(p =\u0026gt; p.FirstName, \u0026#34;John\u0026#34;) ); var employeeFixture = fixture.Create\u0026lt;Employee\u0026gt;(); Console.WriteLine(employeeFixture.FirstName) // John } In the code above, the property FirstName is assigned to the value \u0026ldquo;John\u0026rdquo; using the customization API. Now whenever you ask AutoFixture to create an instance or many instances of the Employee class, all instances will have \u0026ldquo;John\u0026rdquo; as the value for the property FirstName.\nWhen working with a property, and that property is an object with its own properties you might attempt to create the following code.\n1 2 3 4 5 6 7 8 9 10 11 12 [Fact] public void UnitTestingExample() { var fixture = new Fixture(); fixture.Behaviors.Add(new OmitOnRecursionBehavior()); fixture.Customize\u0026lt;Employee\u0026gt;(e =\u0026gt; e.With(p =\u0026gt; p.ReportsToNavigation.FirstName, \u0026#34;Benjamin\u0026#34;) // Throws an exception. ) var employeeFixture = fixture.Create\u0026lt;Employee\u0026gt;(); Console.WriteLine(employeeFixture.FirstName) } The code above will lead to an exception being thrown, instead, use the customize API again to assign a value to the nested property, create the fixture, then use the customize API again on the property of the parent object, and use the created fixture as the value.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public void UnitTestingExample() { var fixture = new Fixture(); fixture.Customize\u0026lt;Employee\u0026gt;(e =\u0026gt; e.With(p =\u0026gt; p.FirstName, \u0026#34;John\u0026#34;)); var employeeFixture1 = fixture.Create\u0026lt;Employee\u0026gt;() fixture.Customize\u0026lt;Employee\u0026gt;(e =\u0026gt; { return e.With(p =\u0026gt; p.FirstName, \u0026#34;Benjamin\u0026#34;) .With(p =\u0026gt; p.ReportsToNavigation, employeeFixture1) }); var employeeFixture2 = fixture.Create\u0026lt;Employee\u0026gt;(); Console.WriteLine(employeeFixture1.FirstName); // John Console.WriteLine(employeeFixture2.FirstName); // Benjamin } AutoFixture can also create collections.\n1 2 3 4 5 6 7 8 9 10 11 12 [Fact] public void UnitTestingExample() { var fixture = new Fixture(); fixture.Behaviors.OfType\u0026lt;ThrowingRecursionBehavior\u0026gt;() .ToList() .ForEach(b =\u0026gt; fixture.Behaviors.Remove(b)); fixture.Behaviors.Add(new OmitOnRecursionBehavior()) var employeeFixture = fixture.CreateMany\u0026lt;Employee\u0026gt;(); var a = employeeFixture.Count(); Console.WriteLine(employeeFixture.Count()); // 3 } You can specify the total number of objects to be created by passing an amount as an argument to CreateMany.\n1 2 3 4 5 6 7 8 9 10 11 12 [Fact] public void UnitTestingExample() { var fixture = new Fixture(); fixture.Behaviors.OfType\u0026lt;ThrowingRecursionBehavior\u0026gt;() .ToList() .ForEach(b =\u0026gt; fixture.Behaviors.Remove(b)); fixture.Behaviors.Add(new OmitOnRecursionBehavior()) var employeeFixture = fixture.CreateMany\u0026lt;Employee\u0026gt;(100); var a = employeeFixture.Count(); Console.WriteLine(employeeFixture.Count()); // 100 } One flaw with the code I have shown is that if you are writing many tests that share the fixture configuration then the tests become hard to maintain. As documented in Keep your unit tests DRY with AutoFixture Customizations by Enrico Campidoglio you can centralize same configurations into a class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class UnitTestingExample { [Fact] public void Test1() { var fixture = new Fixture(); fixture.Customize(new EmployeeCustomization()); var employeFixture = fixture.Create\u0026lt;Employee\u0026gt;(); Console.WriteLine(employeFixture.FirstName); // John } } public class EmployeeCustomization : ICustomization { public void Customize(IFixture fixture) { fixture.Behaviors.Add(new OmitOnRecursionBehavior()); fixture.Customize\u0026lt;Employee\u0026gt;(e =\u0026gt; e.With(p =\u0026gt; p.FirstName, \u0026#34;John\u0026#34;)); } } With a centralized configuration class, test maintenance should be much easier. Another key feature of AutoFixure, it supports all the major .NET testing frameworks like XUnit, NUnit, Moq and so on.\n","permalink":"http://localhost:1313/post/2023/using-autofixture/","summary":"\u003cp\u003eI enjoy writing unit tests and any tools that make writing tests easier are appreciated. For the last year, I have incorporated \u003ca href=\"https://github.com/AutoFixture/AutoFixture/\"\u003eAutoFixture\u003c/a\u003e into all of my unit tests. I have found AutoFixture to be an excellent tool, it changed the way I approach the \u0026ldquo;Arrange\u0026rdquo; phase.\u003c/p\u003e\n\u003cp\u003ePreviously, my arrange phase involved manually assigning values to properties, in a small class that is referenced by a few tests, you may tolerate manually assigning values. Once you start to deal with a class that has many properties such as nested properties as shown on the \u0026ldquo;Employee\u0026rdquo; class below, things get out of hand.\u003c/p\u003e","title":"Using AutoFixture"},{"content":"In my last post, Kubernetes In WSL - Connect to a service from Windows, I explored a few ways to connect to a Kubernetes service from the host machine, Windows. In the end of that blog post, I stated that using HostPort was the best option because at the time I did not know how to assign a static IP address to WSL.\nWithout using a static IP address, when WSL is restarted a new IP is assigned. Having a dynamic IP made it harder for me to connect to a Kubernetes service from Windows as I would need to update all my configurations whenever a new IP was assigned to WSL.\nLuckily I was not the only developer that wanted WSL to use a static IP. In WSL2 Set static ip?, a few developers outlined how you can create a Hyper-V switch to bridge the network between WSL and Windows. By using a Hyper-V switch, the WSL IP becomes static, and by having a static IP, I can create a load balancer that uses that static IP to serve traffic.\nIn today\u0026rsquo;s post, I want to demonstrate how to create a Hyper-V switch, how to assign it to WSL distro and how to add a load balancer to a Kubernetes service that uses the static IP.\nPrerequisite Before I get started I wanted to point out some requirements. First, you need to be on Windows 11 Enterprise or Pro, Hyper-V is not available in Windows 11 Home, though you can try to enable it using the method documented in this post. As far as I know, what I am about to show you does not work on Windows 10 or older Windows systems.\nBelow you will find my current system spec, you should be using the same version or higher.\n1 2 OS Name: Microsoft Windows 11 Pro OS Version: 10.0.22621 N/A Build 22621 and my WSL version, you should be using the same version or higher.\n1 2 3 4 5 6 7 WSL version: 1.0.3.0 Kernel version: 5.15.79.1 WSLg version: 1.0.47 MSRDC version: 1.2.3575 Direct3D version: 1.606.4 DXCore version: 10.0.25131.1002-220531-1700.rs-onecore-base2-hyp Windows version: 10.0.22621.963 Hyper-V Switch The first step in using a Hyper-V switch is to enable the feature in Windows. Open Powershell as an admin and run the following command.\n1 DISM /Online /Enable-Feature /All /FeatureName:Microsoft-Hyper-V /all After the command has been successfully executed you will be prompted to restart Windows, do so.\nOnce Windows fully boots up, search the Hyper-V manager on the start menu and open it.\nInside the Hyper-V manager, under Actions, there should be an option called \u0026ldquo;Virtual Switch Manager\u0026rdquo;, click on that option\nThe Virtual Switch Manager window will now appear. Under \u0026ldquo;Create virtual switch\u0026rdquo; make sure to have the \u0026ldquo;External\u0026rdquo; option selected, then click the \u0026ldquo;Create Virtual Switch\u0026rdquo; button.\nOn the next screen, you will need to provide a name for the switch, the name is important as we will use it later on. The External Network has been chosen, I would leave it alone, just make sure that the checkbox \u0026ldquo;Allow management operating system to share this network adapter\u0026rdquo; is checked, then click on the \u0026ldquo;Apply\u0026rdquo; button.\nA new window will appear giving a warning about the network being disrupted due to this change. Click \u0026ldquo;Yes\u0026rdquo; to continue.\nAfter the Switch has been created, click the OK button.\nCongratulations, you have successfully configured your Hyper-V switch. You can confirm by going to the networks view, there should be a new adapter listed called Network Bridge as seen in the image below.\nWSL Now that the network bridge has been created WSL needs to be updated to use the bridge. On windows use File Explorer to navigate to %UserProfile%, this is where the file .wslconfig needs to be placed. In case you are not aware, the file .wslconfig is used to configure global settings for WSL regardless of distro.\nIf you already have a .wslconfig file, open it up, if not, create a new file. Add the following content at the end of the file.\n1 2 3 [wsl2] networkingMode=bridged vmSwitch=WSL Where WSL, the name of the switch created in the Hyper-V manager is assigned to vmSwitch. If you don\u0026rsquo;t remember the name open Hyper-V manager again or run the following command from Powershell as an admin.\n1 Get-VMSwitch -SwitchType External | Select Name, SwitchType, NetAdapterInterfaceDescription, AllowManagementOS Here is the result of that command on my system.\n1 2 3 Name SwitchType NetAdapterInterfaceDescription AllowManagementOS ---- ---------- ------------------------------ ----------------- WSL External Killer(R) Wi-Fi 6 AX1650x 160MHz Wireless Network Adapter (200NGW) True After applying the new settings to the .wslconfig file restart WSL using the following command from Powershell\n1 wsl --shutdown. Open a new WSL shell instance and run the following command.\n1 ip a In the output, look for the value of eth0.\nThis is the new static IP that WSL will have. You can confirm that the IP does not change by restarting WSL again or by shutting down Windows.\nGreat, with a static IP we can now configure the Kubernetes load balancer.\nKubernetes Follow my Use Kubernetes In WSL to install Kubernetes in WSL.\rAs shown on my Kubernetes In WSL - Connect to a service from Windows we can use MetalLB to create a load balancer.\nFirst, grab the WSL IP, for me, it is now 192.168.2.10, I will increment the last octet by 1 to not conflict with the node\u0026rsquo;s IP address.\nIn WSL, enable MetalLB using the following command.\n1 microk8s enable metallb:192.168.2.11/32 /32 at the end of the IP is used here to keep the load balancer IP static. If you don\u0026rsquo;t understand why then may I suggest watching Understanding CIDR Ranges and dividing networks\nOnce MetalLB is ready, you can serve traffic to a load balancer.\nFor example, take the following YAML obtained from the official Kubernetes docs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 You can deploy the Nginx deployment by running the following command.\n1 kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml Then expose the service created using the following command.\n1 kubectl expose deployment nginx-deployment --port=31080 --target-port=80 --type=LoadBalancer If you run the following command.\n1 kubectl get svc -o wide You will see that the Nginx service created was assigned an external IP, the load balancer\u0026rsquo;s IP.\nMoment of truth, can we reach this Nginx service running on http://192.168.2.11:31080 from Windows.\nWe can! The service can be reached from Windows.\nAs mentioned in Kubernetes In WSL - Connect to a service from Windows we can change the Windows host file by adding the load balancer IP and mapping to a domain. Remember you cannot map the load balancer IP to localhost but you could map it to something like www.localhost.com or local.host. If you really wish to access the Kubernetes service using localhost then may I suggest using a port proxy from Windows to WSL.\nlocalhost To use localhost in Windows, you can create a port proxy from Windows to WSL using the following command from an admin shell.\n1 netsh interface portproxy add v4tov4 listenport=31080 listenaddress=0.0.0.0 connectport=31080 connectaddress=192.168.2.11 Here, I\u0026rsquo;m port proxying traffic on port 31080 on Windows localhost, 0.0.0.0 to port 31080 on 192.168.2.11, my load balancer IP. This will allow me to access the service using http://localhost:31080/ from Windows as shown on the screen below.\nConclusion Having a static IP on WSL makes Kubernetes development in WSL so much easier, being able to connect a service running on Windows to a service running on Kubernetes is an awesome feature to have.\nQuick warning, if you restart Windows, the configuration above will continue to work but I have noted that Kubernetes takes a few minutes to accept traffic again from Windows. Not exactly sure as to why, if you are in hurry simply open WSL and ping the service directly from within WSL.\nHope this post helped you get load balancing working for Kubernetes in WSL.\nThis will be my last post for 2022, see you next year.\n","permalink":"http://localhost:1313/post/2022/use-static-ip-in-wsl/","summary":"\u003cp\u003eIn my last post, \u003ca href=\"/post/2022/connect-to-a-service-in-a-kubernetes-instance-hosted-on-wsl-from-windows/\"\u003eKubernetes In WSL - Connect to a service from Windows\u003c/a\u003e, I explored a few ways to connect to a Kubernetes service from the host machine, Windows. In the end of that blog post, I stated that using \u003ca href=\"/post/2022/connect-to-a-service-in-a-kubernetes-instance-hosted-on-wsl-from-windows/#using-hostport\"\u003eHostPort\u003c/a\u003e was the best option because at the time I did not know how to assign a static IP address to WSL.\u003c/p\u003e\n\u003cp\u003eWithout using a static IP address, when WSL is restarted a new IP is assigned. Having a dynamic IP made it harder for me to connect to a Kubernetes service from Windows as I would need to update all my configurations whenever a new IP was assigned to WSL.\u003c/p\u003e","title":"Use Static IP In WSL"},{"content":"Today\u0026rsquo;s post is a follow-up to my Use Kubernetes In WSL blog post, where I outlined how to install Kubernetes on WSL. As noted at the end of the post, I was having issues connecting from the host, a windows machine, to Kubernetes in WSL.\nConnection Issue The main issue I was facing was that I could not connect to a pod running on Kubernetes using window\u0026rsquo;s localhost. Take the following Nginx deployment obtained from the official Kubernetes documentation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 I can use the YAML content above to create a deployment by executing the following kubectl command.\n1 kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml I can confirm the deployment was successful by verifying that all 3 pods are up and running using the following kubectl command.\n1 kubectl get pods Once the pods are ready to receive traffic we can expose our deployment using the following kubectl command.\n1 kubectl expose deployment nginx-deployment --port=31080 --target-port=80 --type=NodePort target-port is where the container is listening for requests coming from outside the node. port is where the container is listening inside the cluster, in this case, port 80. Once the service has been created you can use the following kubectl command to see the cluster port, the service port assigned to the Nginx deployment service, and the randomly generated port for local connections.\n1 kubectl get svc -o wide Here is my output of the command above.\n1 2 3 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.152.183.1 \u0026lt;none\u0026gt; 443/TCP 9m27s \u0026lt;none\u0026gt; nginx-deployment NodePort 10.152.183.122 \u0026lt;none\u0026gt; 31080:30454/TCP 83s app=nginx This is where I incorrectly assumed that I could reach the Nginx service running on Kubernetes from Windows\u0026rsquo; host using localhost. I made this assumption because WSL\u0026rsquo;s localhost now by default binds to Windows\u0026rsquo; localhost. I assumed that by exposing the deployment, the randomly generated port, 30454, which is used to connect to the service through localhost on WSL would bind to port 30454 on the Window\u0026rsquo;s host and allow me to access the Nginx service.\nThese assumptions were wrong. The following screenshots confirm it.\nFailed to connect to port 30454 from Windows' localhost.\rI really needed to figure out a way to connect to the Nginx app running in k8s on WSL while using localhost from windows. I ended up chasing four possible solutions that I now want to share with you.\nUsing the Node IP The first approach I took to connect to Nginx from Windows was to use the node IP. If you followed the commands under Connection Issue and have all three Nginx pods up and running then replace localhost with the node IP on your browser in Windows.\nIn my case, the node IP is 172.23.207.235, I obtain that value by running the following command.\n1 kubectl get node -o wide Here is the output of the command above.\n1 2 NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gohan Ready \u0026lt;none\u0026gt; 30m v1.25.4 172.23.207.235 \u0026lt;none\u0026gt; Ubuntu 22.04.1 LTS 5.15.79.1-microsoft-standard-WSL2 containerd://1.6.9 To connect to NGINX I opened up my browser to 172.23.207.235:30454.\nThe following screenshot shows that I can connect to a service running in Kubernetes on WSL from Windows using the node IP address.\rBingo, I can reach the service from Windows. I wanted to connect to the service using localhost, so I thought, why not map 172.23.207.235 to Window\u0026rsquo;s localhost, as it turns out, that is big fat NO. I learned that you should never attempt it, leave localhost alone. Instead map it to something similar, perhaps local.host or www.localhost.com\nThe following screenshot show you can create your own version of localhost.\rIf you still cannot reach the service after having modified the host\u0026rsquo;s file, flush your DNS in Windows using the following command.\n1 ipconfig /flushdns I Then realize that using the node IP to approach had two flaws.\nThe port is randomly generated when the service is created. I needed the port to predetermined. The Node IP can change if you restart or shut down WSL. As of December 2022, there is no easy way to set the IP of WSL to be static. These two flaws made this approach not a viable solution so I moved on to the next approach, port forwarding.\nUsing Port-Forward This approach involves using port forwarding\nAfter following the commands under Connection Issue and having verified that all 3 pods are up and running, use the following kubectl command to port forward from the WSL\u0026rsquo;s localhost to Windows\u0026rsquo; localhost. Remember, they are now one and the same.\nTo start port forwarding traffic to the Nginx pods run the following command.\n1 kubectl port-forward --address 0.0.0.0 service/nginx-deployment 31080 Kubernetes will forward traffic from 31080 to port 80 on the container, and since WSL\u0026rsquo;s localhost:31080 is now the same as Windows port 31080, I can open up the browser to localhost:31080 to connect to the service.\nThe following screenshot shows that you can connect using localhost so long as you are port-forwarding the traffic in WSL.\rAnother approach would be to forward traffic from the pod instead of the service using the following command, the result is the same.\n1 kubectl port-forward nginx-deployment-7fb96c846b-4c4r4 32196:80 nginx-deployment-7fb96c846b-4c4r4 is the name of one of the three pods running. This is great, I love being able to connect using localhost, however, this solution is temporary, as soon as you stop port-forwarding traffic, the connection will stop work working on Windows.\nOn to the next approach, using MetalLB.\nUsing MetalLB This approach involves using a microk8s addon, MetalLB, to allow load balancing. After going through it I realized that this approach is exactly as Using Node Ip. If you didn\u0026rsquo;t like that solution then you can skip this part or not, you can learn how to use MetalLB. Fun!\nMetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols. It can be enabled in microk8s using the following command.\n1 microk8s enable metallb Note that when you execute the command, MetalLb is going to expect you to provide an IP. You can specify it as a range like 10.64.140.43-10.64.140.49,192.168.0.105-192.168.0.111 or using CDIR notation. I prefer CDIR notation.\nFirst, I need an IP address. If I run the following command I will get the IP of the node.\n1 kubectl get node -o wide Gives me the following output.\n1 2 NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME gohan Ready \u0026lt;none\u0026gt; 19h v1.25.4 172.23.200.34 \u0026lt;none\u0026gt; Ubuntu 22.04.1 LTS 5.15.79.1-microsoft-standard-WSL2 containerd://1.6.9 For my load balancer IP, I\u0026rsquo;m going to change the last octet from 34 to 49, so the IP for MetalLb is going to be 172.23.200.49.\n1 microk8s enable metallb:172.23.200.49/32 Note that I\u0026rsquo;m using /32, this keeps the load balancer IP static. If you don\u0026rsquo;t understand why /32 makes the IP static then may I recommend watching Understanding CIDR Ranges and dividing networks\nDelete the Nginx service if you created one.\n1 kubectl delete svc nginx-deployment Now expose the deployment again but this time the type will be LoadBalancer, not NodePort\n1 kubectl expose deployment nginx-deployment --port=31080 --target-port=80 --type=LoadBalancer Back on Windows, I\u0026rsquo;ll open a web browser and navigate to http://172.23.200.49:31080/ to confirm I can reach the Nginx service.\nAs expected, I can reach it.\nJust like the Using the Node IP approach, if don\u0026rsquo;t enjoy using an IP address to access the service, modify the windows hosts file and map the IP of the load balancer to a custom domain.\nYou could also port proxy the traffic from Windows to WSL using netsh interface portproxy.\nFor example,\n1 netsh interface portproxy add v4tov4 listenport=31080 listenaddress=0.0.0.0 connectport=31080 connectaddress=172.23.207.235 Remember, if you restart WSL, a new IP will be assigned to the node, which means your load balance IP will no longer router traffic, you will need to reenable MetalLB using the new node IP to get traffic flowing into the Kubernetes service and also remap the netsh interace.\nUsing HostPort What ultimately ended up being my preferred solution. HostPort keeps everything simple, no hosts files, no IPs, and no fuss. I consider this approach to be the same as port forwarding but unlike Port Forwarding, this approach is a more permanent solution, well so long as WSL is not restarted or shut down.\nHostPort applies to the Kubernetes containers. The port is exposed to the WSL network host. This is often an approach not recommended because the Host IP can change. In WSL that happens when WSL is restarted or shut down.\nTo see it in action start from a clean slate, and delete all nginx-deployments \u0026amp; nginx-services running, now we are going to deploy Nginx again but this time we are going to modify the deployment by adding an additional configuration, hostPort as seen in the YAML below which is the content of my deploy.yaml file.\nThe number of replicas was reduced from 3 to 1, because as mentioned above, the host port applies to a container running in a pod, and you cannot map one port to multiple containers. You could leave the number of containers as 3 but note that when the pods are created, Kubernetes will only set one of the three as ready, that one pod that is ready will be the only one that can serve traffic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 hostPort: 5700 As you can tell from the YAML, I decide to use port 5700. I can now apply this deployment using the following command.\n1 kubectl apply -f deploy.yaml You can use describe to see that the deployment is now bound to port 5700 in WSL.\n1 kubectl describe deployment nginx-deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Name: nginx-deployment Namespace: default CreationTimestamp: Thu, 15 Dec 2022 23:59:27 -0500 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 1 Selector: app=nginx Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.14.2 Port: 80/TCP Host Port: 5700/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available False MinimumReplicasUnavailable Progressing True ReplicaSetUpdated OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-deployment-66cdbdf488 (1/1 replicas created) Events: \u0026lt;none\u0026gt; Great, need one additional configuration. This time on the host, open up Powershell as an administrator and execute the following command.\n1 netsh interface portproxy add v4tov4 listenport=5700 listenaddress=0.0.0.0 connectport=5700 connectaddress=172.23.207.235 Where listenport is a port in our window\u0026rsquo;s host, listenaddress is the host\u0026rsquo;s localhost, connectport is the host port defined in our deploy.yaml file, and connectaddress is the node IP address obtained using the following command. This command forwards traffice from http://localhost:5700 to http://172.23.207.235:5700\n1 kubectl get node -o wide After the netsh interface portproxy command is executed I was ready to connect. I opened a browser to localhost:5700 on the Windows machine.\nThe following screenshot shows that I can connect to the Nginx service from Windows' localhost.\rPerfect I can now connect to Nginx as if it were natively running on Windows. Quick tip, before deciding which port to use on the command netsh interface portproxy run the following command.\n1 netsh interface portproxy show all It will output any mapping, you may already have created or had created by another service. The port listed is unavailable, and therefore, cannot be remapped unless you delete it using the following command.\n1 netsh interface portproxy delete v4tov4 listenport=5700 listenaddress=0.0.0.0 Using the host port solves my original issue, I can now connect to services running on Kubernetes in WSL from Windows using localhost.\nConclusion For now, I feel like HostPort is the best solution I could come up with, even if the IP changes whenever I restart WSL. If the day ever comes when I can set a static IP in WSL then MetalLB would probably be my preferred choice since HostPort limits the number of PODs to one.\nThanks for reading.\n","permalink":"http://localhost:1313/post/2022/connect-to-a-service-in-a-kubernetes-instance-hosted-on-wsl-from-windows/","summary":"\u003cp\u003eToday\u0026rsquo;s post is a follow-up to my \u003ca href=\"/post/2022/use-kubernetes-in-wsl/\"\u003eUse Kubernetes In WSL\u003c/a\u003e blog post, where I outlined how to install Kubernetes on WSL. As noted at the end of the post, I was having issues connecting from the host, a windows machine, to Kubernetes in WSL.\u003c/p\u003e\n\u003ch2 id=\"connection-issue\"\u003eConnection Issue\u003c/h2\u003e\n\u003cp\u003eThe main issue I was facing was that I could not connect to a pod running on Kubernetes using window\u0026rsquo;s localhost. Take the following Nginx deployment obtained from the \u003ca href=\"https://k8s.io/examples/controllers/nginx-deployment.yaml\"\u003eofficial\u003c/a\u003e Kubernetes documentation.\u003c/p\u003e","title":"Kubernetes In WSL - Connect to a service from Windows"},{"content":"If you are a fan of the strategy pattern, and you find yourself adding a lot of conditional logic around each strategy then you should consider replacing all branching logic using inversion of control.\nTake the following code as an example. It defines a strategy for reading different file types. For simplicity, the code writes out to the console a message, in a real-world application, the logic would be far more complex, but we are not interested in that logic, rather we are interested in how the strategy pattern works and how we can improve its usage.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public interface IStrategy { void RunStrategy(); } public class TextFileStrategy : IStrategy { public void RunStrategy() { Console.WriteLine(\u0026#34;Reading text file....\u0026#34;); } } public class PDFFileStrategy : IStrategy { public void RunStrategy() { Console.WriteLine(\u0026#34;Reading PDF file....\u0026#34;); } } public class PNGFileStrategy : IStrategy { public void RunStrategy() { Console.WriteLine(\u0026#34;Reading PNG File....\u0026#34;); } } Typically, there is a context class, this is the container class that decides which strategy to use. For our example, I created the following context class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public class StrategyContext { public void SelectStrategy(string fileExtesion) { if (fileExtesion is \u0026#34;.txt\u0026#34;) { var textFileStrategy = new TextFileStrategy(); textFileStrategy.RunStrategy(); } else if (fileExtesion is \u0026#34;.pdf\u0026#34;) { var pdfFileStrategy = new PDFFileStrategy(); pdfFileStrategy.RunStrategy(); } else if(fileExtesion is \u0026#34;.png\u0026#34;) { var pngFileStrategy = new PNGFileStrategy(); pngFileStrategy.RunStrategy(); } } } What you see above is often shown as an example of the strategy pattern, and it is often found in real-world apps. Technically, the code above is valid and can be easily maintained if you have a few strategies. Once you start to reach more than a few strategies then the code above can become a problem as every new strategy requires adding another conditional check.\nAn approach that I consider to be an improvement is to decide the strategy via inversion of control. Take the example above, instead of if/else or switch statements, the context class takes a collection of all available strategies, then based on the file extension the corresponding strategy is selected.\nHere is how the code would look.\nFirst, the IStrategy interface is updated by adding a new AppliesTo method. The purpose of this method is to know if the strategy can satisfy the given file type by looking at the file extension.\n1 2 3 4 5 public interface IStrategy { bool AppliesTo(string fileExtension); void RunStrategy(); } With the updated interface, each strategy is also updated.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public class TextFileStrategy : IStrategy { public bool AppliesTo(string fileExtension) { return fileExtension.Equals(\u0026#34;.txt\u0026#34;, StringComparison.OrdinalIgnoreCase); } public void RunStrategy() { Console.WriteLine(\u0026#34;Reading text file....\u0026#34;); } } public class PDFFileStrategy : IStrategy { public bool AppliesTo(string fileExtension) { return fileExtension.Equals(\u0026#34;.pdf\u0026#34;, StringComparison.OrdinalIgnoreCase); } public void RunStrategy() { Console.WriteLine(\u0026#34;Reading PDF file....\u0026#34;); } } public class PNGFileStrategy : IStrategy { public bool AppliesTo(string fileExtension) { return fileExtension.Equals(\u0026#34;.png\u0026#34;, StringComparison.OrdinalIgnoreCase); } public void RunStrategy() { Console.WriteLine(\u0026#34;Reading PNG File....\u0026#34;); } } And the updated context class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class StrategyContext { private readonly IStrategy[] _strategies; public StrategyContext(IStrategy[] strategies) { _strategies = strategies; } public void SelectStrategy(string fileExtesion) { var strategy = _strategies.FirstOrDefault(s =\u0026gt; s.AppliesTo(fileExtesion)); strategy.RunStrategy(); } } The class now takes a collection of all concrete implementations of IStrategy. Look how much cleaner the context class has become, all the conditional logic is gone and the best part, adding a new strategy only involves adding the new strategy class via dependency injection. With this approach, the context class never has to change with the addition of a new strategy. Pretty neat.\nBy the way, the technique shown above can be applied whenever you find yourself working with interfaces that share similar functionality.\n","permalink":"http://localhost:1313/post/2022/power-up-the-strategy-pattern-with-inversion-of-control/","summary":"\u003cp\u003eIf you are a fan of the \u003ca href=\"https://refactoring.guru/design-patterns/strategy\"\u003estrategy pattern\u003c/a\u003e, and you find yourself adding a lot of conditional logic around each strategy then you should consider replacing all branching logic using inversion of control.\u003c/p\u003e\n\u003cp\u003eTake the following code as an example. It defines a strategy for reading different file types. For simplicity, the code writes out to the console a message, in a real-world application, the logic would be far more complex, but we are not interested in that logic, rather we are interested in how the strategy pattern works and how we can improve its usage.\u003c/p\u003e","title":"Power Up The Strategy Pattern With Inversion Of Control"},{"content":"A while back I made a blog post titled, Tools For The Modern Day Developer, in which I listed the tools I thought at the time every developer should be using. Today, I still stand by that list, but I would like to add an additional tool that lately has become super useful to me, that tool is Mockoon.\nMockoon is a free and open-source mock API tool created by Guillaume in 2017. It offers Docker support, a CLI, importing and exporting of Mockoon environments, it works with OpenAPI 3.0 files. In many ways it feel like Postman, even the UI feels like the old classic Postman UI.\nI have been using it for the last few months at work and on side projects. It has allowed me to create powerful mocks around service that may not have a sandbox as well as for local development. Instead of spinning a bunch of services locally for debugging, I only run the servie I\u0026rsquo;m actually working on, all external API calls are mocked.\n","permalink":"http://localhost:1313/post/2022/mockoon/","summary":"\u003cp\u003eA while back I made a blog post titled, \u003ca href=\"https://www.yunier.dev/post/2021/tools-for-the-modern-developer/\"\u003eTools For The Modern Day Developer\u003c/a\u003e, in which I listed the tools I thought at the time every developer should be using. Today, I still stand by that list, but I would like to add an additional tool that lately has become super useful to me, that tool is \u003ca href=\"https://mockoon.com/\"\u003eMockoon\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/post/2022/mockoon/mockoon.png\" alt=\"Mockoon\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003eMockoon is a free and open-source mock API tool created by \u003ca href=\"https://github.com/255kb\"\u003eGuillaume\u003c/a\u003e in 2017. It offers Docker support, a CLI, importing and exporting of Mockoon environments, it works with OpenAPI 3.0 files. In many ways it feel like \u003ca href=\"https://www.postman.com/\"\u003ePostman\u003c/a\u003e, even the UI feels like the old classic Postman UI.\u003c/p\u003e","title":"Mockoon"},{"content":"There have been a few instances where I could not figure out a problem within WSL. Problems that would be easier to fix if I had a UI instead of just an instance of the WSL shell. If you find yourself in such a situation know that you can install the UI portion, a Linux desktop on top of WSL. Once the UI has been installed you can RDP into the WSL instance allowing you to use the WSL distro as if it were natively installed on your machine.\nInstall A Linux Desktop To get started you will need to determine which Linux Desktop environment you would like to install. You have many options to choose from but I recommend you stick to something that you are familiar with or something light like xfce. I will choose Xfce as it has a small footprint.\nTo install the xfce desktop environment run the following commands on your WSL terminal.\n1 2 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y sudo apt install xfce4 -y The first command will install available upgrades on the Ubuntu WSL instance, the second command will install xfce.\nDisplay Manager During the installation, the prompt below may appear. If it does, set the display manager as gdm3 by hitting enter.\nlightdm If you pick the wrong value and you would like to switch to lightdm as the display manager then run the following command.\n1 sudo dpkg-reconfigure lightdm gdm3 If you picked lightdm and would like to switch to gdm3 then run the following command.\n1 sudo dpkg-reconfigure gdm3 IP Address With xfce installed I need to find out the WSP IP address. That can be done by running the following command on the WSL terminal.\n1 hostname -I As seen in the screenshot above, the WSL IP is 172.17.113.211.\nEnable XRDP To be able to connect to the WSL instance from an RDP client like Window\u0026rsquo;s Remote Desktop Connection, you will need to enable the xrdp service. It should already be installed on your machine, if it is not, then run the following command from the WSL terminal to install it.\n1 sudo apt install xrdp -y You can verify that it is installed by checking if the service is running, execute the following command on the WSL terminal.\n1 sudo service xrdp status If installed correctly, you will get an output similar to the one shown in the screenshot below.\nNow, start the service by running the following command from the WSL terminal.\n1 sudo service xrdp start RDP Open up your RDP client. Connect to the WSL IP address obtained as the output of hostname -I. The XRDP username and password prompt window will appear. Before you type your account credentials, close the WSL terminal, if you attempt to RDP into the WSL instance while you have the WSL terminal running may run into the following problems.\nRDP will immediately close your connection and kick you out. RDP will not close but instead will give you a black screen. As you can see from the screenshot below, I was able to RDP into the WSL instance. I can now run WSL through the UI as if it were natively installed on my machine.\nTroubleshooting Second User RDP does not allow a user with an active session to RDP into the machine that has the active session. You can tell you are facing this problem if you RDP into the WSL instance, and authenticate successfully but then only see a black screen. As stated above, close any instances of the WSL shell and try again. Another option, if you feel like you need to have the WSL terminal instance running while trying to RDP, create a second user using the following command.\n1 sudo adduser username Set the username and password. RDP into the WSL instance using the new username.\n","permalink":"http://localhost:1313/post/2022/remote-desktop-into-wsl/","summary":"\u003cp\u003eThere have been a few instances where I could not figure out a problem within WSL. Problems that would be easier to fix if I had a UI instead of just an instance of the WSL shell. If you find yourself in such a situation know that you can install the UI portion, a Linux desktop on top of WSL. Once the UI has been installed you can RDP into the WSL instance allowing you to use the WSL distro as if it were natively installed on your machine.\u003c/p\u003e","title":"Remote Desktop Into WSL"},{"content":"If you are writing unit tests in .NET, you may eventually find the need to generate code coverage reports to show how much coverage your project has. The best tool for code coverage reports in my experience has been coverlet because it supports both .NET Framework and .NET Core.\nNUnit NUnit, the tried and tested framework originally being a port of JUnit. A powerful tool that when combined with coverlet console can be used to generate code coverage reports. To demonstrate, I will create an NUnit test project targeting .NET Framework 4.8 along with a Class Library type project also targeting .NET Framework 4.8\nThe class library is going to be simple, it will have a single class called Calculator, this class will have a method that adds two numbers together. A classic example used often in programming tutorials.\nThe calculator class definition can be seen in the code snippet below.\n1 2 3 4 public class Calculator { public int Add(int x, int y) { return x + y; } } And the following code is one of the tests that I created. It asserts that adding 1 and 2 results in 3.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Test] public void Calculator_AddingOnePlusTwo_AssertResultIsThree() { // Arrange var x = 1; var y = 2; // Ac var sut = new Calculator(); var result = sut.Add(x, y); // Assert Assert.AreEqual(3, result); } To generate a code coverage report for the unit test above I will need to install the NUnit console so that I can execute the test from a terminal like Powershell. You can install the NUnit Console by going to the official repository, under releases, find the latest version, and install it. If you are a fan of Chocolately then you can install the console runner by running the following command from a terminal.\n1 choco install nunit-console-runner Confirm the console was installed correctly by running the following command on a terminal.\n1 nunit3-console.exe If the command is not recognized, then you will need to update your path environment variables to include nunit-console.exe.\nThe environment paths are as follows\n1 C:\\ProgramData\\chocolatey\\bin If you install the nunit3 console throught chocolatey on the C drive.\n1 C:\\Program Files (x86)\\Nunit.org\\nunit-console. Is the path if you install the nunit console through an exe obtained under the releases tab in the official repo.\nThe next step will be to install a .NET Core SDK. As of November 2022, the SDK is at version 7, since .NET 7 was just released.\nWith the .NET SDK installed, run the following commands.\n1 2 dotnet tool install --global dotnet-reportgenerator-globaltool --version 5.1.12 dotnet tool install --global coverlet.console --version 3.2.0 The first tool, the report generator is a tool that will create an HTML report from the output file created by the coverlet console.\nWith everything installed, I can create the report using the following command.\n1 coverlet \u0026#34;TestProject2/bin/Release/net472/TestProject2.dll\u0026#34; --target \u0026#34;nunit3-console.exe\u0026#34; --targetargs \u0026#34;TestProject2/bin/Release/net472/TestProject2.dll --noresult\u0026#34; --format opencover --output ./results.xml The command above can be broken down as follows.\ncoverlet is used to invoke the coverlet console with a path to the DLL that contains the unit tests. target is used to invoke an external test runner, in this case, NUnit 3. targetargs is used to pass parameters to the target runner, NUnit. In the case above we are telling Nunit to run the tests located in the TestProject2 by giving it a direct path to the DLL, then we use \u0026ndash;noresultto tell NUnit to not generate a test result file. I am using this argument here because, for our example, we are not interested in the test results file genearted by NUnit. If you are interested in the file, then ommit \u0026ndash;noresult from the command. format is used to tell the coverlet console what format should be used when it outputs the coverage report. In this example I used opencover, the available formats are json, lcov, opencover, cobertura, and teamcity. output is used to let coverlet know where to place the final result as well as what to name the file. All the supported parameters for the coverlet console are documented here. The parameters that can be passed to NUnit under \u0026ndash;targetargs are documented here.\nThe content of the results.xml file is as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;CoverageSession\u0026gt; \u0026lt;Summary numSequencePoints=\u0026#34;1\u0026#34; visitedSequencePoints=\u0026#34;1\u0026#34; numBranchPoints=\u0026#34;0\u0026#34; visitedBranchPoints=\u0026#34;0\u0026#34; sequenceCoverage=\u0026#34;100\u0026#34; branchCoverage=\u0026#34;100\u0026#34; maxCyclomaticComplexity=\u0026#34;1\u0026#34; minCyclomaticComplexity=\u0026#34;1\u0026#34; visitedClasses=\u0026#34;1\u0026#34; numClasses=\u0026#34;1\u0026#34; visitedMethods=\u0026#34;1\u0026#34; numMethods=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;Modules\u0026gt; \u0026lt;Module hash=\u0026#34;EA0AA059-8032-41FD-AD74-4BFECC4C8892\u0026#34;\u0026gt; \u0026lt;ModulePath\u0026gt;ClassLibrary1.dll\u0026lt;/ModulePath\u0026gt; \u0026lt;ModuleTime\u0026gt;2022-11-17T03:15:39\u0026lt;/ModuleTime\u0026gt; \u0026lt;ModuleName\u0026gt;ClassLibrary1\u0026lt;/ModuleName\u0026gt; \u0026lt;Files\u0026gt; \u0026lt;File uid=\u0026#34;1\u0026#34; fullPath=\u0026#34;C:\\Users\\Yunier\\source\\repos\\TestProject1\\ClassLibrary1\\Class1.cs\u0026#34; /\u0026gt; \u0026lt;/Files\u0026gt; \u0026lt;Classes\u0026gt; \u0026lt;Class\u0026gt; \u0026lt;Summary numSequencePoints=\u0026#34;1\u0026#34; visitedSequencePoints=\u0026#34;1\u0026#34; numBranchPoints=\u0026#34;0\u0026#34; visitedBranchPoints=\u0026#34;0\u0026#34; sequenceCoverage=\u0026#34;100\u0026#34; branchCoverage=\u0026#34;100\u0026#34; maxCyclomaticComplexity=\u0026#34;1\u0026#34; minCyclomaticComplexity=\u0026#34;1\u0026#34; visitedClasses=\u0026#34;1\u0026#34; numClasses=\u0026#34;1\u0026#34; visitedMethods=\u0026#34;1\u0026#34; numMethods=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;FullName\u0026gt;Library.Calculator\u0026lt;/FullName\u0026gt; \u0026lt;Methods\u0026gt; \u0026lt;Method cyclomaticComplexity=\u0026#34;1\u0026#34; nPathComplexity=\u0026#34;1\u0026#34; sequenceCoverage=\u0026#34;100\u0026#34; branchCoverage=\u0026#34;100\u0026#34; isConstructor=\u0026#34;False\u0026#34; isGetter=\u0026#34;False\u0026#34; isSetter=\u0026#34;False\u0026#34; isStatic=\u0026#34;True\u0026#34;\u0026gt; \u0026lt;Summary numSequencePoints=\u0026#34;1\u0026#34; visitedSequencePoints=\u0026#34;1\u0026#34; numBranchPoints=\u0026#34;0\u0026#34; visitedBranchPoints=\u0026#34;0\u0026#34; sequenceCoverage=\u0026#34;100\u0026#34; branchCoverage=\u0026#34;100\u0026#34; maxCyclomaticComplexity=\u0026#34;1\u0026#34; minCyclomaticComplexity=\u0026#34;1\u0026#34; visitedClasses=\u0026#34;0\u0026#34; numClasses=\u0026#34;0\u0026#34; visitedMethods=\u0026#34;1\u0026#34; numMethods=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;MetadataToken /\u0026gt; \u0026lt;Name\u0026gt;System.Int32 Library.Calculator::Add(System.Int32,System.Int32)\u0026lt;/Name\u0026gt; \u0026lt;FileRef uid=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;SequencePoints\u0026gt; \u0026lt;SequencePoint vc=\u0026#34;1\u0026#34; uspid=\u0026#34;5\u0026#34; ordinal=\u0026#34;0\u0026#34; sl=\u0026#34;5\u0026#34; sc=\u0026#34;1\u0026#34; el=\u0026#34;5\u0026#34; ec=\u0026#34;2\u0026#34; bec=\u0026#34;0\u0026#34; bev=\u0026#34;0\u0026#34; fileid=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;/SequencePoints\u0026gt; \u0026lt;BranchPoints /\u0026gt; \u0026lt;MethodPoint vc=\u0026#34;1\u0026#34; uspid=\u0026#34;0\u0026#34; p8:type=\u0026#34;SequencePoint\u0026#34; ordinal=\u0026#34;0\u0026#34; offset=\u0026#34;0\u0026#34; sc=\u0026#34;0\u0026#34; sl=\u0026#34;5\u0026#34; ec=\u0026#34;1\u0026#34; el=\u0026#34;5\u0026#34; bec=\u0026#34;0\u0026#34; bev=\u0026#34;0\u0026#34; fileid=\u0026#34;1\u0026#34; xmlns:p8=\u0026#34;xsi\u0026#34; /\u0026gt; \u0026lt;/Method\u0026gt; \u0026lt;/Methods\u0026gt; \u0026lt;/Class\u0026gt; \u0026lt;/Classes\u0026gt; \u0026lt;/Module\u0026gt; \u0026lt;/Modules\u0026gt; \u0026lt;/CoverageSession\u0026gt; This is the code coverage report, we can prettify it into an HTML document by using reportgenerator as follows.\n1 reportgenerator -reports:\u0026#34;*.xml\u0026#34; -targetdir:\u0026#34;report\u0026#34; --reporttypes: \u0026#34;Html\u0026#34; The command above can be broken down as follows.\nreportgenerator is the dotnet tool we installed, it will take the XML file, inspect the contect of said file, then create an HTML report from that content. reports is the path to the location of the report you which to convert. Note that it accepted wild cards. You can use **/*.xml for recursive search. targetdir is used to specify the location where the report should be generated. If the folder does not exist, it will be created. reporttypes is used to specify the report format, in this case, Html. The resulting HTML report will be located under the report folder, in that folder, locate the index.html page and you should see the code report summary as showned below.\nAs you can see from the image, an HTML report was generated from the result.xml file, and as you can see from the image the Calculator class has full code coverage.\nXUnit If you are using XUnit and would like to generate code coverage reports as demonstrated above then you are in luck, because generating reports in XUnit is much easier than NUnit. To demonstrate how to generate code coverage reports with XUnit, I will clone the Calculator project mentioned above for NUnit but this time I will target .NET 6.\nI will once again created the Calculator class.\n1 2 3 4 public class Calculator { public int Add(int x, int y) { return x + y; } } and added unit tests but this time using XUnit.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class CalculatorTests { [Fact] public void Calculator_AddingOnePlusTwo_AssertResultIsThree() { // Arrange var x = 1; var y = 2; // Ac var sut = new Calculator(); var result = sut.Add(x, y); // Assert Assert.Equal(3, result); } } To get code coverage, we are going to need to add a few additional NuGet packages. Those packages are as follows.\nxunit.testlogger - This is an XML logger for XUnit. coverlet.collector - Coverlet, the tool used for code coverage. These packages must be installed on the test project by running the following commands.\n1 2 dotnet add Test.csproj package coverlet.collector --version 3.2.0 dotnet add Test.csproj package XunitXml.TestLogger --version 3.0.70 Replace Test.csproj with the path to you project.\nTo generate a code coverage report I will build the solution and then run the following command from a terminal.\n1 dotnet test \u0026#34;XUnit.sln\u0026#34; --configuration Release --collect:\u0026#34;XPlat Code Coverage\u0026#34; --logger:xunit -- DataCollectionRunSettings.DataCollectors.DataCollector.Configuration.Format=opencover The command above can be broken down as follows.\ndotnet test is used to run the unit test project, in my case I use the solution file but you could give it the csproj file of the unit test project. configuration is used to set the .NET configuration, in my case I wanted to run the unit tests under the Release configuration collect set the data collection using a friendly name. The \u0026ldquo;XPlat Code Coverage\u0026rdquo; argument is a friendly name that corresponds to the data collectors from Coverlet. This name is required but is case-insensitive. logger is used to specify a logger for test results. For a list of available options see Available test loggers. DataCollectionRunSettings is used to set the format, in my case opencover. Just like before, the command generates an XML file, in my case the file is located in the same folder as the XUnit project. The file is named coverage.opencover.xml and the content inside is as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;CoverageSession\u0026gt; \u0026lt;Summary numSequencePoints=\u0026#34;1\u0026#34; visitedSequencePoints=\u0026#34;1\u0026#34; numBranchPoints=\u0026#34;0\u0026#34; visitedBranchPoints=\u0026#34;0\u0026#34; sequenceCoverage=\u0026#34;100\u0026#34; branchCoverage=\u0026#34;100\u0026#34; maxCyclomaticComplexity=\u0026#34;1\u0026#34; minCyclomaticComplexity=\u0026#34;1\u0026#34; visitedClasses=\u0026#34;1\u0026#34; numClasses=\u0026#34;1\u0026#34; visitedMethods=\u0026#34;1\u0026#34; numMethods=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;Modules\u0026gt; \u0026lt;Module hash=\u0026#34;15702009-C2DD-4419-BE7E-BD070E0C297A\u0026#34;\u0026gt; \u0026lt;ModulePath\u0026gt;Library.dll\u0026lt;/ModulePath\u0026gt; \u0026lt;ModuleTime\u0026gt;2022-11-17T03:49:17\u0026lt;/ModuleTime\u0026gt; \u0026lt;ModuleName\u0026gt;Library\u0026lt;/ModuleName\u0026gt; \u0026lt;Files\u0026gt; \u0026lt;File uid=\u0026#34;1\u0026#34; fullPath=\u0026#34;C:\\Users\\Yunier\\source\\repos\\XUnit\\Library\\Class1.cs\u0026#34; /\u0026gt; \u0026lt;/Files\u0026gt; \u0026lt;Classes\u0026gt; \u0026lt;Class\u0026gt; \u0026lt;Summary numSequencePoints=\u0026#34;1\u0026#34; visitedSequencePoints=\u0026#34;1\u0026#34; numBranchPoints=\u0026#34;0\u0026#34; visitedBranchPoints=\u0026#34;0\u0026#34; sequenceCoverage=\u0026#34;100\u0026#34; branchCoverage=\u0026#34;100\u0026#34; maxCyclomaticComplexity=\u0026#34;1\u0026#34; minCyclomaticComplexity=\u0026#34;1\u0026#34; visitedClasses=\u0026#34;1\u0026#34; numClasses=\u0026#34;1\u0026#34; visitedMethods=\u0026#34;1\u0026#34; numMethods=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;FullName\u0026gt;Library.Calculator\u0026lt;/FullName\u0026gt; \u0026lt;Methods\u0026gt; \u0026lt;Method cyclomaticComplexity=\u0026#34;1\u0026#34; nPathComplexity=\u0026#34;1\u0026#34; sequenceCoverage=\u0026#34;100\u0026#34; branchCoverage=\u0026#34;100\u0026#34; isConstructor=\u0026#34;False\u0026#34; isGetter=\u0026#34;False\u0026#34; isSetter=\u0026#34;False\u0026#34; isStatic=\u0026#34;True\u0026#34;\u0026gt; \u0026lt;Summary numSequencePoints=\u0026#34;1\u0026#34; visitedSequencePoints=\u0026#34;1\u0026#34; numBranchPoints=\u0026#34;0\u0026#34; visitedBranchPoints=\u0026#34;0\u0026#34; sequenceCoverage=\u0026#34;100\u0026#34; branchCoverage=\u0026#34;100\u0026#34; maxCyclomaticComplexity=\u0026#34;1\u0026#34; minCyclomaticComplexity=\u0026#34;1\u0026#34; visitedClasses=\u0026#34;0\u0026#34; numClasses=\u0026#34;0\u0026#34; visitedMethods=\u0026#34;1\u0026#34; numMethods=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;MetadataToken /\u0026gt; \u0026lt;Name\u0026gt;System.Int32 Library.Calculator::Add(System.Int32,System.Int32)\u0026lt;/Name\u0026gt; \u0026lt;FileRef uid=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;SequencePoints\u0026gt; \u0026lt;SequencePoint vc=\u0026#34;1\u0026#34; uspid=\u0026#34;5\u0026#34; ordinal=\u0026#34;0\u0026#34; sl=\u0026#34;5\u0026#34; sc=\u0026#34;1\u0026#34; el=\u0026#34;5\u0026#34; ec=\u0026#34;2\u0026#34; bec=\u0026#34;0\u0026#34; bev=\u0026#34;0\u0026#34; fileid=\u0026#34;1\u0026#34; /\u0026gt; \u0026lt;/SequencePoints\u0026gt; \u0026lt;BranchPoints /\u0026gt; \u0026lt;MethodPoint vc=\u0026#34;1\u0026#34; uspid=\u0026#34;0\u0026#34; p8:type=\u0026#34;SequencePoint\u0026#34; ordinal=\u0026#34;0\u0026#34; offset=\u0026#34;0\u0026#34; sc=\u0026#34;0\u0026#34; sl=\u0026#34;5\u0026#34; ec=\u0026#34;1\u0026#34; el=\u0026#34;5\u0026#34; bec=\u0026#34;0\u0026#34; bev=\u0026#34;0\u0026#34; fileid=\u0026#34;1\u0026#34; xmlns:p8=\u0026#34;xsi\u0026#34; /\u0026gt; \u0026lt;/Method\u0026gt; \u0026lt;/Methods\u0026gt; \u0026lt;/Class\u0026gt; \u0026lt;/Classes\u0026gt; \u0026lt;/Module\u0026gt; \u0026lt;/Modules\u0026gt; \u0026lt;/CoverageSession\u0026gt; And just like before, I can prettify the XML by converting it to an HTML document using the following command.\n1 reportgenerator -reports:\u0026#34;**/*.xml\u0026#34; -targetdir:\u0026#34;report\u0026#34; --reporttypes: \u0026#34;Html\u0026#34; The command above can be broken down as follows.\nreportgenerator is the dotnet tool we installed. reports is the path to the location of the report you which to convert. Note that it accepted wild cards. Note the usage of double wildcards for recursive search. targetdir is used to specify the location where the report should be generated. If the folder does not exist, it will be created. reporttypes is used to specify the report format, in this case, Html. As you can see, I get an HTML report that is very similar to the one generated in the NUnit example, which is expected.\nConclusion I hope these two examples can guide you in adding cove coverage reports to your projects. Feel free to reach out if you have any questions.\n","permalink":"http://localhost:1313/post/2022/code-coverage/","summary":"\u003cp\u003eIf you are writing unit tests in .NET, you may eventually find the need to generate code coverage reports to show how much coverage your project has. The best tool for code coverage reports in my experience has been \u003ca href=\"https://github.com/coverlet-coverage/coverlet\"\u003ecoverlet\u003c/a\u003e because it supports both .NET Framework and .NET Core.\u003c/p\u003e\n\u003ch3 id=\"nunit\"\u003eNUnit\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://nunit.org/\"\u003eNUnit\u003c/a\u003e, the tried and tested framework originally being a port of \u003ca href=\"https://junit.org/junit5/\"\u003eJUnit\u003c/a\u003e. A powerful tool that when combined with \u003ca href=\"https://www.nuget.org/packages/coverlet.console\"\u003ecoverlet console\u003c/a\u003e can be used to generate code coverage reports. To demonstrate, I will create an NUnit test project targeting .NET Framework 4.8 along with a Class Library type project also targeting .NET Framework 4.8\u003c/p\u003e","title":"Code Coverage In .NET"},{"content":"If you find yourself in need of having to use Kubernetes in WSL, know that it is possible, hard, but possible. It might require upgrading your machine to Windows 11 if you are on Windows 10 and a few other packages.\nPrerequisite To get started we need to know what version of Windows you are on. This is important because Kubernetes will be installed using Microk8s, which requires having snap installed and working. Snap won\u0026rsquo;t work in older Windows builds.\nAs of November 2022, I recommend being on Windows build version 22621 or higher. You can find out what version you are on by running the following command on Powershell\n1 systeminfo | findstr /B /C:\u0026#34;OS Name\u0026#34; /C:\u0026#34;OS Version\u0026#34; It should output something similar to the following snippet.\n1 2 OS Name: Microsoft Windows 11 Home OS Version: 10.0.22621 N/A Build 22621 Next, install or have WSL installed, you can follow the official guide Install Linux on Windows with WSL. Do not install a distribution through WSL, instead install your desired distribution using Windows Store. This is important as the distro on the Windows Store tends to work better.\nWith WSL installed, confirm you are on the right version using the following command.\n1 wsl -v It should output something similar to the following snippet.\n1 2 3 4 5 6 7 WSL version: 0.70.4.0 Kernel version: 5.15.68.1 WSLg version: 1.0.45 MSRDC version: 1.2.3575 Direct3D version: 1.606.4 DXCore version: 10.0.25131.1002-220531-1700.rs-onecore-base2-hyp Windows version: 10.0.22621.674 As of November 2022, you should be on version 0.70.4.0 or higher. If you run the command above and don\u0026rsquo;t see an output or nothing happens, then it means you are on an old WSL version, you will need to update your version of WSL. You can do so by running the following command from Powershell.\n1 wsl --update Time to install a distribution, simply open the Windows Store and find the distro you would like to use. Personally, I recommend sticking to an Ubuntu-based distro. For this post, I will use Ubuntu 20.04.5 LTS.\nOnce you have the distro installed verify you have an internet connection from within WSL by running the following ping command.\n1 ping google.com Internet If the command above failed then it means you can\u0026rsquo;t reach the internet from WSL. Please read my Connect To The Internet From WSL post before continuing.\nSnap Time to verify which version of Snap is installed on WSL. Run the following commands.\nFirst, update the WSL instance with the latest packages which include the latest version of snap that as of November 2022 is version 2.57.5+20.04.\n1 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y Then confirm you have snap version 2.57.5+20.04 or higher by running the following command.\n1 snap --version You should get something similar to the following code snippet.\n1 2 3 4 5 snap 2.57.5+20.04 snapd unavailable series 16 Windows Subsystem for Linux - kernel 5.15.68.1-microsoft-standard-WSL2 (amd64) With Snap, all the great apps in the Linux ecosystem are in our hands. You can browse available snaps on snapcraft.\nSystemd The last WSL configuration needed is systemd, it can be enabled on WSL using the following command. In the future, this setting should be automatically enabled for you when you install WSL.\nFirst, use nano to open wsl.conf\n1 sudo nano /etc/wsl.conf With wsl.conf opened, add the following lines.\n1 2 [boot] systemd=true Close nano, exist the WSL instance, and execute the following code in a Powershell terminal to restart WSL.\n1 wsl --shutdown Microk8s Now that the WSL instance has been properly configured, Microk8s can be installed. Microk8s is a lightweight Kubernetes distribution that is designed to run on local systems. It was developed by Canonical, the same company that develops Ubuntu.\nTo install run the following command.\n1 sudo snap install microk8s --classic With microk8s installed, we can configure the microk8s alias for kubectl using the following commands.\n1 2 3 sudo snap alias microk8s.kubectl kubectl sudo usermod -a -G microk8s \u0026#34;WSL Username\u0026#34; newgrp microk8s Where \u0026ldquo;WSL Username\u0026rdquo; is the username of your WSL instance, it will be in your WSl terminal before the @ symbol. If you are not sure run the following command.\n1 whoami In my case, the output is the word \u0026ldquo;bleach\u0026rdquo;, so the command to update the alias is as follows.\n1 sudo usermod -a -G microk8s bleach After running all of the commands above you should be able to run kubectl without microk8s, confirm by running the following command.\n1 kubectl get node The command should output something similar to the following code snippet.\n1 2 NAME STATUS ROLES AGE VERSION gohan Ready \u0026lt;none\u0026gt; 15m v1.25.3 If so, then congratulations, you have successfully installed and configured Kubernetes on WSL.\nA quick note, WSL shares the same localhost as your Windows machine, this is why you can run an app on localhost on WSL and be able to access it on your windows machine. With Kubernetes, there is an additional configuration required to get this working. I will cover those configurations in a future post.\n","permalink":"http://localhost:1313/post/2022/use-kubernetes-in-wsl/","summary":"\u003cp\u003eIf you find yourself in need of having to use Kubernetes in WSL, know that it is possible, hard, but possible. It might require upgrading your machine to Windows 11 if you are on Windows 10 and a few other packages.\u003c/p\u003e\n\u003ch2 id=\"prerequisite\"\u003ePrerequisite\u003c/h2\u003e\n\u003cp\u003eTo get started we need to know what version of Windows you are on. This is important because Kubernetes will be installed using Microk8s, which requires having snap installed and working. Snap won\u0026rsquo;t work in older Windows builds.\u003c/p\u003e","title":"Use Kubernetes In WSL"},{"content":"A few months ago I was looking for a new HTTP client to use within my applications. I first checked on awesome dotnet under the HTTP section to see what projects the .NET community is using instead of the default HTTP client. One that immediately stands out is RestSharp, this project has been around for a while and is overall a good choice, but I was looking for something new and fresh, that is when I came across Flurl.\nFlurl, according to its own website, is a modern, fluent, asynchronous, testable, portable, buzzword-laden URL builder and HTTP client library for .NET. That is quite a statement to make, I wanted to see if Flurl held up to that statement by creating a few use cases to see how Flurl works and how it could be used.\nTo demonstrate, I will create a new console application on .NET 6, the app will use Flurl to make an API request, get the response, and serialize it to a JSON object. I also want to demonstrate how Flurl makes testing super easy using its fake HTTP mode.\nFirst, creating the .NET 6 console app involves executing the following command in a terminal.\n1 dotnet new console Now that I have my project, I need to add Flurl using the following commands.\n1 dotnet add package Flurl.Http --version 3.2.4 Now I need an API that I can invoke with Flurl. This is when I like to use HTTPBin. HTTPBin is a simple request/response service, it allows you to test different aspects of the HTTP spec like headers, codes, statuses, and so on.\nFor my first HTTP request, I will use the Headers API, this API takes the headers from your request and returns them as the payload on the response as seen below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Accept-Encoding\u0026#34;: \u0026#34;gzip, deflate, br\u0026#34;, \u0026#34;Accept-Language\u0026#34;: \u0026#34;en-US,en\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;, \u0026#34;Referer\u0026#34;: \u0026#34;https://httpbin.org/\u0026#34;, \u0026#34;Sec-Fetch-Dest\u0026#34;: \u0026#34;empty\u0026#34;, \u0026#34;Sec-Fetch-Mode\u0026#34;: \u0026#34;cors\u0026#34;, \u0026#34;Sec-Fetch-Site\u0026#34;: \u0026#34;same-origin\u0026#34;, \u0026#34;Sec-Gpc\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\u0026#34;, \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-636b1492-6c9692c6282b170967942576\u0026#34; } } Now that I know what the JSON payload will be I can use a tool like json2csharp to create a model that will bind to the API response. Quick note, if using Visual Studio, for a while now there has been an option under paste that creates a model from a JSON or XML payload. It is under Edit \u0026gt; Special Paste \u0026gt; JSON for JSON or Edit \u0026gt; Special Paste \u0026gt; XML for XML.\nUsing json2charp, the JSON payload above is converted into the following model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class Headers { public string Accept { get; set; } [JsonProperty(\u0026#34;Accept-Encoding\u0026#34;)] public string AcceptEncoding { get; set; } [JsonProperty(\u0026#34;Accept-Language\u0026#34;)] public string AcceptLanguage { get; set; } public string Host { get; set; } [JsonProperty(\u0026#34;Sec-Fetch-Dest\u0026#34;)] public string SecFetchDest { get; set; } [JsonProperty(\u0026#34;Sec-Fetch-Mode\u0026#34;)] public string SecFetchMode { get; set; } [JsonProperty(\u0026#34;Sec-Fetch-Site\u0026#34;)] public string SecFetchSite { get; set; } [JsonProperty(\u0026#34;Sec-Fetch-User\u0026#34;)] public string SecFetchUser { get; set; } [JsonProperty(\u0026#34;Sec-Gpc\u0026#34;)] public string SecGpc { get; set; } [JsonProperty(\u0026#34;Upgrade-Insecure-Requests\u0026#34;)] public string UpgradeInsecureRequests { get; set; } [JsonProperty(\u0026#34;User-Agent\u0026#34;)] public string UserAgent { get; set; } [JsonProperty(\u0026#34;X-Amzn-Trace-Id\u0026#34;)] public string XAmznTraceId { get; set; } } public class Root { public Headers headers { get; set; } } Where Root is the top-level representation of the JSON document that is returned by HTTPBin.\nNow that I have my response model, I can make the API request using the following code. Three lines of code are all that are needed to make an HTTP request to the endpoint https://httpbin.org/headers using Flurl.\nExample 1 2 3 var result = await \u0026#34;https://httpbin.org\u0026#34; .AppendPathSegment(\u0026#34;headers\u0026#34;) .GetJsonAsync\u0026lt;Root\u0026gt;(); Simple and super easy. The fluent style interface exposed by Flurl also provides methods to work with POST, PUT, DELETE, adding headers, and using authentication.\nTesting As for testing, it involves putting Flurl into test mode, this can be done using HttpTest class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class FlurlTests { [Fact] public async Task AssertHTTPGetCallsHttpBin() { // Act var httpTest = new HttpTest() // Arrange var sut = await \u0026#34;https://httpbin.org\u0026#34; .AppendPathSegment(\u0026#34;headers\u0026#34;) .GetJsonAsync\u0026lt;Root\u0026gt;() // Assert httpTest.ShouldHaveCalled(\u0026#34;https://httpbin.org/headers\u0026#34;) } } With Flurl in test mode, all requests and configurations can be faked and controlled giving you the ability to test complex scenarios. If you are using dependency injection and Flurl\u0026rsquo;s IFlurlClientFactory you are going to need to inject PerBaseUrlFlurlClientFactory or provide your own implementation of IFlurlClientFactory.\nError Handling Error handling in Flurl is a bit different, the default behavior is to throw an exception on any calls that do not result in a status code that is in the 200 range. I like this behavior because it works great if you use an exception handling middleware that can take an exception thrown by Flurl and convert the exception into a problem details response.\n","permalink":"http://localhost:1313/post/2022/fun-with-flurl/","summary":"\u003cp\u003eA few months ago I was looking for a new HTTP client to use within my applications. I first checked on \u003ca href=\"https://github.com/quozd/awesome-dotnet\"\u003eawesome dotnet\u003c/a\u003e under the \u003ca href=\"https://github.com/quozd/awesome-dotnet#http\"\u003eHTTP section\u003c/a\u003e to see what projects the .NET community is using instead of the default HTTP client. One that immediately stands out is \u003ca href=\"https://github.com/restsharp/RestSharp\"\u003eRestSharp\u003c/a\u003e, this project has been around for a while and is overall a good choice, but I was looking for something new and fresh, that is when I came across \u003ca href=\"https://flurl.dev/\"\u003eFlurl\u003c/a\u003e.\u003c/p\u003e","title":"Fun With Flurl"},{"content":"Problem You have installed WSL successfully on your machine only to find out that you cannot connect to the internet. I have encountered this problem before sometimes it can be fixed externally, but from my experience, no matter what you do, you will end up having to mock around with the resolv.conf file within WSL, more on that later.\nYou may encounter an internet issue in WSL when the network administrator had configured Windows Defender to not allow local fire rules to be merged with rules applied at the enterprise level. You can confirm that this is applicable to you by searching for Windows Defender Firewall with Advance Security on the start menu, then going to Action, then Properties, under properties switching to the Public Profile tab, then clicking customize under settings. Now look under \u0026ldquo;Rule Merging\u0026rdquo;, if these options are set to no, then you will not be able to connect from WSL.\nYour first option here is to talk to the network administrator, and see if they can change the rule. If they can, then great if not then you will have to follow the guide at the end of this blog post.\nAnother way you can encounter this issue is when you actually have internet access but you cannot connect to resources behind a VPN. As of October 2022, the only solution that I am aware of that works is modifying the resolve.conf file within WSL. Keep an eye on the following GitHub issues,5068,4277, 1350, I\u0026rsquo;m hoping that this problem is eventually fixed and becomes a setting that can be applied when WSL is installed.\nWSL 101 Before I get into the solution I would like to explain what is happening. When you launch WSL by default it is configured to write a resolv.config file in the /etc directory. This file is what controls DNS resolution in WSL. You can see the contect of the file by running the following command from WSL\n1 sudo nano /etc/resolv.config The content of the file resembles the following code snippet.\n1 2 3 4 # This file was automatically generated by WSL. To stop automatic generation of this file, add the following \u0026gt; # [network] # generateResolvConf = false nameserver XXX.XX.XX.X Where nameserver point to the IP of your machine. What needs to be done is to change the nameserver to point to another DNS provider like 8.8.8.8 (Google) or 1.1.1.1 (Cloudflare), before that, the setting that makes WSL generates a new resolv.config on starts needs to be changed, otherwise, you would lose your settings on every boot of WSL.\nSolution The first step in getting internet in WSL is to update the file wsl.conf located in the /etc directory. This file applies WSL setting per WSL distribution, if you would like to apply a setting across all distributions then you will need to modify the .wslconfig file. For more information, visit Advanced settings configuration in WSL.\nBack to wsl.cofig, run the following command to modify the file.\n1 sudo nano /etc/wsl.conf If this is your first time opening this file then it is more than likely empty, if not delete the content and replace it with the following code snippet.\n1 2 3 [network] generateHosts = false generateResolvConf = false Save the file and exit nano.\nTime to update the resolv.conf file, you can open the file by running the following code snippet\n1 sudo nano /etc/resolv.conf Change the nameserver to your desired provider, in my case I prefer 1.1.1.1 from Cloudflare, so my file ends up looking like the following code snippet.\n1 2 3 4 # This file was automatically generated by WSL. To stop automatic generation of this file, add the following \u0026gt; # [network] # generateResolvConf = false nameserver 1.1.1.1 Save and exit nano. Run a ping command.\n1 ping google.com You should get back a response similar to the code snippet below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 PING google.com (142.250.64.174) 56(84) bytes of data. 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=1 ttl=113 time=21.2 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=2 ttl=113 time=23.1 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=3 ttl=113 time=22.3 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=4 ttl=113 time=19.4 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=5 ttl=113 time=19.4 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=6 ttl=113 time=21.2 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=7 ttl=113 time=19.2 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=8 ttl=113 time=20.2 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=9 ttl=113 time=19.7 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=10 ttl=113 time=18.4 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=11 ttl=113 time=19.0 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=12 ttl=113 time=20.6 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=13 ttl=113 time=22.3 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=14 ttl=113 time=22.5 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=15 ttl=113 time=20.2 ms 64 bytes from mia09s22-in-f14.1e100.net (142.250.64.174): icmp_seq=16 ttl=113 time=22.2 ms Congratulations, you have internet access.\nImportant, to make these changes permanent you will need to shut down WSL. On a shell/terminal from the host machine, not WSL, run the following command.\n1 wsl --shutdown Wait a few seconds, then open WSL again, and confirm that you still have internet access, if not double-check yourself, and start by confirming that a new instance of WSL did not reset the resolv.conf file.\nVPN Oh, right. You followed the instructions above and were able to restore internet access within WSL. However, you still cannot connect to resources that are behind a VPN. The solution to this problem is simple, you have to add the IP of your VPN provider.\nOn a shell/terminal on the host machine, not WSL, run nslookup while connected to the VPN, this is essential, you must be connected to the VPN.\n1 nslookup The command should output a result that is similar to the following code snippet.\n1 2 Default Server: xoxoxox Addresss: YY.Y.YYY.YY Copy the IP address, open WSL, and run the following command to open resolv.conf\n1 sudo nano /etc/resolv.conf Update the content by adding another nameserver, you can put it before or after doesn\u0026rsquo;t matter. You should note though that there is a 5-second timeout in WSL. So if you look up some DNS and the first server doesn\u0026rsquo;t find it, it will take about 5 seconds for the first DNS server to time out, then the second DNS server will do a DNS lookup.\nAfter updating the resolv.conf file, the content may look similar to the following code snippet. Where YY.Y.YYY.YY represents the IP obtained from the nslookup command.\n1 2 3 4 5 # This file was automatically generated by WSL. To stop automatic generation of this file, add the following \u0026gt; # [network] # generateResolvConf = false nameserver YY.Y.YYY.YY nameserver 1.1.1.1 Do another ping against google to confirm the internet is still accessible, then do another ping against a resource that seats behind the VPN.\nBoth ping commands should work.\n","permalink":"http://localhost:1313/post/2022/connect-to-the-internet-from-wsl/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eYou have installed WSL successfully on your machine only to find out that you cannot connect to the internet. I have encountered this problem before sometimes it can be fixed externally, but from my experience, no matter what you do, you will end up having to mock around with the resolv.conf file within WSL, more on that later.\u003c/p\u003e\n\u003cp\u003eYou may encounter an internet issue in WSL when the network administrator had configured Windows Defender to not allow local fire rules to be merged with rules applied at the enterprise level. You can confirm that this is applicable to you by searching for \u003cstrong\u003eWindows Defender Firewall with Advance Security\u003c/strong\u003e on the start menu, then going to Action, then Properties, under properties switching to the Public Profile tab, then clicking \u003cstrong\u003ecustomize\u003c/strong\u003e under settings. Now look under \u0026ldquo;Rule Merging\u0026rdquo;, \u003cstrong\u003eif these options are set to no\u003c/strong\u003e, then you will not be able to connect from WSL.\u003c/p\u003e","title":"Connect To The Internet From WSL"},{"content":"The Postman app is an excellent tool for building and testing Web APIs. It gets even better when you combine it with Newman, which allows you to execute your Postman scripts on a continuous integration system like Bitbucket Pipelines. While both Postman and Newman are awesome, you may encounter issues while working with both apps.\n1 2 3 if(pm.request.body.isEmpty){ // Code Omitted For Brevity } One issue you may encounter is having a script that was written in Postman, successfully tested using Postman, fail when executed using Newman. For example, the script above, which was written in Postman, is part of a series of tests that inspects the HTTP request body. The script can fail when executed on Newman because the function isEmpty does not exist.\nThe error generated by Newman is \u0026ldquo;Cannot read properties of undefined (reading \u0026lsquo;isEmpty\u0026rsquo;)\u0026rdquo;.\nIf you have done enough JavaScript development, you would know that the error means the function was invoked before it could be loaded. What is happening here is that in the Postman sandbox, the app you use to build your API collection and tests imports some external libraries implicitly, for example, in Postman you can do something like the following code snippet in a Pre-request script or test.\n1 var bodyAsBase64 = CryptoJS.enc.Base64.strigify(pm.request.body.raw); The code above works even though in any other JavaScript runtime like Node.js you would first need to import it CryptoJS as seen in the following code snippet.\n1 2 3 var CryptoJS = require(\u0026#34;crypto-js\u0026#34;); var ciphertext = CryptoJS.AES.encrypt(\u0026#39;my message\u0026#39;, \u0026#39;secret key 123\u0026#39;).toString(); console.log(\u0026#34;ciphertext=\u0026#34;, ciphertext); This is why it is best to always be explicit when importing external dependencies on your Postman script. To solve our issue of the function isEmpty not being defined. We need to import the postman sdk because that is where the function isEmpty is defined. Our original code needs to be updated.\n1 2 3 4 5 6 7 8 var sdk = require(\u0026#39;postman-collection\u0026#39;); var options = { raw = pm.request.body } var body = new sdk.RequestBody(options) if(body.isEmpty){ // Code Omitted For Brevity } First, as mentioned, import the Postman SDK, then create an options object using the request body, then instantiate the SDK and call the class you want to use, since we are working with a request body, the class RequestBody is called with the previously defined options being passed to it.\nNow, when the code is executed in Newman, it will not fail because the function isEmpty will be defined when invoked by your script.\n","permalink":"http://localhost:1313/post/2022/newman-function-is-not-defined/","summary":"\u003cp\u003eThe Postman app is an excellent tool for building and testing Web APIs. It gets even better when you combine it with Newman, which allows you to execute your Postman scripts on a continuous integration system like Bitbucket Pipelines. While both Postman and Newman are awesome, you may encounter issues while working with both apps.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-javascript\" data-lang=\"javascript\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003epm\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003erequest\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ebody\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003eisEmpty\u003c/span\u003e){\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// Code Omitted For Brevity\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eOne issue you may encounter is having a script that was written in Postman, successfully tested using Postman, fail when executed using Newman. For example, the script above, which was written in Postman, is part of a series of tests that inspects the HTTP request body. The script can fail when executed on Newman because the function isEmpty does not exist.\u003c/p\u003e","title":"Newman - Function Is Not Defined"},{"content":"I was recently presented with a unique challenge at work. I needed to create a script that clones repositories from Bitbucket. The problem is that as of June 2022, Bitbucket only supports managing repositories using OAuth via two grant types, the authorization code grant \u0026amp; the implicit grant.\nI won\u0026rsquo;t get into the details here but the implicit grant is no longer recommended and is in fact discouraged from ever being used. Regardless of which flow I use I will end up facing the same problem, the browser. In both the implicit and authorization grant, user interaction (3-legged OAuth) is required, the end-user must provide their credentials in order to properly authenticate, in some cases this may even include multifactor authentication.\nNormally this isn\u0026rsquo;t much of a problem, except in the case where the end-user is running a script from a terminal like Powershell. When the script runs, a browser must be opened to some predefined URL that will prompt the user to authenticate, when this happens the identity provider, in my case Bitbucket, will redirect the user back to another predefined URL, in the redirect URL, the authorization code will be included. This authorization code must then be exchanged for an access token.\nThe challenged I faced was how to retrieve the authorization code from the redirect URL? The redirect happens on the web browser but the script is being run under a terminal. Luckily I have some experience working with OAuth and generally understood what needed to be done at a high level. When the redirect occurs, a listener must be ready to extract the authorization code from the redirect URL and inject it into the terminal executing the script and while I knew what needed to be done I myself have never done anything like this before, hence my statement of being presented with a unique challenge.\nI want to take the time to document my solution to the problem above in today\u0026rsquo;s post. Instead of Bitbucket, I\u0026rsquo;ll use Google for my example. If you want to follow along, I suggest reading Getting Google OAuth Access Token using Google APIs by Osanda Deshan Nimalarathna to properly configure your OAuth client.\nWith a properly configured OAuth client, we can move on to the next step, the authorization code listener. The purpose of the listener is to grab the authorization token from the redirect URL. This is done by having a local web server that listens for the redirect URL, then when the user is redirected the listener outputs the authorization code to stdout. By writing the code to stdout, a terminal like Powershell or any Linux terminal can save the authorization code value to a variable that can later be referenced when the code is used to get an access token.\nA few implications to be aware of. First, the redirect URL must be localhost with a port of your choosing. Second, the client redirect URL must be the same localhost URL. You can choose to redirect to the root for example, with something like http:localhost:9000 or http:localhost:9000/auth/callback. The redirect URL can be anything you want, up to you, you just need to make sure the local server is running and listening to any request made to the redirect URL endpoint otherwise you will not be able to capture the authorization code.\nI ultimately ended up building the listener with .NET but you can use anything you want. Some languages like python come with a built-in local server that can be executed using the following command.\n1 python3 -m http.server Other options include using great packages like http-server if you prefer to write the listener in Node.js. The server implementation can be done in any language that can write to stdout. As for the .NET implementation, I wasn\u0026rsquo;t sure how to go about doing it, I decided to search and came across a blog post from Dean Ward titled Authenticating to Google using PowerShell and OAuth which pointed me in the right direction. Below is the code created by Dean.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Program { static readonly CancellationTokenSource _cts = new CancellationTokenSource(); static Task Main(string[] args) =\u0026gt; WebHost.CreateDefaultBuilder() // prevent status messages being written to stdout .SuppressStatusMessages(true) // disable all logging to stdout .ConfigureLogging(config =\u0026gt; config.ClearProviders()) // listen on the port passed on the args .UseKestrel(options =\u0026gt; options.ListenLocalhost(int.Parse(args[0]))) .Configure(app =\u0026gt; app.Run(async context =\u0026gt; { var message = \u0026#34;ERROR! Unable to retrieve authorization code.\u0026#34;; if (context.Request.Query.TryGetValue(\u0026#34;code\u0026#34;, out var code)) { // we received an authorization code, output it to stdout Console.WriteLine(code); message = \u0026#34;Done!\u0026#34;; } await context.Response.WriteAsync(message + \u0026#34; Check Dev-Local-Setup to continue\u0026#34;); // cancel the cancellation token so the server stops _cts.Cancel(); }) ) .Build() // run asynchronously using the cancellation token // to signal when the process should end. This will be awaited // by the framework and the process will end when the cancellation // token is signalled. .RunAsync(_cts.Token); } As you can see, the code creates a local web server that listens on localhost on a port that is passed as a parameter when the server is started. I tried to port the code to .NET 6 but it did not work. The code above was written on .NET Core 3. There is a substantial difference between .NET Core 3.1 and .NET in terms of how the server is initialized, therefore, I had to slightly rewrite Dean\u0026rsquo;s code.\nThe code provided by Dean in .NET 6 ended up like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 public class Program { private static readonly CancellationTokenSource CancellationTokenSource = new CancellationTokenSourc(); public static async Task Main(string[] args) { var builder = WebApplication.CreateBuilder(args); builder.WebHost.SuppressStatusMessages(true); builder.WebHost.ConfigureLogging(loggingBuilder =\u0026gt; loggingBuilder.ClearProviders()); builder.WebHost.UseKestrel(options =\u0026gt; { options.ListenLocalhost(int.Parse(args[0])); }); var app = builder.Build(); app.Map(\u0026#34;/oauth/callback\u0026#34;, HandleCallback); await app.RunAsync(CancellationTokenSource.Token); } private static void HandleCallback(IApplicationBuilder app) { app.Run(context =\u0026gt; { if (context.Request.Query.TryGetValue(\u0026#34;code\u0026#34;, out var code)) { // we received an authorization code, output it to stdout Console.WriteLine(code); CancellationTokenSource.Cancel(); } return Task.CompletedTask; }); } } My code is very similar to Dean\u0026rsquo;s code though I should point out a few things. Just like Dean\u0026rsquo;s code, the setting to suppress status code messages is still set to true. This is essential because status messages are the messages you see when you run a .NET application and those messages are written to stdout. Without this setting, stdout would be polluted with data that we do not need, which means we would need additional work to properly parse the data to get the authorization code. Again, keep stdout clean of any unnecessary messages.\nThe next step is two clean the logging providers which leaves only the console as a provider. This is an optional step in a project this small. The next step is to configure kestrel, just like in Dean\u0026rsquo;s code we take the port as an argument that was passed to the application when the command dotnet run was executed. Next, the web app is built and we map our redirect URL to a handler, the handler will attempt to get the \u0026ldquo;code\u0026rdquo; parameter from the redirect URL and write to stdout. This means the value will be available for any terminal to use, then using a cancellation source, a cancel signal is sent to shut down the listener. This step is essential, the cancellation signal tells .NET that the server can be shut done. You don\u0026rsquo;t want to be in a state where the local server is running indefinitely.\nOne last tip, the most important, do not forget to await app.RunAsync. I was having a lot of trouble when I was testing, the local server was never available this is because I was not awaiting the task. So, don\u0026rsquo;t forget to use the await keyword to await app.RunAsync, take a look at .NET Generic Host in ASP.NET Core for more details.\nEverything is now ready for us to capture the authorization code from a terminal. All that missing is the script that will execute all of this work. Dean\u0026rsquo;s script works but I found out that it also required some small modifications. Below you will find my modified PowerShell script.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 function Get-AccessToken { $accessToken = $null; $port = 8000 $clientId = \u0026#34;\u0026lt;your client id\u0026gt;\u0026#34; $clientSecret = \u0026#34;\u0026lt;your client secret\u0026gt;\u0026#34; $redirectUrl = \u0026#34;http://localhost:$port/oauth/callback\u0026#34; $url = \u0026#34;https://accounts.google.com/o/oauth2/auth?scope=https://www.googleapis.com/auth/drive\u0026amp;response_type=code\u0026amp;access_type=offline\u0026amp;redirect_uri=$redirectUrl\u0026amp;client_id=$clientId\u0026#34; Write-Host \u0026#34;Launching web browser to authenticate...\u0026#34; Start $url # Build the .NET solution dotnet build -c Release \u0026#39;path to project or solution\u0026#39; # Run the .NET solution $authorizationCode = \u0026amp; dotnet run -c Release --no-build -project \u0026#39;path to project or solution\u0026#39; -- $port if ($authorizationCode) { $authorizationResponse = Invoke-RestMethod -Uri \u0026#34;https://www.googleapis.com/oauth2/v4/token?code=$authorizationCode\u0026amp;client_id=$clientId\u0026amp;client_secret=$clientSecret\u0026amp;redirect_uri=http://localhost:$port\u0026amp;grant_type=authorization_code\u0026#34; -Method Post $accessToken = $authorizationResponse.access_token } else { Write-Host \u0026#34;Unexpected error while retrieving access token\u0026#34; -ForegroundColor Red } return $accessToken; } Something you may notice from the above Powershell is that the commands that build and run the local server are separate. This was done on purpose, you could combine both commands into a single command, dotnet run, because dotnet runs executes dotnet build by default. However, I found a flaw to this approach, when you run dotnet run, dotnet build will be executed and in that execution, logs will be written to stdout, which again is something we want to avoid, we want to keep stdout clean of any messages or logs. So my solution was to split the commands into a build and run and to use the \u0026ndash;no\u0026ndash;build flag on dotnet run.\nBefore I end this post, I do want to let you know that I did look at other options such as using a WebView window as described by Stephen Owen in Using PowerShell and oAuth. I found WebView to be vastly inferior, it lacks the proper controls and tools that modern browsers providers, such as the convenience of a password manager that can auto-fill your information or the fact that you may already be authenticated and don\u0026rsquo;t have to authenticate again. If you are interested in using WebView you can find a sample code over in this repo.\n","permalink":"http://localhost:1313/post/2022/authorization-code-from-terminal/","summary":"\u003cp\u003eI was recently presented with a unique challenge at work. I needed to create a script that clones repositories from Bitbucket. The problem is that as of June 2022, Bitbucket only supports managing repositories using OAuth via two grant types, the authorization code grant \u0026amp; the implicit grant.\u003c/p\u003e\n\u003cp\u003eI won\u0026rsquo;t get into the details here but the implicit grant is no longer recommended and is in fact discouraged from ever being used. Regardless of which flow I use I will end up facing the same problem, the browser. In both the implicit and authorization grant, user interaction (3-legged OAuth) is required, the end-user must provide their credentials in order to properly authenticate, in some cases this may even include multifactor authentication.\u003c/p\u003e","title":"Authorization Code From Terminal"},{"content":"I was recently talking to another developer about the importance of never exposing internal identifiers to the outside world. A well-known example of this is using an auto-incrementing identity field in SQL and exposing that field through an API. A client can look at the highest number to tell how many records exist, in an ordering system this is far from ideal. Now everyone will know how many orders you have created. I recommend watching The Internet of Pwned Things by Troy Hunt for a real-world example.\nOne way to avoid the problem above is by simply hashing/encoding your identifiers. You can use a UUID but as Planet Scale points out in their Why we chose NanoIDs for PlanetScale\u0026rsquo;s API blog post, UUIDs don\u0026rsquo;t provide a good developer experience. As an API developer, I want to provide the best DX possible.\nI started thinking about the process of hashing identifiers, and I wanted to build one from the ground up to expand my knowledge and understanding. Now I could build a C# version of the nano id implementation, but that already exists. Instead, what I will do is tackle another known problem, URL shortening. The idea here is to take a URL like https://www.yunier.dev and convert it into something like https://tinyurl.com/yckpk37h, where yckpk37h is more than likely the hashed representation of the internal identifier in the database used by TinyURL. I\u0026rsquo;m honestly guessing here but I know that is how a typical URL shortening system is built.\nThe process of shortening a URL can be broken down into the following steps.\nCheck if the URL provided exists, which means we have probably already shortened it. If the URL has previously been shortened then return the short URL. Otherwise, create a new record, which creates a new identifier. Hash the identifier to get the long URL. Save the short URL and return it. The following diagram provides a high-level overview of the process described above. The diagram was built using Excalidraw. I want to implement steps three through five. I\u0026rsquo;ll start by creating a new .NET 6 project that will write and read from a SQLite database, I picked SQLite to keep things simple. The SQLite table structure consists of three columns, an identity column, the hashed column to store the hashed id, and the URL column to store the original value. Here is the markdown representation of that table.\nId Hash Url 100000 q0U https://www.yunier.dev/ 200000 Q1O https://en.wikipedia.org/ 300000 1g2I https://gohugo.io/ The first step in building my own hash function is to decide which characters can be used by the hashed function, essentially I need an alphabet. For a URL shortening service an alphabet composed of the numbers from 0 to 9, the letters a to z, and A to Z is generally considered good enough, it means that the alphabet will be composed of a total of 62 characters. One of the most important aspects of a good hashing function is that the function should properly handle collisions. By using Base62, collision will be impossible because the conversion is consistent, there will never be any duplicates.\nThe next step I need to take is to determine how many URLs can the application support while keeping in mind that as of 2017, SQLite has a limit of 140 terabytes. URLs tend to be pretty long, but for the sake of simplicity, I\u0026rsquo;m going to build the system under the assumption that on average an URL is roughly 200 characters long which translates to each URL consuming roughly 200 bytes. The next assumption that I will make is that the service will be around for 20 years and that it needs to support at least 1 billion URLs.\nSo that is 200 bytes times 20 years times our 1,000,000,000 minimum requirements gives 4,000,000,000,000 bytes or 4TB. 4TB is well below the 140 terabytes limit of SQLite and allows plenty of room in case the application needs to scale beyond these assumptions.\nIn the last step, the most important, I need to understand what is required to build my own Base62 function. Luckily, the internet is full of knowledgeable people like Matthias Kerstner and in his blog post Shortening Strings (URLs) using Base 62 Encoding he provides the algorithm to create a Base62 hashing function, you can find his algorithm below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 hashDigits = [] dividend = ID remainder = 0 while(dividend \u0026gt; 0) remainder = modulo(dividend, 62) dividend = divide(dividend, 62) hashDigits.prepend(remainder) endwhiled base62Alphabet = [a,b,c,...,A,B,C,...,0,1,2,...] hashDigitsCount = hashDigits.count() hashString = \u0026#34;\u0026#34; i = 0 while(hashDigitsCount \u0026gt; i) hashString += base62Alphabet[hashDigits[i]] i++ endwhile The algorithm can be translated into the following C# code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 var digits = new List\u0026lt;int\u0026gt;(); var dividend = value; while (dividend \u0026gt; 0) { int remainder = (int) (dividend % 62); dividend /= 62; digits.Insert(0, remainder); } const string alphabet = \u0026#34;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\u0026#34;; var count = digits.Count; var hashedValue = string.Empty; var i = 0; while (count \u0026gt; i) { hashedValue += alphabet[digits[i]]; i++; } Now that I have the hash function defined all that is left for me to do is to add it to a service class and inject that service into my controller. For example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [ApiController] [Route(\u0026#34;api\u0026#34;)] [Consumes(\u0026#34;application/json\u0026#34;)] [Produces(\u0026#34;application/json\u0026#34;)] public class UrlController : ControllerBase { private readonly IHashingService _hashingService; private readonly UrlshorterContext _urlshorterContext; public UrlController(IHashingService hashingService ,UrlshorterContext urlshorterContext) { _hashingService = hashingService; _urlshorterContext = urlshorterContext; } [HttpPost] [Route(\u0026#34;urls\u0026#34;)] [ProducesResponseType(typeof(UrlEntity), StatusCodes.Status201Created)] public IActionResult CreateUrlResource(string url) { var urlEntity = new UrlEntity { Url = url }; _urlshorterContext.Urls.Add(urlEntity); _urlshorterContext.SaveChanges(); urlEntity.Hash = _hashingService.ToBase62(urlEntity.Id); _urlshorterContext.SaveChanges(); return CreatedAtAction(nameof(GetUrlResource), new { hash = urlEntity.Hash}, urlEntity); } } I think I have everything that I need, time to test the API by sending the following command.\n1 2 3 4 curl -X \u0026#39;POST\u0026#39; \\ \u0026#39;https://localhost:7069/api/urls?url=https://www.kerstner.at/2012/07/shortening-strings-using-base-62-encoding\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; \\ -d \u0026#39;\u0026#39; Executing the command above yields the following JSON response.\n1 2 3 4 5 { \u0026#34;id\u0026#34;: 19, \u0026#34;hash\u0026#34;: \u0026#34;j\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.kerstner.at/2012/07/shortening-strings-using-base-62-encoding/\u0026#34; } with the following HTTP response headers.\n1 2 3 4 content-type: application/json; charset=utf-8 date: Sat,30 Apr 2022 00:13:00 GMT location: https://localhost:7069/api/urls/j server: Kestrel Perfect, the API is working as expected, it accepted a URL, it provided a hashed which can now be postpended to a URL to create something similar to https://tinyurl.com/yckpk37h. In an upcoming blog post, I\u0026rsquo;m going to go deeper into hashing, I\u0026rsquo;ll explore hashing techniques and different methods and how to create a hashing function with a low collision probability.\n","permalink":"http://localhost:1313/post/2022/shortening-urls/","summary":"\u003cp\u003eI was recently talking to another developer about the importance of never exposing internal identifiers to the outside world. A well-known example of this is using an auto-incrementing identity field in SQL and exposing that field through an API. A client can look at the highest number to tell how many records exist, in an ordering system this is far from ideal. Now everyone will know how many orders you have created. I recommend watching \u003ca href=\"https://youtu.be/FRsRoaubPiY?t=2363\"\u003eThe Internet of Pwned Things\u003c/a\u003e by \u003ca href=\"https://twitter.com/troyhunt\"\u003eTroy Hunt\u003c/a\u003e for a real-world example.\u003c/p\u003e","title":"Shortening URLs"},{"content":"Learned a cool little trick a while back from Khalid. As a developer, you will often run into scenarios that require you to get a subset of all fields from a model. There are many ways to achieve this task, returning the type and then grabbing each property, for example, take the following User type.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class User { public User(string name, DateTime dob) { var random = new Random(); Id = random.Next(); Name = name; DateOfBirth = dob; } public int Id { get; set; } public string Name {get; set; } public DateTime DateOfBirth { get; set; } } If you want to obtain the name and id property you can take the following approach.\n1 2 3 var user = new User(\u0026#34;James\u0026#34;, DateTime.Now); var userName = user.Name; var userDateOfBirth = user.DateOfBirth; Simple enough, I\u0026rsquo;m sure every developer at some point in their career has written code similar to the example above. The trick I learned makes this even simpler. It involves declaring deconstructing methods to extract information from types. The same example as above can now be written as.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class User { public User(string name, DateTime dob) { var random = new Random(); Id = random.Next(); Name = name; DateOfBirth = dob; } public int Id { get; set; } public string Name {get; set; } public DateTime DateOfBirth { get; set; } public void Deconstruct(out string name, out DateTime dob) { name = Name; dob = DateOfBirth; } } var user = new User(\u0026#34;James\u0026#34;, DateTime.Now); var (userName, userDateOfBirth) = user; You can declare multiple deconstructors per type, simply change the signature, so in the example above if I wanted to get the Id and date of birth instead of the name and the date of birth, I would need to add the following code to the User type.\n1 2 3 4 5 public void Deconstruct(out int id, out DateTime dob) { id = Id; dob = DateOfBirth; } If you prefer not to pollute your types with many deconstruct methods, you have the option to declare them in an extension class. For example, in the case of the User type, I can declare a new class UserExtensions.\n1 2 3 4 public static class UserExtension { public static void Deconstruct(this User user, out string name , out DateTime dob) =\u0026gt; (name, dob) = (user.Name, user.DateOfBirth); } and you use the same syntax to invoke it.\n1 2 var user = new User(\u0026#34;James\u0026#34;, DateTime.Now); var (userName, userDateOfBirth) = user; This is super useful when you need to extract various values and don\u0026rsquo;t want to use a Tuple or declare another class.\n","permalink":"http://localhost:1313/post/2022/extracting-values-from-types/","summary":"\u003cp\u003eLearned a cool \u003ca href=\"https://twitter.com/buhakmeh/status/1308089098306039814/photo/1\"\u003elittle trick\u003c/a\u003e a while back from \u003ca href=\"https://twitter.com/buhakmeh\"\u003eKhalid\u003c/a\u003e. As a developer, you will often run into scenarios that require you to get a subset of all fields from a model. There are many ways to achieve this task, returning the type and then grabbing each property, for example, take the following User type.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e15\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-C#\" data-lang=\"C#\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eUser\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e{\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e User(\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e name, DateTime dob)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e random = \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e Random();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Id = random.Next();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Name = name;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        DateOfBirth = dob;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e Id { \u003cspan style=\"color:#66d9ef\"\u003eget\u003c/span\u003e; \u003cspan style=\"color:#66d9ef\"\u003eset\u003c/span\u003e; }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e Name {\u003cspan style=\"color:#66d9ef\"\u003eget\u003c/span\u003e; \u003cspan style=\"color:#66d9ef\"\u003eset\u003c/span\u003e; }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e DateTime DateOfBirth { \u003cspan style=\"color:#66d9ef\"\u003eget\u003c/span\u003e; \u003cspan style=\"color:#66d9ef\"\u003eset\u003c/span\u003e; }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eIf you want to obtain the name and id property you can take the following approach.\u003c/p\u003e","title":"Extracting Values From Types"},{"content":"I\u0026rsquo;ve been thinking about what it takes to build a good Web API, regardless of the technology (REST vs GraphQL) or philosophy used. One concept that has been stuck on my head is the idea of marking up API documents to provide more context around the data.\nA Web API document is the response returned by the API itself, you will often see this term used in API specifications like GraphQL, HAL, JSON-LD, and JSON:API. Web API documents can be very simple, for example, imagine working with a Web API that manages users. This API may choose to represent the user resource using the following JSON.\n1 2 3 4 5 6 { \u0026#34;id\u0026#34;: 1234, \u0026#34;name\u0026#34;: \u0026#34;Emilio\u0026#34;, \u0026#34;age\u0026#34;: 23, \u0026#34;dateOfBirth\u0026#34;: \u0026#34;2020-03-09T22:18:26.625Z\u0026#34; } You may be building Web APIs like this example, if you are, know that technically there is nothing wrong with this approach, but I would advise you to shift away from building Web APIs like this. From my experience, the example above is often true of Web APIs that simply take a data model, the database representation of some business entity then dumped said entity onto the clients as the API response. Please understand that the database is not your API.\nWeb APIs are more than just your data, Web APIs should be composed between your data, API semantics, and the actions that can be performed on that data. Essentially, you should be enriching your API responses, you should be providing context within your Web API documents.\nProviding more than just data is a good way to ensure that the Web APIs you build is good, usable, and long-lasting. Don\u0026rsquo;t believe me? Take a good look at the World Wide Web. The Web is just a large collection of APIs linked together through hypermedia, these APIs retrieve data in the following format.\n1 2 3 4 5 6 7 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;I\u0026#39;m \u0026lt;em\u0026gt;so\u0026lt;/em\u0026gt; happy to meet you\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; What we have here is a simple HTML document. Now imagine a world where HTML did not exist, instead, the World Wide Web relied on just plain text, just the raw data. The HTML document above in an HTML-less world would simply look like this.\n1 I\u0026#39;m so happy to meet you Do you see the difference?\nIn the two examples above, the data is the same, but the context was lost in the second example. With the HTML, the data was enriched, the html provided a context that is then given to all consumers of this document, and that context was that the client should put an emphasis, see em tag, on the fact that I truly was happy to meet you.\nThis very basic example illustrates the power of HTML, the idea is that data should be marked up for ease of consumption both by humans and computers.\nSwitching back to our imaginary user Web API. How should it\u0026rsquo;s document be marked up?\nWell, how about pagination data? That is always good. How about including schema definitions? Hey, schemas are great, SQL has taught us that since the 1970s. Our user Web API document could now potentially be represented using the following JSON.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \u0026#34;schema\u0026#34;: \u0026#34;https://api.example.com/users/schema\u0026#34;, \u0026#34;meta\u0026#34;: { \u0026#34;page\u0026#34;: 1, \u0026#34;next\u0026#34;: null, \u0026#34;last\u0026#34;: null, \u0026#34;total\u0026#34;: 1 }, \u0026#34;data\u0026#34; : { \u0026#34;id\u0026#34;: 1234, \u0026#34;name\u0026#34;: \u0026#34;Emilio\u0026#34;, \u0026#34;age\u0026#34;: 23, \u0026#34;dateOfBirth\u0026#34;: \u0026#34;2020-03-09T22:18:26.625Z\u0026#34; }, \u0026#34;relationships\u0026#34; : { \u0026#34;permissions\u0026#34;: \u0026#34;https://api.example.com/users/1/permissions\u0026#34; } } This is a better approach to building Web APIs because the Web API document is no longer just data. The document is composed of data and it\u0026rsquo;s context which gives the client additional information. Additionally, your mind starts to shift away from data models to document models which will lead you down the path of building a flexible API document. This is where a spec like JSON:API shines because you never think about what data each API endpoints return, they are all just documents, the only difference is the data within each of those documents.\n","permalink":"http://localhost:1313/post/2022/markup-your-web-api-documents/","summary":"\u003cp\u003eI\u0026rsquo;ve been thinking about what it takes to build a good Web API, regardless of the technology (REST vs GraphQL) or philosophy used. One concept that has been stuck on my head is the idea of marking up API documents to provide more context around the data.\u003c/p\u003e\n\u003cp\u003eA Web API document is the response returned by the API itself, you will often see this term used in API specifications like \u003ca href=\"https://spec.graphql.org/October2021/#sec-Document\"\u003eGraphQL\u003c/a\u003e, \u003ca href=\"https://stateless.group/hal_specification.html\"\u003eHAL\u003c/a\u003e, \u003ca href=\"https://w3c.github.io/json-ld-syntax/#loading-documents\"\u003eJSON-LD\u003c/a\u003e, and \u003ca href=\"https://jsonapi.org/format/#document-structure\"\u003eJSON:API\u003c/a\u003e. Web API documents can be very simple, for example, imagine working with a Web API that manages users. This API may choose to represent the user resource using the following JSON.\u003c/p\u003e","title":"Markup Your Web API Documents"},{"content":".NET makes it super simple to update the dependencies of a project. If you are following a solution structure like Clean Architecture where the Web project should not be referenced by the Core project or you have created your own solution structure that requires certain projects do not reference another project then you might need a way to avoid having developers incorrectly adding dependencies.\nThe diagram above gives a high-level view of all project dependencies in a Clean Architecture solution. Built with Excalidraw.\nYou can prevent the issue described above if you have a strong continuous integration pipeline by having a unit test that asserts that a given project is not referenced by another project, in our example above, you may want to assert that the web project does not reference the core project. The Chinook JSON:API project that I have been working on follows the Clean Architecture project structure. To demonstrate, I will create a unit test that asserts that the core project in the Chinook solution never references the infrastructure project.\nBefore I start writing the Unit Test, I would like to share the csproj file for the Chinook.Core project. As you can see from the configuration below, there is no reference to any other assemblies, our unit test should pass when executed. Also as a reminder, when the solution was first made, a project named Chinook.Core.Test was created to host all unit tests having to do with the core project. This project already has XUnit and FluentAssertions installed. You will need to use FluentAssertions or a library that offers similar capabilities.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;Project Sdk=\u0026#34;Microsoft.NET.Sdk\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;TargetFramework\u0026gt;net6.0\u0026lt;/TargetFramework\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;PackageReference Include=\u0026#34;JsonApiFramework.Server\u0026#34; Version=\u0026#34;2.8.0\u0026#34; /\u0026gt; \u0026lt;PackageReference Include=\u0026#34;MediatR\u0026#34; Version=\u0026#34;9.0.0\u0026#34; /\u0026gt; \u0026lt;PackageReference Include=\u0026#34;Microsoft.AspNetCore.Mvc.NewtonsoftJson\u0026#34; Version=\u0026#34;6.0.2\u0026#34; /\u0026gt; \u0026lt;PackageReference Include=\u0026#34;Microsoft.EntityFrameworkCore.Design\u0026#34; Version=\u0026#34;6.0.2\u0026#34;\u0026gt; \u0026lt;IncludeAssets\u0026gt;runtime; build; native; contentfiles; analyzers; buildtransitive\u0026lt;/IncludeAssets\u0026gt; \u0026lt;PrivateAssets\u0026gt;all\u0026lt;/PrivateAssets\u0026gt; \u0026lt;/PackageReference\u0026gt; \u0026lt;PackageReference Include=\u0026#34;Microsoft.EntityFrameworkCore.Sqlite\u0026#34; Version=\u0026#34;6.0.2\u0026#34; /\u0026gt; \u0026lt;PackageReference Include=\u0026#34;Serilog\u0026#34; Version=\u0026#34;2.10.0\u0026#34; /\u0026gt; \u0026lt;PackageReference Include=\u0026#34;Serilog.AspNetCore\u0026#34; Version=\u0026#34;4.1.0\u0026#34; /\u0026gt; \u0026lt;PackageReference Include=\u0026#34;SimplePatch\u0026#34; Version=\u0026#34;3.0.1\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Project\u0026gt; The unit test itself is rather simple, I just need to grab the assembly of each project and use the NotReference function in FluentAssertions to assert that there is no reference between each assembly.\n1 2 3 4 5 6 7 8 9 10 11 public class ProjectDependencies { [Fact] public void CoreProject_Dependencies_ShouldNotContainAReferenceToInfrastructureProject() { var coreProjectAssembly = typeof(Chinook.Core.ChinookJsonApiDocumentContext).Assembly; var infrastructureProjectAssembly = typeof(Chinook.Infrastructure.Database.ChinookDbContext).Assembly; coreProjectAssembly.Should().NotReference(infrastructureProjectAssembly); } } As expected, the test passed when I executed dotnet test on the Chinook.Core.UnitTest.csproj file. With this test being executed on a continuous integration pipeline on every build I can be sure that nobody will accidentally adds the Infrastructure project as a dependency of the Core project.\nYou can use this technique in your own solutions to do the same. I\u0026rsquo;ve found this solution to be better than documenting the project dependencies on a README file or any other type of documentation because there is no guarantee that a developer will ever read or see that documentation.\n","permalink":"http://localhost:1313/post/2022/preventing-invalid-assembly-dependencies/","summary":"\u003cp\u003e.NET makes it super simple to update the dependencies of a project. If you are following a solution structure like \u003ca href=\"https://github.com/ardalis/CleanArchitecture#design-decisions-and-dependencies\"\u003eClean Architecture\u003c/a\u003e where the \u003ca href=\"https://github.com/ardalis/CleanArchitecture#the-web-project\"\u003eWeb project\u003c/a\u003e should not be referenced by the \u003ca href=\"https://github.com/ardalis/CleanArchitecture#the-core-project\"\u003eCore project\u003c/a\u003e or you have created your own solution structure that requires certain projects do not reference another project then you might need a way to avoid having developers incorrectly adding dependencies.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/post/2022/preventing-invalid-assembly-dependencies/clean-architecture-projet-dependencies.png\" alt=\"clean architecture project dependencies\"  /\u003e\r\n\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe diagram above gives a high-level view of all project dependencies in a Clean Architecture solution. Built with \u003ca href=\"https://excalidraw.com/\"\u003eExcalidraw\u003c/a\u003e.\u003c/p\u003e","title":"Preventing Invalid Assembly Dependencies"},{"content":"A few weeks ago I came across a blog post from Aaron Francis in which he talks about creating efficient pagination using deferred joins. A technique he remembered reading in High Performance MySQL: Proven Strategies for Operating at Scale.\nThe idea is that without deferred joins pagination queries can impact response time. Pagination is done using an OFFSET to skip over a number of records, however, even though the results are skipped, the database must still fetch those records. Meaning we are reading data from the disk and immediately discarding it. This is an inefficient process and is what causes pagination performance to degrade as you paginate over more records.\nI want to run a test, I have a query used by an application to paginate a database with roughly 700K records. The following SQL statement is generated by a client application to paginate over those records looks somewhat like like this.\n1 2 3 4 5 6 7 8 9 10 DECLARE @PageNumber AS INT DECLARE @RowsOfPage AS INT SET @PageNumber = 1000 SET @RowsOfPage = 100 SELECT * FROM Customers c ORDER BY Id OFFESET(@PageNumber-1) * @RowsOfPage ROWS FETCH NEXT @RowsOfPage ROWS ONLY Note that for simplicity, I have hard-coded the page number and page row directly in the queyr, normally these are parameters that come from the app. Now, according to the data provided by the Client Statistics tool found on SSMS. The client processing time for this query after 10 iterations on average was 2393.0000 ms. The total execution time after 10 iterations was on average 3287.0000 ms and the total time waiting on the server to reply was 894.0000 ms.\nThe same query modified to use deferred joins looks somewhat like this.\n1 2 3 4 5 6 7 8 9 10 11 DECLARE @PageNumber AS INT DECLARE @RowsOfPage AS INT SET @PageNumber = 1000 SET @RowsOfPage = 100 SELECT * FROM Customers c INNER JOIN( SELECT Id from Customers ORDER BY Id OFFESET(@PageNumber - 1) * @RowsOfPage ROWS FETCH NEXT @RowsOfPage ROWS ONLY ) as t ON c.Id = t.Id I ran the query above against the same database with roughly 700K records. I got some rather impressive results. The client processing time was 981.8000 ms after 10 iterations, down from the original 2393.0000 ms. The total execution time was 1402.8000 ms after 10 iterations, down from 3287.0000 ms and the wait time on server reply was 421.0000 ms after 10 iterations, down from 894.0000 ms.\nThose are some rather serious improvements. I\u0026rsquo;m thinking I will need to update the client app to use the updated query with deferred joins. It should improve the overall performance of the application, which is always a win in my book.\nI highly recommend reading Aaron\u0026rsquo;s original blog post and checking out the book High Performance MySQL: Proven Strategies for Operating at Scale to learn more about deferred joins.\n","permalink":"http://localhost:1313/post/2022/faster-web-api-pagination/","summary":"\u003cp\u003eA few weeks ago I came across a blog \u003ca href=\"https://aaronfrancis.com/2022/efficient-pagination-using-deferred-joins\"\u003epost\u003c/a\u003e from \u003ca href=\"https://aaronfrancis.com/\"\u003eAaron Francis\u003c/a\u003e in which he talks about creating efficient pagination using deferred joins. A technique he remembered reading in \u003ca href=\"https://www.amazon.com/High-Performance-MySQL-Strategies-Operating-dp-1492080519/dp/1492080519\"\u003eHigh Performance MySQL: Proven Strategies for Operating at Scale\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe idea is that without deferred joins pagination queries can impact response time. Pagination is done using an \u003ca href=\"https://www.geeksforgeeks.org/sql-offset-fetch-clause/\"\u003eOFFSET\u003c/a\u003e to skip over a number of records, however, even though the results are skipped, the database must still fetch those records. Meaning we are reading data from the disk and immediately discarding it. This is an inefficient process and is what causes pagination performance to degrade as you paginate over more records.\u003c/p\u003e","title":"Faster Web API Pagination"},{"content":"It has been a while since I blogged about JSON:API. In my last post on JSON:API I covered how to create new resources. In today\u0026rsquo;s post, I want to go over how I expose pagination links. Pagination links allow a client to page through a collection of resources. A shift of control from the client back to the server.\nHere is an example of a possible JSON:API response that includes pagination links.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;data\u0026#34;: [ { // omitted for brevity } ], \u0026#34;links\u0026#34;: { \u0026#34;up\u0026#34;: \u0026#34;http://example.com/\u0026#34;, \u0026#34;self\u0026#34;: \u0026#34;http://example.com/articles?page[number]=3\u0026amp;page[size]=1\u0026#34;, \u0026#34;first\u0026#34;: \u0026#34;http://example.com/articles?page[number]=1\u0026amp;page[size]=1\u0026#34;, \u0026#34;prev\u0026#34;: \u0026#34;http://example.com/articles?page[number]=2\u0026amp;page[size]=1\u0026#34;, \u0026#34;next\u0026#34;: \u0026#34;http://example.com/articles?page[number]=4\u0026amp;page[size]=1\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;http://example.com/articles?page[number]=13\u0026amp;page[size]=1\u0026#34; } } As you can see, along with the data object, the API responses included a Links object, within the Links object, you can find links for up, self, first, prev, next, and last. These are all relationship name as defined in Link Relations by the Internet Assigned Numbers Authority (IANA).\nThe \u0026ldquo;up\u0026rdquo; link refers to a parent document in a hierarchy of documents. The \u0026ldquo;self\u0026rdquo; link is an identifier for the current document. The \u0026ldquo;first\u0026rdquo; link refers to the furthest preceding resource in a series of resources. The \u0026ldquo;prev\u0026rdquo; link indicates that the link\u0026rsquo;s context is a part of a series and that the previous document in the series is the link target. The \u0026ldquo;next\u0026rdquo; link indicates that the link\u0026rsquo;s context is part of a series and the next document in the series is the link\u0026rsquo;s target. The \u0026ldquo;last\u0026rdquo; link refers to the furthest following resource in a series of resources.\nThe absence or presence of the pagination link is significant, if the \u0026ldquo;next\u0026rdquo; link exists, then there are more pages for the client to paginate through. If the \u0026ldquo;next\u0026rdquo; link does not exist, then the client has reached the last page. If the \u0026ldquo;prev\u0026rdquo; link exists, then the client is not on the first page. If neither a \u0026ldquo;next\u0026rdquo; or \u0026ldquo;prev\u0026rdquo; link exists, there is only one page.\nI want to update the Chinook project by exposing pagination links on the customers resource. For that I will need to add a code to support reading and writing Links, calculating the total number of pages to determine if there is more than one page.\nI\u0026rsquo;ll start by adding the following PagedList class, this class will help me determine how many pages are available and if a previous and next page exists.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 public class PagedList { public PagedList(int recordCount, int pageNumber, int pageSize) { PageSize = pageSize; PageNumber = pageNumber; RecordCount = recordCount; } private int PageSize { get; } private int PageNumber { get; } private int RecordCount { get; } private int NumberOfPages =\u0026gt; (int)Math.Ceiling(RecordCount / (double)PageSize); private bool HasAPreviousPage =\u0026gt; PageNumber \u0026gt; 1; private bool HasANextPage =\u0026gt; PageNumber \u0026lt; NumberOfPages; public bool HasNextPage() { return HasANextPage; } public bool HasPreviousPage() { return HasAPreviousPage; } public int GetPageSize() { return PageSize; } public int GetNextPageNumber() { return PageNumber + 1; } public int GetPreviousPageNumber() { return PageNumber - 1; } public int GetFirstPageNumber() { return 1; } public int GetLastPageNumber() { return NumberOfPages; } } Next, I will add code to handle creating the pagination links, it will also need to enforce pagination rules. For example, I prefer setting a limit on the number of records that can be retrieved per page. This is to prevent a single client from crashing the entire API. In my experience, I have found 100 to be the sweet spot.\nWe can drive the rules through configurations defined on our AppSettings.json file.\n1 2 3 4 5 6 7 { \u0026#34;PageConfiguration\u0026#34;: { \u0026#34;DefaultNumber\u0026#34; : 1, \u0026#34;DefaultSize\u0026#34; : 10, \u0026#34;DefaultMax\u0026#34; : 100 } } To load these settings, I will create a PageConfigurationSettings class, see below, this class will then be registered using the default DI framework from .NET so that these settings can be consumed by any class in the API.\n1 2 3 4 5 6 7 8 9 10 11 public class PageConfigurationSettings { public PageConfiguration PageConfiguration { get; set; } } public class PageConfiguration { public int DefaultNumber { get; set; } public int DefaultSize { get; set; } public int DefaultSize { get; set; } } To properly manage the creation of pagination links I created three classes, one to read the query parameters from the incoming request and one to update those parameters and the last class will handle creating the Links by relying on the classes I just mentioned.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 public class PaginationLinkWriter { private readonly PagedList _pagedList; private readonly UriQueryParametersWriter _uriQueryParametersWriter; public PaginationLinkWriter(UriQueryParametersReader uriQueryParametersReader, UriQueryParametersWriter uriQueryParametersWriter, int totalCount) { var pageNumber = uriQueryParametersReader.GetPageNumberFromRequestUri(); var pageSize = uriQueryParametersReader.GetSizeNumberFromRequestUri(); _pagedList = new PagedList(totalCount, pageNumber, pageSize); _uriQueryParametersWriter = uriQueryParametersWriter; } public Link GetNextPageLink() { if (_pagedList.HasNextPage()) { var nextPageNumber = _pagedList.GetNextPageNumber(); var nextPageSize = _pagedList.GetPageSize(); var updatedParameters = GetPageQueryParameters(nextPageNumber, nextPageSize); var nextPageUri = _uriQueryParametersWriter.ReplaceQueryParameters(updatedParameters); var nextPageLink = LinkBuilder.CreateResourceLink(nextPageUri); return nextPageLink; } else { return Link.Empty; } } public Link GetPreviousPageLink() { if (_pagedList.HasPreviousPage()) { var nextPageNumber = _pagedList.GetPreviousPageNumber(); var nextPageSize = _pagedList.GetPageSize(); var updatedParameters = GetPageQueryParameters(nextPageNumber, nextPageSize); var nextPageUri = _uriQueryParametersWriter.ReplaceQueryParameters(updatedParameters); var nextPageLink = LinkBuilder.CreateResourceLink(nextPageUri); return nextPageLink; } else { return Link.Empty; } } public Link GetLastPageLink() { var nextPageNumber = _pagedList.GetLastPageNumber(); var nextPageSize = _pagedList.GetPageSize(); var updatedParameters = GetPageQueryParameters(nextPageNumber, nextPageSize); var nextPageUri = _uriQueryParametersWriter.ReplaceQueryParameters(updatedParameters); var nextPageLink = LinkBuilder.CreateResourceLink(nextPageUri); return nextPageLink; } public Link GetFirstPageLink() { var nextPageNumber = _pagedList.GetFirstPageNumber(); var nextPageSize = _pagedList.GetPageSize(); var updatedParameters = GetPageQueryParameters(nextPageNumber, nextPageSize); var nextPageUri = _uriQueryParametersWriter.ReplaceQueryParameters(updatedParameters); var nextPageLink = LinkBuilder.CreateResourceLink(nextPageUri); return nextPageLink; } private IDictionary\u0026lt;string, string\u0026gt; GetPageQueryParameters(int pageNumber, int pageSize) { var updatedParameters = new Dictionary\u0026lt;string, string\u0026gt; { {UriKeyWords.PageNumber, pageNumber.ToString()}, {UriKeyWords.PageSize, pageSize.ToString()} }; return updatedParameters; } } public class UriQueryParametersReader { private Uri CurrentRequestUri; private QueryParameters CurrentRequestUriQueryParameters; private PageConfigurationSettings PageConfigurationSettings; public UriQueryParametersReader(Uri requestUri, PageConfigurationSettings pageConfigurationSettings) { CurrentRequestUri = requestUri; CurrentRequestUriQueryParameters = QueryParameters.Create(requestUri); PageConfigurationSettings = pageConfigurationSettings; } public int GetPageNumberFromRequestUri() { var pageParameters = CurrentRequestUriQueryParameters.Page; var hasPageParameterInRequestUri = pageParameters.TryGetValue(UriKeyWords.number, out var pageParamertersInUri); if(hasPageParameterInRequestUri) { var pageNumber = pageParamertersInUri.First(); return int.Parse(pageNumber); } else { return PageConfigurationSettings.PageConfiguration.DefaultNumber; } } public int GetSizeNumberFromRequestUri() { var pageParameters = CurrentRequestUriQueryParameters.Page; var hasPageParameterInRequestUri = pageParameters.TryGetValue(UriKeyWords.size, out var pageParamertersInUri); if(hasPageParameterInRequestUri) { var pageSize = pageParamertersInUri.First(); return int.Parse(pageSize); } else { return PageConfigurationSettings.PageConfiguration.DefaultSize; } } } public class UriQueryParametersWriter { private Uri RequestUri; public UriQueryParametersWriter(Uri requestUri) { RequestUri = requestUri; } public Uri ReplaceQueryParameters(IDictionary\u0026lt;string, string\u0026gt; parameters) { var parsedQueryParameters = QueryHelpers.ParseQuery(RequestUri.Query); foreach (var parameterToReplace in parameters) { parsedQueryParameters.Remove(parameterToReplace.Key); parsedQueryParameters.Add(parameterToReplace.Key, parameterToReplace.Value); } return RequestUri.AddQueryStringsToUri(parsedQueryParameters); } public Uri StripParametersFromUri(IEnumerable\u0026lt;string\u0026gt; parameters) { var parsedQueryParameters = QueryHelpers.ParseQuery(RequestUri.Query); foreach (var parameterToRemove in parameters) { parsedQueryParameters.Remove(parameterToRemove); } return RequestUri.AddQueryStringsToUri(parsedQueryParameters); } public Uri StripParametersFromUri(string parameter) { var parsedQueryParameters = QueryHelpers.ParseQuery(RequestUri.Query); parsedQueryParameters.Remove(parameter); return RequestUri.AddQueryStringsToUri(parsedQueryParameters); } } I also created the following helper classes to make everything easier.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public static class UriKeyWords { public static string PageNumber = $\u0026#34;page[{number}]\u0026#34;; public static string PageSize = $\u0026#34;page[{size}]\u0026#34;; public const string size = nameof(size); public const string number = nameof(number); } public static class LinksKeyWords { public const string next = nameof(next); public const string prev = nameof(prev); public const string last = nameof(last); public const string first = nameof(first); public const string describedBy = nameof(describedBy); } public class CustomerQuerySpecification : IEntityQuerySpecification\u0026lt;Customer\u0026gt; { public Expression\u0026lt;Func\u0026lt;Customer, bool\u0026gt;\u0026gt; FilterExpression { get; } = entity =\u0026gt; true; // result in no SQL generated public int Take { get; } public int Skip { get; } public CustomerQuerySpecification(UriQueryParametersReader uriQueryParametersReader) { var pageNumber = uriQueryParametersReader.GetPageNumberFromRequestUri(); var pageSize = uriQueryParametersReader.GetSizeNumberFromRequestUri(); Take = pageSize; Skip = (pageNumber - 1) * pageSize; } } I have everything I need to expose pagination links on the customer resource. Time to update the customer resource class by updating the GetCustomerResourceCollection method in the CustomerResource class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 public class CustomerResource : ICustomerResource { private readonly ILogger\u0026lt;CustomerResource\u0026gt; _logger; private readonly IMediator _mediator; private readonly UriQueryParametersReader _uriQueryParametersReader; private readonly UriQueryParametersWriter _uriQueryParametersWriter; private readonly Uri _currentRequestUri public CustomerResource( ILogger\u0026lt;CustomerResource\u0026gt; logger, IMediator mediator, IHttpContextAccessor httpContextAccessor, UriQueryParametersReader uriQueryParametersReader, UriQueryParametersWriter uriQueryParametersWritery) { _logger = logger; _mediator = mediator; _uriQueryParametersReader = uriQueryParametersReader; _uriQueryParametersWriter = uriQueryParametersWritery; _currentRequestUri = httpContextAccessor.HttpContext.GetCurrentRequestUri(); ; public async Task\u0026lt;Document\u0026gt; GetCustomerResourceCollection() { var customerResourceResult = await _mediator.Send(new GetCustomerResourceCollectionCommand()) // Build hypermedia links var linktToCustomerJsonSchema = SchemaLinksBuilder.BuildLinkToCustomerSchema(_currentRequestUri); var linkBuilder = new PaginationLinkWriter(_uriQueryParametersReader, _uriQueryParametersWriter, customerResourceResult.Count) using var chinookDocumentContext = new ChinookJsonApiDocumentContext(_currentRequestUri); var document = chinookDocumentContext .NewDocument(_currentRequestUri) .SetJsonApiVersion(JsonApiVersion.Version10) .Links() .AddSelfLink() .AddUpLink() .AddLink(LinksKeyWords.next, linkBuilder.GetNextPageLink()) .AddLink(LinksKeyWords.last, linkBuilder.GetLastPageLink()) .AddLink(LinksKeyWords.first, linkBuilder.GetFirstPageLink()) .AddLink(LinksKeyWords.prev, linkBuilder.GetPreviousPageLink()) .AddLink(LinksKeyWords.describedBy, linktToCustomerJsonSchema) .LinksEnd() .ResourceCollection(customerResourceResult.Value) .Relationships() .AddRelationship(InvoiceResourceKeyWords.ToManyRelationShipKey, new[] { Keywords.Related }) .RelationshipsEnd() .Links() .AddSelfLink() .LinksEnd() .ResourceCollectionEnd() .WriteDocument() _logger.LogInformation(\u0026#34;Request for {URL} generated JSON:API document {doc}\u0026#34;, _currentRequestUri, document); return document; } } Using the fluent style API exposed by JsonAPIFramework, I can define additional links using the AddLink() method. Now, updating the CustomerResource class is not enough. I need to make use of the pagination parameters. Mainly passing the parameters down all the way down to the data layer so that Entity Framework can build the correct SQL query. I update the class GetCustomerResourceCollectionHandler to accept the CustomerQuerySpecification class. I also used CountAsync to get the total number of records based on the current request. This is a really important step, if the count is not correct, the pagination links will not be built correctly. The count also needs to take into account possible filtering. Since our project does not support filtering at the moment, the filter property in CustomerQuerySpecification is simply set to true. Here is another nifty thing to note, EF will accept the following code and not generate any additional SQL statements.\n1 public Expression\u0026lt;Func\u0026lt;Customer, bool\u0026gt;\u0026gt; FilterExpression { get; } = entity =\u0026gt; true; The fact that EF doesn\u0026rsquo;t generate any additional SQL statements allows is great. I often used this to my advantage whenever I create custom filter expressions using extension methods.\nAnyways, back to the code, here is the updated GetCustomerResourceCollectionHandler.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class GetCustomerResourceCollectionHandler : IRequestHandler\u0026lt;GetCustomerResourceCollectionCommand, EntityCollectionResult\u0026lt;Customer\u0026gt;\u0026gt; { private readonly ChinookDbContext _chinookDbContext; private readonly CustomerQuerySpecification _customerQuerySpecification public GetCustomerResourceCollectionHandler(ChinookDbContext chinookDbContext, CustomerQuerySpecification customerQuerySpecification) { _chinookDbContext = chinookDbContext; _customerQuerySpecification = customerQuerySpecification; public async Task\u0026lt;EntityCollectionResult\u0026lt;Customer\u0026gt;\u0026gt; Handle(GetCustomerResourceCollectionCommand request, CancellationToken cancellationToken) { var count = await _chinookDbContext.Customers.CountAsync(_customerQuerySpecification.FilterExpression); var value = await _chinookDbContext.Customers .TagWithSource() .Skip(_customerQuerySpecification.Skip) .Take(_customerQuerySpecification.Take) .ToListAsync(cancellationToken) return new EntityCollectionResult\u0026lt;Customer\u0026gt;(count, value); } } Let\u0026rsquo;s test our change. As of today, February 13, 2022, there are only 59 customers on the Chinook SQL lite database. Therefore, when a client navigates to the customers resource without specifying any paging parameters, the total number of pages should be 6, since we have 59 customers, the client will be on the first page and the default page size is 10. I\u0026rsquo;m going to act as the client and navigate to the customer resource.\nI\u0026rsquo;ll send the following HTTP request.\n1 GET /customers HTTP/1.1 The request yields the following response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:5001/customers\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:5001\u0026#34;, \u0026#34;next\u0026#34;: \u0026#34;https://localhost:5001/customers?page%5Bnumber%5D=2\u0026amp;page%5Bsize%5D=10\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;https://localhost:5001/customers?page%5Bnumber%5D=6\u0026amp;page%5Bsize%5D=10\u0026#34;, \u0026#34;first\u0026#34;: \u0026#34;https://localhost:5001/customers?page%5Bnumber%5D=1\u0026amp;page%5Bsize%5D=10\u0026#34;, \u0026#34;prev\u0026#34;: { \u0026#34;href\u0026#34;: null }, \u0026#34;describedBy\u0026#34;: \u0026#34;https://localhost:5001/customers/schemas\u0026#34; }, \u0026#34;data\u0026#34;: [ // omitted for brevity ] } Note that the links are URL encoded as they should be, but we can see from the response that the prev link is null, which indicates we are on the first page. We see that the last link decoded is customers?page[number]=6\u0026amp;page[size]=10. This is correct, the last page is 6 because there are only six pages. As I client, I now want to navigate to the last page.\nI\u0026rsquo;ll send the following HTTP request.\n1 GET /customers?page[number]=6\u0026amp;page[size]=10 HTTP/1.1 The request yields the following API response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:5001/customers?page%5Bnumber%5D=6\u0026amp;page%5Bsize%5D=10\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:5001\u0026#34;, \u0026#34;next\u0026#34;: { \u0026#34;href\u0026#34;: null }, \u0026#34;last\u0026#34;: \u0026#34;https://localhost:5001/customers?page%5Bnumber%5D=6\u0026amp;page%5Bsize%5D=10\u0026#34;, \u0026#34;first\u0026#34;: \u0026#34;https://localhost:5001/customers?page%5Bnumber%5D=1\u0026amp;page%5Bsize%5D=10\u0026#34;, \u0026#34;prev\u0026#34;: \u0026#34;https://localhost:5001/customers?page%5Bnumber%5D=5\u0026amp;page%5Bsize%5D=10\u0026#34;, \u0026#34;describedBy\u0026#34;: \u0026#34;https://localhost:5001/customers/schemas\u0026#34; }, \u0026#34;data\u0026#34;: [ // omitted for brevity ] } We can see from the response that the next link is now null, which is correct, indicating that we are on the last page. Good so far, now let\u0026rsquo;s change the request. As a client I want to get 100 records, not 10 upon my first HTTP request, meaning page one. Such a request should result in a response with a prev and next link as null.\nSending the following HTTP request.\n1 GET /customers?page[number]=6\u0026amp;page[size]=100 HTTP/1.1 The request yields the following API response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:5001/customers?page%5Bnumber%5D=1\u0026amp;page%5Bsize%5D=100\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:5001\u0026#34;, \u0026#34;next\u0026#34;: { \u0026#34;href\u0026#34;: null }, \u0026#34;last\u0026#34;: \u0026#34;https://localhost:5001/customers?page%5Bnumber%5D=1\u0026amp;page%5Bsize%5D=100\u0026#34;, \u0026#34;first\u0026#34;: \u0026#34;https://localhost:5001/customers?page%5Bnumber%5D=1\u0026amp;page%5Bsize%5D=100\u0026#34;, \u0026#34;prev\u0026#34;: { \u0026#34;href\u0026#34;: null }, \u0026#34;describedBy\u0026#34;: \u0026#34;https://localhost:5001/customers/schemas\u0026#34; }, \u0026#34;data\u0026#34;: [ // omitted for brevity ] } Our API response above looks correct. The customer resource now supports pagination links. All that is left to do is apply the same code change to all the resource collections. I will do that offline and update the API at a later time. As always you can verify this API changes yourself by visiting the Chinook API directly, I recommend having some type of JSON viewer enable like JSON Viewer.\nTill next time. Cheerio.\n","permalink":"http://localhost:1313/post/2022/json-api-pagination-links/","summary":"\u003cp\u003eIt has been a while since I blogged about \u003ca href=\"https://jsonapi.org/\"\u003eJSON:API\u003c/a\u003e. In my last post on JSON:API I covered how to create \u003ca href=\"/post/2021/json-api-creating-new-resources/\"\u003enew resources\u003c/a\u003e. In today\u0026rsquo;s post, I want to go over how I expose pagination links. \u003ca href=\"https://jsonapi.org/examples/#pagination\"\u003ePagination links\u003c/a\u003e allow a client to page through a collection of resources. A shift of control from the client back to the server.\u003c/p\u003e\n\u003cp\u003eHere is an example of a possible JSON:API response that includes pagination links.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e15\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e{\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003e\u0026#34;data\u0026#34;\u003c/span\u003e: [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#75715e\"\u003e// omitted for brevity\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  ],\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003e\u0026#34;links\u0026#34;\u003c/span\u003e: {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003e\u0026#34;up\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;http://example.com/\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003e\u0026#34;self\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;http://example.com/articles?page[number]=3\u0026amp;page[size]=1\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003e\u0026#34;first\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;http://example.com/articles?page[number]=1\u0026amp;page[size]=1\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003e\u0026#34;prev\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;http://example.com/articles?page[number]=2\u0026amp;page[size]=1\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003e\u0026#34;next\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;http://example.com/articles?page[number]=4\u0026amp;page[size]=1\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003e\u0026#34;last\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;http://example.com/articles?page[number]=13\u0026amp;page[size]=1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eAs you can see, along with the \u003ca href=\"https://jsonapi.org/format/#document-top-level\"\u003edata\u003c/a\u003e object, the API responses included a \u003ca href=\"https://jsonapi.org/format/#document-links\"\u003eLinks\u003c/a\u003e object, within the Links object, you can find links for up, self, first, prev, next, and last. These are all relationship name as defined in \u003ca href=\"https://www.iana.org/assignments/link-relations/link-relations.xhtml\"\u003eLink Relations\u003c/a\u003e by the Internet Assigned Numbers Authority (IANA).\u003c/p\u003e","title":"JSON:API - Pagination Links"},{"content":"Over the last few years, our industry has moved away from monolith architectures to microservice architectures. For a good number of reasons, this trend continues to remain strong. One issue you may encounter with creating microservices if you are not using a monorepo is that you may have to build a new project whenever you need to add a new microservice to your app ecosystem. Creating a new project may include creating new repositories in Bitbucket/Github, configuring a continuous integration pipeline, adding build/deploy scripts, building the project based on some folder structure that has been determined by the team. It can also involve configuring how the service captures logs, communicating with other services, exposing an Open API definition, configuring a docker file, having a helm chart to deploy to k8s, installing external packages, and so on.\nIn other words, building a new project is hard enough, having to rebuild it over and over again as you add new microservices can be a painful experience that takes away time that can be better spent on something more useful. A solution to this problem is to create a microservice generator.\nA microservice generator is a tool or combination of tools that create a microservice. They can accept configurations via parameter or can be preconfigured with some setting i.e arm vs x86. The goal of a microservice generator is to minimize the pain and time it takes to create a new project and at a minimum should be able to run on your local development environment with no additional work. Now, saying microservice generator may sound fancy, in reality, you could already have a microservice generator, the generator can be as simple as a bash or PowerShell script that creates a new code project and add all the required dependency needed to run the code.\nIf you\u0026rsquo;ve used Go or a service built with Go, like Helm, then you may already be familiar with the concepts of templates engines. If not, then all you really need to know is that template engines are used to interpolate strings effectively. In the dotnet world, we have the dotnet template engine. In fact, you may have already been enjoying its benefits without noticing it. Have you ever created a new dotnet console from the command line using the following syntax?\n1 dotnet new console In the command above, the console is one of the default templates that come with the .NET SDK. In dotnet you can create and publish your own templates, which is what I have used to create a microservice generator. Using the dotnet template engine as a microservice generator has in my opinion been super useful. The dotnet template engine has allowed me to create a microservice generator that can generate up a new dotnet project configured with a set number NuGet packages that I have determined to be essential i.e. Serilog, EF Core, XUnit. The template engine has also helped me ensure that logging is configured and ready for use, a developer doesn\u0026rsquo;t need to understand how the logs are extracted and propagated across our monitoring system, instead they just need to write what they would like to log. The template engine can be used to ensure a certain folder structure is followed. I\u0026rsquo;m a fan of structure proposed by Ardalis in Clean Architecture. The template engine can also be used to create and or modify a Docker file to ensure you can create a Docker image of your project as soon as you create the microservice.\nSee the greatest feature of the dotnet template engine in my opinion is that it works on any text file. This means you can use the engine to configure script files, Docker files, helm charts, configuration as code tools that are file-based, continuous integration tools like bitbucket pipeline, or any other continuous integration tool that are configured via a file, vscode configuration files like task.json, and so on. This reduces the amount of time a team has to spend on creating new projects. Imagine being able to create a new code project in less than five seconds that can be used and be deployed immediately, that has Unit Test projects with all the dependent libraries used for mocking and testing. That has been configured to use Docker and Helm. A project that has a continuous integration pipeline ready to be used. All from a single command execution. That is the power obtained by using the dotnet template engine. Hard to beat those benefits.\nIn a future blog post, I will demonstrate how to create, publish and consume your own microservice generator using the dotnet template engine.\n","permalink":"http://localhost:1313/post/2022/microservice-generator/","summary":"\u003cp\u003eOver the last few years, our industry has moved away from monolith architectures to microservice architectures. For a good number of \u003ca href=\"https://stackify.com/6-key-benefits-of-microservices-architecture/\"\u003ereasons\u003c/a\u003e, this trend continues to remain strong. One issue you may encounter with creating microservices if you are not using a \u003ca href=\"https://www.youtube.com/watch?v=9iU_IE6vnJ8\"\u003emonorepo\u003c/a\u003e is that you may have to build a new project whenever you need to add a new microservice to your app ecosystem. Creating a new project may include creating new repositories in Bitbucket/Github, configuring a continuous integration pipeline, adding build/deploy scripts, building the project based on \u003ca href=\"https://github.com/ardalis/CleanArchitecture#design-decisions-and-dependencies\"\u003esome folder structure\u003c/a\u003e that has been determined by the team. It can also involve configuring how the service captures logs, communicating with other services, exposing an Open API definition, configuring a docker file, having a helm chart to deploy to k8s, installing external packages, and so on.\u003c/p\u003e","title":"Microservice Generator"},{"content":"Development tools are an essential part of our job, they make us work smarter not harder, they simplify processes and make us more productive. In this post, I want to share some tools that I have found over the years that have made my job easier. If you have a similar experience with a tool that is not listed here, then I would love you hear from you.\nGrepapp The first tool I want to talk about is the grepapp, this tool is becoming one of my favorite tools. This app allows you to quickly search through repositories hosted in Github. This is great if you want to see how other developers have implemented an algorithm, interface, or what I\u0026rsquo;ve found to be most useful, understanding how a not well-documented feature of a library gets used.\nFork The second tool, Fork, has been my most used tool for the last four years. Fork is a git client, the best in my opinion, it makes repository management super easy, and its interactive rebase is pretty damn good, and it is super fast.\nIf you are not into visual git client and are still rocking the terminal, then I suggest checking out gitexplorer. Git explorer helps you figure out which command line arguments are needed to execute whatever task you need on git.\nReadME.so I\u0026rsquo;ve already written about how writing a good ReadME is a skill. The website readme.so can help you with that skill, it can scaffold a pretty awesome ReadME file for you to utilize on your projects.\nBundlePhobia BundlePhobia is a great tool for front-end developers, bundlephobia can analyze the cost of adding an npm package to your stack. For example, if you wanted to add react to your project then bundlephobia would generate the following report.\nAs you can see from the report, unminified, downloading react over the network would cost 6.9 kb while if it was gzipped, the cost would be 2.8 kb. Bundlephobia can also scan a package.json, this will help you understand what it will it cost to serve all the packages you have in your project.json file.\nGitignore If you are starting out with a brand new project, you will eventually need to configure git to ignore certain files and paths. Gitignore.io makes this super simple, just type what language, framework, or ide you are using and it will generate a git ignore file based on the most commonly ignore items for your project.\nExcalidraw Need to quickly draw out an architecture? Look no further than Excalidraw, this tool allows you to collaborate with other users. You can quickly make architecture, flow, network, use case, basically any diagram you want, you can also export them, save them or share them with someone. Exalidraw is great because it offers the same capabilities as any other diagram tool out there without having to sign up, the whole process is simple, open the website and start working.\nCronGuru Working with a cron expression and you are not familiar with the syntax? Crontab can help, simply type the cron expression and crontab will translate it to plain old English. They also a good list of tips and examples that can help you understand cron expressions.\nDockerSlim Need to minimize the size of your docker images? DockerSlim does just that, in some case, the docker container image doesn\u0026rsquo;t even have to change. Learn more about this tool over at their official website.\n","permalink":"http://localhost:1313/post/2021/tools-for-the-modern-developer/","summary":"\u003cp\u003eDevelopment tools are an essential part of our job, they make us work smarter not harder, they simplify processes and make us more productive. In this post, I want to share some tools that I have found over the years that have made my job easier. If you have a similar experience with a tool that is not listed here, then I would love you hear from you.\u003c/p\u003e\n\u003ch4 id=\"grepapp\"\u003eGrepapp\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://grep.app/\"\u003e\u003cimg loading=\"lazy\" src=\"/post/2021/tools-for-the-modern-developer/grepapp.png\" alt=\"GrepApp\"  /\u003e\r\n\u003c/a\u003e\u003c/p\u003e","title":"Tools For The Modern-Day Developer"},{"content":"Idempotency, is one of the key features any Web API should have. The idea is that software is unrealiable, the network can fail, the database the API connects to could be offline, the API itself could be performing an intense operation that impacts performance. For all these reasons an API client may resubmit a request, not much of a problem if you are dealing with GET, HEAD, PUT or DELETE, these HTTP methods are idempotent, POST and PATCH on the other hand are not.\nAn HTTP POST to an orders API will create a new order every time the API gets called. This behavior is not desired, after all, your customers are not going to be happy to see that they have been charged for multiple orders. For this reason, it is essential that when you create a Web API, an effort should be made into making all POST and PATCH requests idempotent.\nTwo techniques have surfaced over the years on how to make POST and PATCH idempotent. The first technique involves having the API provide the client with a one-time URI. This one-time URI will come with an embedded token. The idea here is that the API can track each token in order to determine if the request is being submitted for the first time or if the request has already been processed or is being processed.\nA more popular technique, evangelised by Stipe, comes in the form of using an Idempotancy-Key header. In this approach, the key is generated using V4 UUIDs, to guarantee enough randomness, the client is then responsible for including this header on all POST and PATCH requests, and like in the first technique, the server is responsible for determining if the request is being submitted for the first time or if the request has already been processed.\n","permalink":"http://localhost:1313/post/2021/idempotency-in-a-web-api/","summary":"\u003cp\u003eIdempotency, is one of the key features any Web API should have. The idea is that \u003ca href=\"https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing\"\u003esoftware is unrealiable\u003c/a\u003e, the network can fail, the database the API connects to could be offline, the API itself could be performing an intense operation that impacts performance. For all these reasons an API client may resubmit a request, not much of a problem if you are dealing with \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/GET\"\u003eGET\u003c/a\u003e, \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/HEAD\"\u003eHEAD\u003c/a\u003e, \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/PUT\"\u003ePUT\u003c/a\u003e or \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/DELETE\"\u003eDELETE\u003c/a\u003e, these HTTP methods are idempotent, \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/post\"\u003ePOST\u003c/a\u003e and \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/PATCH\"\u003ePATCH\u003c/a\u003e on the other hand are not.\u003c/p\u003e","title":"Idempotency In A Web API"},{"content":"When building a Web API, RESTful or GraphQL, you may want to expose some functionality that allows a client application to sort data.\nFrom my experience, this is often not implemented correctly. Many developers fail to realize that sorting should always be sort plus one. The plus one is a unique value, like a primary key or identifier. The reason for this is that sorting in most databases, like SQL Server, is nondeterministic, meaning the sort function may return different results each time they are called with a specific set of input values even if the database state that they access remains the same.\nIt is important to not make this mistake in a Web API, especially in a RESTful system given that RESTful APIs rely upon HTTP. In HTTP, the GET, HEAD, PUT, and DELETE methods are idempotent, the same request should always return the same value.\nHaving an API that uses idempotency correctly becomes super useful when you have API caching. Idempontacy will give you greater cache hits. For example, take the following data set based on one of my favorite TV shows, WestWorld.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [ { \u0026#34;id\u0026#34;: 869671, \u0026#34;url\u0026#34;: \u0026#34;https://www.tvmaze.com/episodes/869671/westworld-1x01-the-original\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;The Original\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;regular\u0026#34;, \u0026#34;runtime\u0026#34;: 60, }, { \u0026#34;id\u0026#34;: 911201, \u0026#34;url\u0026#34;: \u0026#34;https://www.tvmaze.com/episodes/911201/westworld-1x02-chestnut\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Chestnut\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;regular\u0026#34;, \u0026#34;runtime\u0026#34;: 60 }, { \u0026#34;id\u0026#34;: 911204, \u0026#34;url\u0026#34;: \u0026#34;https://www.tvmaze.com/episodes/911204/westworld-1x03-the-stray\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;The Stray\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;regular\u0026#34;, \u0026#34;runtime\u0026#34;: 60 } ] This data is exposed by the tvmaze API, imagine now that this API allows you to sort the data using the following request.\n1 https://api.tvmaze.com/episode?sortDesc=runtime,type You may get the following response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [ { \u0026#34;id\u0026#34;: 869671, \u0026#34;url\u0026#34;: \u0026#34;https://www.tvmaze.com/episodes/869671/westworld-1x01-the-original\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;The Original\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;regular\u0026#34;, \u0026#34;runtime\u0026#34;: 60 }, { \u0026#34;id\u0026#34;: 911204, \u0026#34;url\u0026#34;: \u0026#34;https://www.tvmaze.com/episodes/911204/westworld-1x03-the-stray\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;The Stray\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;regular\u0026#34;, \u0026#34;runtime\u0026#34;: 60, }, { \u0026#34;id\u0026#34;: 911201, \u0026#34;url\u0026#34;: \u0026#34;https://www.tvmaze.com/episodes/911201/westworld-1x02-chestnut\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Chestnut\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;regular\u0026#34;, \u0026#34;runtime\u0026#34;: 60 } ] The request is executed again, it may now return the following response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [ { \u0026#34;id\u0026#34;: 911201, \u0026#34;url\u0026#34;: \u0026#34;https://www.tvmaze.com/episodes/911201/westworld-1x02-chestnut\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Chestnut\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;regular\u0026#34;, \u0026#34;runtime\u0026#34;: 60, }, { \u0026#34;id\u0026#34;: 869671, \u0026#34;url\u0026#34;: \u0026#34;https://www.tvmaze.com/episodes/869671/westworld-1x01-the-original\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;The Original\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;regular\u0026#34;, \u0026#34;runtime\u0026#34;: 60, }, { \u0026#34;id\u0026#34;: 911204, \u0026#34;url\u0026#34;: \u0026#34;https://www.tvmaze.com/episodes/911204/westworld-1x03-the-stray\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;The Stray\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;regular\u0026#34;, \u0026#34;runtime\u0026#34;: 60, } ] The same request returned a different result each time it was executed even though the data did not change. This is because all three records have the same runtime and the same type, the sort function will correctly sort the data by runtime and type, but since the request wasn\u0026rsquo;t specific enough the other properties are randomly sorted. Again, to avoid this type of issue, always include an extra sort property that is guaranteed to be unique, like an identifier. If caching is supported then the two API requests above would have all resulted in a cache miss, since the data was in a different order, resulting in a different ETAG for each request, thus a miss.\nThe API request above should really be.\n1 https://api.tvmaze.com/episode?sortDesc=runtime,type,id By including the Id property, the sorting order will now always be guaranteed, because the id field will always be unique. By always adding a unique field to a sort operation, the API will achieve greater cache hits and provide an overall better experience for the consumer of the API.\n","permalink":"http://localhost:1313/post/2021/sort-functions-are-nondeterministic/","summary":"\u003cp\u003eWhen building a Web API, RESTful or GraphQL, you may want to expose some functionality that allows a client application to sort data.\u003c/p\u003e\n\u003cp\u003eFrom my experience, this is often not implemented correctly. Many developers fail to realize that sorting should always be sort plus one. The plus one is a unique value, like a primary key or identifier. The reason for this is that sorting in most databases, \u003ca href=\"https://docs.microsoft.com/en-us/sql/t-sql/queries/select-order-by-clause-transact-sql?redirectedfrom=MSDN\u0026view=sql-server-ver15#arguments\"\u003elike SQL Server\u003c/a\u003e, is nondeterministic, meaning the sort function may return different results each time they are called with a specific set of input values even if the database state that they access remains the same.\u003c/p\u003e","title":"Sort Functions Are Non-Deterministic"},{"content":"In my last post I wrote about how you can leverage JSON Schema to do Web API validation. The main benefit is that the API can expose the schema as an API resource, clients of the API can consume the schema and execute it on their end against any data. The benefit of doing API validation like this is that the client does not need to duplicate any validation logic, they only need to execute the schema. In this post, I would like to explore API validation in .NET, using the library FluentValidation and exposing validation errors using Problem Details.\nI\u0026rsquo;ll start by creating a new Web API project using the following dotnet command.\n1 dotnet new webapi Next, I\u0026rsquo;ll add the package FluentValidation as a dependency.\n1 dotnet add package FluentValidation.AspNetCore The .NET webapi template, the one used when I executed dotnet new webapi, comes with a weathers controller that exposes the following model.\n1 2 3 4 5 6 7 8 9 10 public class WeatherForecast { public DateTime Date { get; set; } public int TemperatureC { get; set; } public int TemperatureF =\u0026gt; 32 + (int)(TemperatureC / 0.5556); public string Summary { get; set; } } I\u0026rsquo;m going to enhance the Web API project I just created by introducing a new HTTP POST endpoint in the API. This new endpoint will allow a client app to create a new WeatherForecast resource. The WeatherForecast model will also be enhanced by having fluent validation enforce the following rules.\nThe field, Date, is required, must be UTC. The field, Summary, is required and must not be empty. The field, TemperatureC is required and must be a valid range I\u0026rsquo;ll need to create a class that implements AbstractValidator. I will name the class, WeatherForecastValidator, as seen below, it will be used to define all the validation rules that pertain to the WeatherForecast model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class WeatherForecastValidator : AbstractValidator\u0026lt;WeatherForecast\u0026gt; { public WeatherForecastValidator() { RuleFor(x =\u0026gt; x.Date) .NotNull() .WithErrorCode(\u0026#34;missingDate\u0026#34;) .WithMessage(\u0026#34;Date is required field\u0026#34;) .Must(x =\u0026gt; x.Kind == DateTimeKind.Utc) .WithErrorCode(\u0026#34;dateIsNotUtc\u0026#34;) .WithMessage(\u0026#34;Date is not in UTC format.\u0026#34;); RuleFor(x =\u0026gt; x.Summary) .NotNull() .WithErrorCode(\u0026#34;missingSummary\u0026#34;) .WithMessage(\u0026#34;Summary is a required field\u0026#34;) .NotEmpty() .WithErrorCode(\u0026#34;invalidSummary\u0026#34;) .WithMessage(\u0026#34;Summary cannot be empty\u0026#34;); RuleFor(x =\u0026gt; x.TemperatureC) .InclusiveBetween(-18, 40) .WithErrorCode(\u0026#34;invalidTemperatureValue\u0026#34;) .WithMessage(\u0026#34;TemperatureC must be between -18 and 40\u0026#34;); } } Now that the validation rules are in place, I need to register the validator on the .NET pipelines so that any validation errors are properly propagated through the .NET application. To register a validator class, just use the AddFluentValidation extension method off of the AddControllers method in the Startup class. You can add them one by one or have FluentValidation scan a given assembly to automatically import validators.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class Startup { public Startup(IConfiguration configuration) { Configuration = configuration; } public IConfiguration Configuration { get; } public void ConfigureServices(IServiceCollection services) { services.AddControllers() .AddFluentValidation(fv =\u0026gt; fv.RegisterValidatorsFromAssemblyContaining\u0026lt;Program\u0026gt;()); } } The last thing I now need to do is update the WeatherForecast controller with the new HTTP POST endpoint.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 [ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class WeatherForecastController : ControllerBase { private static readonly string[] Summaries = new[] { \u0026#34;Freezing\u0026#34;, \u0026#34;Bracing\u0026#34;, \u0026#34;Chilly\u0026#34;, \u0026#34;Cool\u0026#34;, \u0026#34;Mild\u0026#34;, \u0026#34;Warm\u0026#34;, \u0026#34;Balmy\u0026#34;, \u0026#34;Hot\u0026#34;, \u0026#34;Sweltering\u0026#34;, \u0026#34;Scorching\u0026#34; }; private readonly ILogger\u0026lt;WeatherForecastController\u0026gt; _logger; public WeatherForecastController(ILogger\u0026lt;WeatherForecastController\u0026gt; logger) { _logger = logger; } [HttpGet] public IEnumerable\u0026lt;WeatherForecast\u0026gt; Get() { var rng = new Random(); return Enumerable.Range(1, 5).Select(index =\u0026gt; new WeatherForecast { Date = DateTime.Now.AddDays(index), TemperatureC = rng.Next(-20, 55), Summary = Summaries[rng.Next(Summaries.Length)] }) .ToArray(); } [HttpPost] public IActionResult Post([FromBody] WeatherForecast weather) { if(!ModelState.IsValid) { var validationProblemDetails = new ValidationProblemDetails(ModelState); return BadRequest(validationProblemDetails); } return Created(weather); } } Perfect, I can test the new endpoint by sending an HTTP POST to the WeatherForecast endpoint with the following JSON in the HTTP request body.\n1 2 3 4 5 { \u0026#34;date\u0026#34;: \u0026#34;2021-11-07T01:29:13.695Z\u0026#34;, \u0026#34;temperatureC\u0026#34;: 0, \u0026#34;summary\u0026#34;: \u0026#34;\u0026#34; } As you can tell from the payload, the field summary is empty, this should trigger a validation error that should end up in me getting a ProblemDetals response object. When I submit the HTTP request, I received the following response from the API.\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#34;type\u0026#34;: \u0026#34;https://tools.ietf.org/html/rfc7231#section-6.5.1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;One or more validation errors occurred.\u0026#34;, \u0026#34;status\u0026#34;: 400, \u0026#34;traceId\u0026#34;: \u0026#34;00-6f0c73e8915f564488e1b4ab538c3ec0-ae270bf71b7ee44b-00\u0026#34;, \u0026#34;errors\u0026#34;: { \u0026#34;Summary\u0026#34;: [ \u0026#34;Summary cannot be empty\u0026#34; ] } } Nice, as you can see the error messages defined in the WeatherForecastValidator are included on the response object. With this approach, the API can send a friendly error message to the client whenever a validation error occurs. FluentValidation even helps you with Localizations.\nCouple of things to note. The validations I\u0026rsquo;ve written here are arbitrary and outright stupid, for example, celsius can include decimal values like -17.8, which would be 0 Fahrenheit. Please ignore the validity of the validation rules I have written here, this post is meant to demonstrate how to provide practical API validation in .NET by using tools and patterns like FluentValidation and ProblemDetails.\nIf you are in .NET Core 3 or earlier, ModelState is not serialized using camel casing. You can see that in the example above, where the word \u0026ldquo;Summary\u0026rdquo;, in the error array has an upper case s. This was fixed in 7439 for newer versions of .NET. In 3 or earlier you can fix that by using the following code.\n1 2 services.AddControllers() .AddNewtonsoftJson(mvcNewtonsoftJsonOptions =\u0026gt; mvcNewtonsoftJsonOptions.UseCamelCasing(processDictionaryKeys: true)); If you prefer to not have a check on the state of ModelState on every action that requires validation. You can configure a global response by configuring the ApiBehaviorOptions in .NET on the StartUp.cs class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public void ConfigureServices(IServiceCollection services) { services.AddControllers() .AddFluentValidation(fv =\u0026gt; fv.RegisterValidatorsFromAssemblyContaining\u0026lt;Program\u0026gt;()); services.AddSwaggerGen(c =\u0026gt; { c.SwaggerDoc(\u0026#34;v1\u0026#34;, new OpenApiInfo { Title = \u0026#34;webapi\u0026#34;, Version = \u0026#34;v1\u0026#34; }); }); services.Configure\u0026lt;ApiBehaviorOptions\u0026gt;(options =\u0026gt; { options.InvalidModelStateResponseFactory = context =\u0026gt; { var problemDetails = new ValidationProblemDetails(context.ModelState) { Instance = context.HttpContext.Request.Path, Status = 400, Type = $\u0026#34;https://httpstatuses.com/400\u0026#34;, Detail = \u0026#34;Validation Error\u0026#34; }; return new BadRequestObjectResult(problemDetails) { ContentTypes = { \u0026#34;application/problem+json\u0026#34; } }; }; }); } ","permalink":"http://localhost:1313/post/2021/a-practical-web-api-validation-strategy/","summary":"\u003cp\u003eIn my \u003ca href=\"/post/2021/a-better-web-api-validation-strategy/\"\u003elast post\u003c/a\u003e I wrote about how you can leverage \u003ca href=\"https://json-schema.org/\"\u003eJSON Schema\u003c/a\u003e to do Web API validation. The main benefit is that the API can expose the schema as an API resource, clients of the API can consume the schema and execute it on their end against any data. The benefit of doing API validation like this is that the client does not need to duplicate any validation logic, they only need to execute the schema. In this post, I would like to explore API validation in .NET, using the library \u003ca href=\"https://fluentvalidation.net/\"\u003eFluentValidation\u003c/a\u003e and exposing validation errors using \u003ca href=\"https://datatracker.ietf.org/doc/html/rfc7807\"\u003eProblem Details\u003c/a\u003e.\u003c/p\u003e","title":"A Practical Web API Validation Strategy"},{"content":"As an API developer, you will eventually need to determine how to handle data validation.\nThe .NET ecosystem offers a few options, the first option, validation attributes, can be used to annotate how a model should be validated. Validation attributes are great, they don\u0026rsquo;t require any external dependencies, you can specify error messages, create your own custom validator, validate against many data types.\nFor example, take the following Movie class, notice how the properties have been annotated with validation rules.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class Movie { public int Id { get; set; } [Required] [StringLength(100)] public string Title { get; set; } [ClassicMovie(1960)] [DataType(DataType.Date)] [Display(Name = \u0026#34;Release Date\u0026#34;)] public DateTime ReleaseDate { get; set; } [Required] [StringLength(1000)] public string Description { get; set; } [Range(0, 999.99)] public decimal Price { get; set; } public Genre Genre { get; set; } public bool Preorder { get; set; } } Another popular option in .NET is using the library Fluent Validation, this library allows you to define validation rules that are then applied against a data class. Fluent Validation offers the same capabilities as validation attributes, with one additional benefit, using the Fluent Validation library means that your classes are kept clean. The validation rules are defined in a configuration class.\nFor example, take the following CustomerValidator class, which defines rules that can be applied to the Customer class.\n1 2 3 4 5 6 7 8 9 public class CustomerValidator : AbstractValidator\u0026lt;Customer\u0026gt; { public CustomerValidator() { RuleFor(x =\u0026gt; x.Surname).NotEmpty(); RuleFor(x =\u0026gt; x.Forename).NotEmpty().WithMessage(\u0026#34;Please specify a first name\u0026#34;); RuleFor(x =\u0026gt; x.Discount).NotEqual(0).When(x =\u0026gt; x.HasDiscount); RuleFor(x =\u0026gt; x.Address).Length(20, 250); RuleFor(x =\u0026gt; x.Postcode).Must(BeAValidPostcode).WithMessage(\u0026#34;Please specify a valid postcode\u0026#34;); } } These validation techniques are often used the same way in a Web API, be that REST, GraphQL, or even GRPC. The API client, usually a UI application running on the browser, sends an HTTP request to the API to create or update a resource. The API then runs the request through validation to determine if the request contains any input that is outside of the accepted range. For example, imagine the case where a REST API exposes an endpoint, customers, the endpoint accepts an HTTP POST as a way to create new customer resources, now in this fictional API, all customers must have a valid email address. When the API receives the request to create a new customer, it must ensure that the email address is valid before the customer can be created. This presents a problem, the end-user using the UI app doesn\u0026rsquo;t have immediate feedback. Meaning, the end-user had to probably fill out a registration form, the form is then submitted on behalf of the end-user, who must then wait for the API to validate the data, if there is a problem, then the UI is notified, which in turn alerts the user that there was a problem. Repeating the process until there are no more validation errors.\nThere is nothing necessarily wrong with doing validation as described above, but it can be better. The UI application can apply the same validation rules as the API. Thus avoiding sending a request to the API that will just fail with a validation error. Typically, the UI application will have obtained the validation rules through some form of documentation, like an Open API definition file or word of mouth, the API developer informing the UI developer. I believe this approach to doing validation is the most common, at least it is the one I have encountered the most and it works. Especially if you are running a small system where the API developer and UI developer are the same developer or if your API only has one client.\nThere are some drawbacks to doing validation as described above, that is when the API has many clients. Each client has to implement validation, and it must be done correctly. Should the API validation rules change, then so must each client, they each need to change their validation rules, which requires syncing up and redeploying each client application. Not an easy task, even if you yourself developed all the clients, it would still require a coordinated effort to update all clients. Some developers would argue that this is when API versioning should be done, true, but only in the case that the resource itself requires modification. The validation rules are what changed, not the underlying resource.\nTo summarize, our problems are as follows.\nClient applications have to implement validation rules, often it involves the client copying the validation rules that the API uses. Validation rules need to be more dynamic. Major changes must be versioned. How can these problems be solved, while arriving at a better API architecture?\nTo avoid the first problem, client applications duplicating validation rules, the rules themselves should come from the API. One of the beauties of the REST architectural style is that everything is centered around resources. What are resources? Well, just about anything can be a resource, even validation rules. By having the API exposes the validation rules as a resource, the UI application does not have to duplicate the validation rules. Instead, the API can provide the UI with the validation code, the UI can then execute the code to confirm the data is valid. Having the API provide the UI with code to execute is not a new idea. In fact, it is one of the key constraints in the REST architecture, code on demond. The idea is that the UI application is extended by having the API providing additional logic. This idea is not just limited to REST, there is no rule that prevents you from doing the same in GraphQL or GRPC.\nSo, validation rules will be exposed as a resource. How? Well, most APIs are built with JSON, and JSON Schema can be used to described JSON, plus it allows us to provide human-readable documentation. That sounds familiar, describing data formats while providing human-readable messages is the key essence of data validation. JSON Schema is the perfect tool for API validation, the schema can be exposed as an API resource, the client application can then consume the schema, execute it and validate the data. This is perfect, JSON Schema solves all of the problems identified above. The client doesn\u0026rsquo;t have to duplicate any validation rules. The schema resource can be versioned in case of a major change. The rules are now more dynamic and are easily changed.\nI want to create an example to see how it all would work.\nA while back I exposes a customers resource on my Chinook JSON:API project. The endpoint does not have any data validation, you can send a POST request with an empty firstName or lastName property and the API won\u0026rsquo;t stop the request, it will fail due to a database constraint. I purposely skipped over validation when I created the endpoint since I figure I would eventually write this post and would tackle data validation at that time.\nI am going to modify the Chinook API project by exposing a JSON Schema resource that can be retrieved by a client app. The client application can then execute the schema on their end to confirm that the data being submitted adheres to the schema definition. For those unaware, here is how a customer is represented as of October 2021 on the Chinook API.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;Luís\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Gonçalves\u0026#34;, \u0026#34;company\u0026#34;: \u0026#34;Embraer - Empresa Brasileira de Aeronáutica S.A.\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;Av. Brigadeiro Faria Lima, 2170\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;São José dos Campos\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;SP\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;Brazil\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12227-000\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;+55 (12) 3923-5555\u0026#34;, \u0026#34;fax\u0026#34;: \u0026#34;+55 (12) 3923-5566\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;luisg@embraer.com.br\u0026#34; } } } Base on the Chinook database schema, at least the following validation should exist.\nThe field firstName cannot be null, must be no more than 40 characters. The field lastName cannot be null, must be no more than 20 characters. The field company, is an optional field, when provided, it cannot be more than 80 characters. The field address, is an optional field, when provided, it cannot be more than 70 characters. The field city, is an optional field, when provided, it cannot be more than 40 characters. The field state, is an optional field, when provided, it cannot be more than 40 characters. The field country, is an optional field, when provided, it cannot be more than 40 characters. The field postalCode, is an optional field, when provided, it cannot be more than 10 characters. The field phone, is an optional field, when provided, it cannot be more than 24 characters. The field fax, is an optional field, when provided, it cannot be more than 24 characters. The email field is required, must be no more than 60 characters, must be a valid email. Now, time to write the JSON schema that will be used for validation. You could choose to write it by hand, but there are better ways. I find stoplight to be the perfect tool to use to create REST APIs.\nWhat is even better is that they support writing JSON schemas, they make it super simple, give it a resource model in JSON, they\u0026rsquo;ll generate the model, then you\u0026rsquo;ll be given the opportunity to annotate the model with validation rules. For example, I can take the customer model above and import it into their system.\nI\u0026rsquo;ll add all the rules to the model, like which field is required, the minimum and maximum length of a field, a regex expression for the email field. That will generate the following JSON file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 { \u0026#34;$schema\u0026#34;: \u0026#34;https://json-schema.org/draft/2019-09/schema\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Chinook customer model.\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [ \u0026#34;type\u0026#34;, \u0026#34;attributes\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1 }, \u0026#34;attributes\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [ \u0026#34;firstName\u0026#34;, \u0026#34;lastName\u0026#34;, \u0026#34;email\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;firstName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 40 }, \u0026#34;lastName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 20 }, \u0026#34;company\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 80 }, \u0026#34;address\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 70 }, \u0026#34;city\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 40 }, \u0026#34;state\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 40 }, \u0026#34;country\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 40 }, \u0026#34;postalCode\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 10 }, \u0026#34;phone\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 24 }, \u0026#34;fax\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;maxLength\u0026#34;: 24 }, \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1, \u0026#34;pattern\u0026#34;: \u0026#34;^[a-zA-Z0-9.!#$%\u0026amp;\u0026#39;*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\u0026#34;, \u0026#34;maxLength\u0026#34;: 60, \u0026#34;format\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;example\u0026#34;: \u0026#34;yunier@hey.com\u0026#34; } } } }, \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;required\u0026#34;: [ \u0026#34;data\u0026#34; ] } Info: You can see the schema in action by visiting this link. Validation should be successful, to see a failed example, go to this link. Validation failed in the second link because the id property should not be included when creating a resource. Since the id is determined by the database.\nThe question now becomes how to expose the JSON schema on the API. Base on section 9.5.1.1 of the JSON Schema Core specification. JSON schemas should be exposed through a relationship link, using the describedBy link, see IANA link relations page for more details. Which is great, linking from one resource to another is the essence of hypermedia driven APIs like the Chinook API project. Now unfortunately, the Chinook API project implements version 1.0 of the JSON:API specification. This version of JSON:API does not provide any guidance on how to use JSON:API with JSON Schema. Luckily for us, the upcoming 1.1 version of JSON:API does provide us with guidance. Per the 1.1 specifications a JSON:API document may contain a Links object as a top level member. Links object may contain a describedBy field, which is a link to a description document (e.g. OpenAPI or JSON Schema) for the current document. I am going to apply the recommendation made in version 1.1 of JSON:API to the Chinook API, even though Chinook implements version 1.0.\nTo keep everything simple, the schema will be hosted directly on the Chinook Project. You should know that there is a JSON Schema store. The store can be used to host any JSON schema you defined or browser for JSON Schemas, for example, appsettings.json, a file used by .NET Core can be found on the store.\nAfter modifying the Chinook API project to support JSON Schema, the customer resource will now include the newly added describedBy link. See the response document below or visit https://chinook-jsonapi.herokuapp.com/customers to see it in action.\n1 2 3 4 5 6 7 8 9 10 11 12 13 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/customers\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:44323\u0026#34;, \u0026#34;describedBy\u0026#34;: \u0026#34;https://localhost:44323/customers/schemas\u0026#34; }, \u0026#34;data\u0026#34;: [ // data omitted for brevity. ] } With this change, clients of the API can now retrieve a schema definition file that they can execute against data to confirm that the data captured by the client is valid. The API and the client will now be in sync with each other when it comes to validation. Should that validation change, then the validation resource would change, this is where versioning would become an important feature.\nOne last question remains, how should the client execute the schema?\nWell, that would depend on the client, JSON Schema is supported in many languages, find your language in their list of supported validators, and use the library that corresponds to your language to execute the schema. Most of you will probably be working in JavaScript, if so, consider using the library ajv. It is highly popular, blazing fast and should cover all your needs.\n","permalink":"http://localhost:1313/post/2021/a-better-web-api-validation-strategy/","summary":"\u003cp\u003eAs an API developer, you will eventually need to determine how to handle data validation.\u003c/p\u003e\n\u003cp\u003eThe .NET ecosystem offers a few options, the first option, \u003ca href=\"https://docs.microsoft.com/en-us/aspnet/core/mvc/models/validation?view=aspnetcore-6.0#validation-attributes\"\u003evalidation attributes\u003c/a\u003e, can be used to annotate how a model should be validated. Validation attributes are great, they don\u0026rsquo;t require any external dependencies, you can specify error messages, create your own \u003ca href=\"https://docs.microsoft.com/en-us/aspnet/core/mvc/models/validation?view=aspnetcore-6.0#validation-attributes\"\u003ecustom validator\u003c/a\u003e, validate against many data types.\u003c/p\u003e\n\u003cp\u003eFor example, take the following Movie class, notice how the properties have been annotated with validation rules.\u003c/p\u003e","title":"A Better Web API Validation Strategy"},{"content":"Recently I was asked to review a Web API written in Node.js. The API exposes an authentication endpoint, this authentication endpoint must be highly available, responsive, and it cannot become a bottleneck, otherwise, the user experience is severely impacted. Unfortunately, the endpoint had become a bottleneck and was impacting the overall performance of the application. Upon further review, it was determined that the problem was coming from a hashing function that takes the user\u0026rsquo;s password, hashes it, and compares the result with the stored hashed password from the database. Here is the code without the implementation details.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 var app = module.exports = express(); hash(function (err, pass, salt, hash) { // implementations omitted for brevity. }); function authenticate(name, pass, fn) { hash({ password: pass, salt: user.salt }, function (err, pass, salt, hash) { // implementations omitted for brevity. }); } app.post(\u0026#39;/login\u0026#39;, function(req, res){ authenticate(req.body.username, req.body.password, function(err, user){ // implementations omitted for brevity. }); }); The hash function used a CPU-intensive algorithm and Node.js is notorious for being bad at heavy CPU operations, it will block the thread. This is because Node.js is single threaded. To ensure the authentication Web API is never blocked by a CPU intensive operation the hashing function was off-loaded to a microservice. This microservice is dedicated to just computing the output of the hash function. The benefit of this approach is that now the authentication Web API will no longer be blocked by CPU work, additionally, the hashing microservice can now be scaled up or down based on load. As it turns out, this is a well-known pattern in Node.js. See this post by Alex Vasilyev to learn more.\nInfo: Here is the new authentication Web API architecture.\n","permalink":"http://localhost:1313/post/2021/improving-a-cpu-intensive-node-app/","summary":"\u003cp\u003eRecently I was asked to review a Web API written in Node.js. The API exposes an authentication endpoint, this authentication endpoint must be highly available, responsive, and it cannot become a bottleneck, otherwise, the user experience is severely impacted. Unfortunately, the endpoint had become a bottleneck and was impacting the overall performance of the application. Upon further review, it was determined that the problem was coming from a hashing function that takes the user\u0026rsquo;s password, hashes it, and compares the result with the stored hashed password from the database. Here is the code without the implementation details.\u003c/p\u003e","title":"Improving A CPU-Intensive Node.js App"},{"content":"I was looking through some of my bookmarked Github issues when I rediscovered issue #32488, in that issue a comment was made that caught my attention. The comment stated that in .NET the order of interfaces impacts performance. This is because in the .NET CLR all class definitions have a collection of methods and interface definitions. Casting is a linear search that walks the interface definition. If you are constantly casting to an Interface located at the end then the CLR must do a longer walk.\nTo see how much of it can impact performance I will use BenchmarkDotNet. In case you didn\u0026rsquo;t know, BenchmarkDotnet is an open-source project that helps you track benchmarks and track the performance of your code. It is an extremely useful project, it is used by Entity Framework, ASP.NET Core itself, Newtonsoft.Json, Autofac, MediatR, SignalR, Serilog, and so on.\nIf you want to learn how to use BenchmarkDotNet then I suggest checking out Tim Corey\u0026rsquo;s intro video.\nTo start the benchmarking experiment I will add a bunch of empty interfaces inside a new console application. The interfaces are as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public interface IDog { } public interface ICat { } public interface IHorse { } public interface IDuck { } public interface ICow { } public interface ISpider { } public interface ITiger { } public interface ILion { } public interface IHuman { } public interface IMonkey { } public interface IDeer { } public interface IHog { } public interface IChicken { } public interface IDonkey { } Next, I am going to add two new classes, one that will execute BechmarkDotNet and the other one will be our base object, I\u0026rsquo;ll call it CanWalk, given that all the interfaces define above are for an animal that can walk.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public class InterfaceBenchmarks { [Benchmark] public void CastToCatFirstInterface() { var walkable = new CanWalk(); for (int i = 0; i \u0026lt; 1000000; i++) { _ = (ICat) walkable; } } [Benchmark] public void CastToDonkeyLastInterface() { var walkable = new CanWalk(); for (int i = 0; i \u0026lt; 1000000; i++) { _ = (IDonkey) walkable; } } } public class CanWalk : ICat, IHorse, IMonkey, ICow, ITiger, ILion, IHuman, IDuck, ISpider, IDeer, IHog, IChicken, IDonkey { } Next, the main method of the console application needs to be updated by calling the BenchmarkDotnetRunner.\n1 2 3 4 5 6 7 class Program { static void Main(string[] args) { BenchmarkRunner.Run\u0026lt;InterfaceBenchmarks\u0026gt;(); } } OK, it is time to execute BenchmarkDotNet, and our results are\u0026hellip;\nWait, what? That is not exactly what I was expecting. Well, it is what I was expecting, it confirms the comment made on the issue #32488, I\u0026rsquo;m just not sure if it is something worth optimizing for given how small the performance difference is between the first and second test. I reran the test and I ended up with the same result. So, while the order of interfaces matters when casting, I would argue that you should not worry about it too much. Having the interface at the start or end will probably have little impact on the overall performance of your application.\nThat is all for now, thanks for reading, until next time.\n","permalink":"http://localhost:1313/post/2021/the-order-of-interfaces-impacts-performance/","summary":"\u003cp\u003eI was looking through some of my bookmarked Github issues when I rediscovered issue \u003ca href=\"https://github.com/dotnet/runtime/pull/32488\"\u003e#32488\u003c/a\u003e, in that issue a \u003ca href=\"https://github.com/dotnet/runtime/pull/32488#discussion_r380818002\"\u003ecomment\u003c/a\u003e was made that caught my attention. The comment stated that in .NET the order of interfaces impacts performance. This is because in the \u003ca href=\"https://docs.microsoft.com/en-us/dotnet/standard/clr\"\u003e.NET CLR\u003c/a\u003e all class definitions have a collection of methods and interface definitions. Casting is a linear search that walks the interface definition. If you are constantly casting to an Interface located at the end then the CLR must do a longer walk.\u003c/p\u003e","title":"The Order Of Interfaces Impacts Performace"},{"content":"Writing good documentation is such an underrated skill, to that extent so is writing ReadME files, ReadME files can be an awesome addition to your project. They give you an opportunity to document all sorts of stuff. For me, a good project should come along with a good ReadME file. The file should outline everything that is necessary for me to interact with the project. Like how to run the unit/integration test, the project\u0026rsquo;s architecture, any terminology, the roadmap for the project, and the most important piece, examples on how to use the project.\nWhenever I begin a new project I try to make a good effort to include a good ReadME file. I like having the project\u0026rsquo;s documentation live with the project\u0026rsquo;s code, not some external site, with this approach documentation is versioned, given that more than likely the code will be managed by some version of control system, git being the most popular these days. This starts the idea that documenation should be treated the same way we treat code.\n1 2 3 Always write documentation as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live. That is my twist to the famous quote. To that extend, writing good documentation can sometimes come down to having good tools. In the case of ReadME files one good tool at your disposal is readme.so, this tool was created by Katherine Peterson, it helps you scaffold a ReadMe file from the ground up. Additionally, you could also explore some of the most popular projects on Github to see how they each approach creating a ReadMe file.\n","permalink":"http://localhost:1313/post/2021/writing-a-good-readme-is-a-skill/","summary":"\u003cp\u003eWriting good documentation is such an underrated skill, to that extent so is writing ReadME files, ReadME files can be an awesome addition to your project. They give you an opportunity to document all sorts of stuff. For me, a good project should come along with a good ReadME file. The file should outline everything that is necessary for me to interact with the project. Like how to run the unit/integration test, the project\u0026rsquo;s architecture, any terminology, the roadmap for the project, and the most important piece, examples on how to use the project.\u003c/p\u003e","title":"Writing A Good ReadME Is A Skill"},{"content":"HTML, CSS, and Javascript, are the languages of the world wide web, the platform. They are used to create websites, to make them interactive, and to make them beautiful. At one point in my career, I was more plugged into this world. A world that I feel I\u0026rsquo;ve fallen behind since I myself have not exclusively worked on a UI project since the days of AngularJS. That doesn\u0026rsquo;t mean that I don\u0026rsquo;t do any front-end work anymore, it is just that these days I spent most of the time doing back-end development. I am familiar with some of the modern frameworks like Angular, React.js, Next.js and everyone\u0026rsquo;s new favorite, svelte. By falling behind I mean that I am not up to date with some of the new tools and technologies that have been created since the days of AngularJS. I want to use this post to write about some of these new techniques and tools that are available for front-end development.\nI would like to start by exploring micro frontends, a term originally coined by the team over at ThoughtWorks on their 2016 technology radar. With their most recent technology radar advising the tech industry to adopt micro frontends. So, just what in the world are micro frontends and how is this pattern for building front-end applications different from island architechture? Given that both micro frontend and island architecture advocate creating independent UI components. The key difference is their approach to building the front-end, the idea behind the island architecture is that the HTML is rendered on the server, the HTML rendered would contain placeholders that denote regions (think widgets) on the HTML that can be hydrated on the client. Essentially you have an HTML page is rendered with most of the functionality readily available for use, loading independent apps on the client.\nBoth micro frontend and island architecture try to bring the benefits of microservices to the front-end, benefits like being technology agnostic and having code isolation. Thought you should really avoid mixing multiple frameworks on the same application, a problem that is known as micro frontend anarchy, though technically possible, it should be avoided. In a micro frontend architecture, you should only standardize on styling and how the different parts of the application are integrated. For styling the go-to techniques are CSS-in-JS, a technique popularized by Christopher Chedeau, andCSS Modules.\nDo not confuse CSS Modules, the community standard with CSS Modules, the standard created by the Web Hypertext Application Technology Working Group (WHATWG) even though they both share the same name. Remember, developers are bad are naming things.\nOne of the main problems these techniques are attempting to solve is the fact that CSS was never bound to any scope, hence the name Cascading Style Sheets. CSS was created in a world where we expected the full HTML to be returned from the server. These techniques aim to avoid namespace collision, performance degradation that comes from using CSS at scale, and reducing the size of CSS bundles. You should know that these patterns come at a cost, you should be aware of their benefits and tradeoff before you use them.\nThen there are web components, which consist of a number of APIs that allow you to build independent applications. This is done using Custom Elements, an API that allows you to define behaviors and styles for new HTML elements. The shadow DOM, which lets you scope CSS. HTML template, defines how to declare fragments of markup that go unused at page load, but can be instantiated later on at runtime, and ES Modules, defines the inclusion and reuse of JS documents in other JS documents. Web components are a great way to build independent applications, applications that are rendered on the client because up until 2020, the only way to use Shadow DOM was to construct a shadow root using JavaScript. For example.\n1 2 3 const host = document.getElementById(\u0026#39;host\u0026#39;); const shadowRoot = host.attachShadow({mode: \u0026#39;open\u0026#39;}); shadowRoot.innerHTML = \u0026#39;\u0026lt;h1\u0026gt;Hello Shadow DOM\u0026lt;/h1\u0026gt;\u0026#39;; If you are working on an app the does server side rendering (SSR), then you will need to use the declaritive shadow dom, otherwise, you will run into performance and rendering issues, FOUC which stands for \u0026lsquo;show a flash of unstyled content\u0026rsquo; being the most common. Declarative Shadow Rom brings the shadow dom to the server. However, it does bring another set of issues as noted by Rich Harris, the creator of svelte.js. Declarative shadow dom is only supported in Chrome as of August 2021.\n1. create a component standard\n2. oops, it doesn\u0026#39;t work without JS\n3. create declarative shadow DOM\n4. oops, embedding CSS in JS is bad\n5. create CSS modules\n6. oops, CSS modules don\u0026#39;t work with declarative shadow DOM\n7. create declarative constructible stylesheets\n8. oops, ... https://t.co/Vm3jqxcB5U\n\u0026mdash; Rich Harris (@Rich_Harris) August 3, 2021 You should check out vanilla-extract. It allows you to write your styles in TypeScript (or JavaScript) with locally scoped class names and CSS Variables, then generate static CSS files at build time.\nAs noted in the tweet, CSS modules do not work with a declarative shadow dom, meaning we can\u0026rsquo;t use CSS modules on server rendering applications. The google chrome team proposed declaritive constructible stylesheets, it aims to fix this problem by relying on APIs that avoid FOUC. I\u0026rsquo;m starting to feel like a lot of these issues could have been avoided if the scope specification had become a part of CSS. Luckily for us some out, there are still fighting the good fight, hoping to simplify how we use CSS because to me all these patterns stem from the fact that we seem to have a hard time scoping CSS.\nAnyways, moving away from web components to another front-end pattern that I\u0026rsquo;ve been seen pop up lately, and that is rendering HTML completely on the server. In reality, this is not a new pattern. This is how most of the web worked up until ~2011. One key player in this area is Hotwire, created by the same guys that made Ruby on Rails. Hotwire is heavily used over at Hey.com. It works great, for the most part, but I am concern with the fact that many of the people that created the project left the project after the basecamp debacle.\nBlazor is another popular choice for building SPA with C# by running on the server or browser utilizing WebAssembly. I honestly don\u0026rsquo;t know how I feel about Blazor and its role in the platform. I do like the idea of being able to use LINQ on the browser, that is simply because LINQ is an awesome feature of the C# language.\nCan\u0026rsquo;t forget about Progressive Web Apps, PWA leverage the power of the platform to create installable applications that do not require a store of any kind to act as a middle man between the consumers and the applications, kinda like the Play Store or Apple Store. Progressive web apps offer many amazing benefits, but I often hear that they can be complicated, which may explain why I\u0026rsquo;ve only seen one or two companies successfully used them. The top companies being Twitter and Google.\nThat be all for now. Till next time, cheerio.\n","permalink":"http://localhost:1313/post/2021/the-platform/","summary":"\u003cp\u003eHTML, CSS, and Javascript, are the languages of the world wide web, the \u003ca href=\"https://youtu.be/BzX4aTRPzno?t=937\"\u003eplatform\u003c/a\u003e. They are used to create websites, to make them interactive, and to make them beautiful. At one point in my career, I was more plugged into this world. A world that I feel I\u0026rsquo;ve fallen behind since I myself have not exclusively worked on a UI project since the days of AngularJS. That doesn\u0026rsquo;t mean that I don\u0026rsquo;t do any front-end work anymore, it is just that these days I spent most of the time doing back-end development. I am familiar with some of the modern frameworks like \u003ca href=\"https://angular.io/\"\u003eAngular\u003c/a\u003e, \u003ca href=\"https://reactjs.org/\"\u003eReact.js\u003c/a\u003e, \u003ca href=\"https://nextjs.org/\"\u003eNext.js\u003c/a\u003e and \u003ca href=\"https://insights.stackoverflow.com/survey/2021#section-most-loved-dreaded-and-wanted-web-frameworks\"\u003eeveryone\u0026rsquo;s new favorite\u003c/a\u003e, \u003ca href=\"https://svelte.dev/\"\u003esvelte\u003c/a\u003e. By falling behind I mean that I am not up to date with some of the new tools and technologies that have been created since the days of AngularJS. I want to use this post to write about some of these new techniques and tools that are available for front-end development.\u003c/p\u003e","title":"The Platform"},{"content":"A worker service is a type of Background Service that are generally use for long-running task. They can be seen as the equivalent of Windows Services in the .NET Framework, though a worker service is not limited to just windows.\nIf you are building a worker service, then more than likely you will need to be able to write log data, be that general information of the worker services or perhaps just errors. If you plan to use Serilog, then this post will show you how to configure Serilog on a worker project.\nTo configure Serilog you will need the following NuGet packages.\n1 2 3 4 dotnet add package Serilog dotnet add package Serilog.Sinks.Console dotnet add package Serilog.Extensions.Hosting dotnet add package Serilog.Settings.Configuration Once the packages are installed modify the Program.cs file to bootstrap Serilog and to confiure Serilog. The following class illustrates how I usually configure Serilog.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 public class Program { public static void Main(string[] args) { Log.Logger = new LoggerConfiguration() .WriteTo.Console() .CreateBootstrapLogger(); try { CreateHostBuilder(args) .Build() .Run(); } catch (Exception ex) { Log.Fatal(ex, \u0026#34;Worker service failed initiation. See exception for more details\u0026#34;); } finally { Log.CloseAndFlush(); } } public static IHostBuilder CreateHostBuilder(string[] args) { return Host.CreateDefaultBuilder(args).ConfigureServices((hostContext, services) =\u0026gt; { services.AddHostedService\u0026lt;Worker\u0026gt;(); }).ConfigureLogging((hostContext, builder) =\u0026gt; { builder.ConfigureSerilog(hostContext.Configuration); }).UseSerilog(); } } The most important piece of code in this class is the usage of the method UserSerilog. Withouth it you will have a configured Serilog but it won\u0026rsquo;t be used by the worker, so don\u0026rsquo;t forgot to use it. As for the method ConfigureSerilog, that is one of my extension methods. It reads Serilog configurations from appsettings.json. Here is the class definition.\n1 2 3 4 5 6 7 8 9 10 11 public static class LoggingBuilderExtensions { public static ILoggingBuilder ConfigureSerilog(this ILoggingBuilder loggingBuilder, IConfiguration configuration) { Log.Logger = new LoggerConfiguration() .ReadFrom.Configuration(configuration) .CreateLogger(); return loggingBuilder; } } and my JSON configuration is as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 { \u0026#34;Serilog\u0026#34;: { \u0026#34;WriteTo\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;Console\u0026#34;, \u0026#34;Args\u0026#34;: { \u0026#34;theme\u0026#34;: \u0026#34;Serilog.Sinks.SystemConsole.Themes.AnsiConsoleTheme::Code, Serilog.Sinks.Console\u0026#34;, \u0026#34;outputTemplate\u0026#34;: \u0026#34;[{Timestamp:HH:mm:ss} {Level:u3}] {Message:lj} \u0026lt;s:{SourceContext}\u0026gt;{NewLine}{Exception}\u0026#34; } } ] } } If you want to use a different Sink, file, for example, then simply install the corresponding NuGet package and update the JSON configurations.\nNow that everything has been configured, I can run the worker to see the prettified console logs.\nAin\u0026rsquo;t Serilog just awesome.\nAnyways, I hope this post helped you configured Serilog.\n","permalink":"http://localhost:1313/post/2021/worker-services-configure-serilog/","summary":"\u003cp\u003eA \u003ca href=\"https://docs.microsoft.com/en-us/dotnet/core/extensions/workers\"\u003eworker service\u003c/a\u003e is a type of \u003ca href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.hosting.backgroundservice?view=dotnet-plat-ext-5.0\"\u003eBackground Service\u003c/a\u003e that are generally use for long-running task. They can be seen as the equivalent of Windows Services in the .NET Framework, though a worker service is not limited to just windows.\u003c/p\u003e\n\u003cp\u003eIf you are building a worker service, then more than likely you will need to be able to write log data, be that general information of the worker services or perhaps just errors. If you plan to use Serilog, then this post will show you how to configure Serilog on a worker project.\u003c/p\u003e","title":"Worker Services Configure Serilog"},{"content":"So far in my JSON:API series I\u0026rsquo;ve covered the home resource, adding your own resource, adding an exception handling middleware and how to expose relationship between resources. For the today\u0026rsquo;s post, I would like to cover creating resources. I will update the chinook project by allowing POST request on the customers collections to add new customers.\nTo get started, the customer controller needs to have a method that will accept the incoming POST request. I\u0026rsquo;ve decided to call the method CreateCustomerResource, the method will accept a JSON:API document from the request body. The full method signature is defined below.\n1 2 3 4 5 6 7 [HttpPost] [Route(CustomerRoutes.CustomerResourceCollection)] public async Task\u0026lt;IActionResult\u0026gt; CreateCustomerResource([FromBody] Document jsonApiDocument) { var document = await _customerResource.CreateCustomerResource(jsonApiDocument); return Created(document.SelfLink(), documnet); } Notice that the method has been decorated with the HttpPost attribute and it is using the same route as the customer resource collection. If no error are encountered, then API returns a 201 Created HTTP status code with a JSON:API document along with a location header pointing to the location of the newly created resource. The helper function, SelfLink is part of JsonApiFramework.\nNext step, updating the customer resource class to take in the incoming JSON:API document from the controller so that it can be dispatched via Mediatr. The response from the mediatr handler is then used to create the response JSON:API document.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public async Task\u0026lt;Document\u0026gt; CreateCustomerResource(Document jsonApiDocument) { var createdCustomerResource = await _mediator.Send(new CreateCustomerResourceCommand(jsonApiDocument)); var currentRequestUri = _httpContextAccessor.HttpContext.GetCurrentRequestUri(); using var chinookDocumentContext = new ChinookJsonApiDocumentContext(currentRequestUri); var document = chinookDocumentContext .NewDocument(currentRequestUri) .SetJsonApiVersion(JsonApiVersion.Version10) .Links() .AddSelfLink() .AddUpLink() .LinksEnd() .Resource(createdCustomerResource) .Relationships() .AddRelationship(InvoiceResourceKeyWords.ToManyRelationShipKey, new[] { Keywords.Related }) .RelationshipsEnd() .Links() .AddSelfLink() .LinksEnd() .ResourceEnd() .WriteDocument(); _logger.LogInformation(\u0026#34;Request for {URL} generated JSON:API document {doc}\u0026#34;, currentRequestUri, document); return document; } The Mediatr command class is also very simple, it accepts an incoming JSON:API document and stores it on a public field that can be access by the handler. Here is the class definition for the command class.\n1 2 3 4 5 6 7 8 9 public class CreateCustomerResourceCommand : IRequest\u0026lt;Customer\u0026gt; { public CreateCustomerResourceCommand(Document document) { Document = document; } public Document Document { get; } } Now, on to the handler. The job of the handler is to extract the new resource out of the JSON:API document, to apply any business rules/logic, and then to finally save the new resource on the database. The class definition for the handler is as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class CreateCustomerResourceHandler : IRequestHandler\u0026lt;CreateCustomerResourceCommand, Customer\u0026gt; { private readonly ChinookDbContext _chinookDbContext; public CreateCustomerResourceHandler(ChinookDbContext chinookDbContext) { _chinookDbContext = chinookDbContext; } public async Task\u0026lt;Customer\u0026gt; Handle(CreateCustomerResourceCommand request, CancellationToken cancellationToken) { var jsonApiDocumentContext = new ChinookJsonApiDocumentContext(request.Document); var resource = jsonApiDocumentContext.GetResource\u0026lt;Customer\u0026gt;(); _chinookDbContext.Customers .Add(resource); await _chinookDbContext .SaveChangesAsync(cancellationToken); return resource; } } Nothing to exciting, the resource data is extracted out of the JSON:API document and it gets attached to EF Core, which then saves it to the database. Simple stuff, it will get a little more complicated later, but for now this will work. Time to test our code and what better tool than POSTMAN to do the test.\nYou should know that POSTMAN comes with a CLI, newman, a great option for executing test written in POSTMAN on your CI/CD pipeline. See this blog post more details.\nI\u0026rsquo;ll go ahead an open up postman. I\u0026rsquo;m going to add new API request of type POST, using https://chinook-jsonapi.herokuapp.com/customers as the request URL. Under the headers tab, I will add a new header, content-type, and set the value to application/vnd.api+json, since this is required by JSON:API. Next, the request body will use the following json as the request body.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;$randomFirstName\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;$randomLastName\u0026#34;, \u0026#34;company\u0026#34;: \u0026#34;$randomCompanyName\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;$randomStreetAddress\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;$randomCity\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;$randomCountryCode\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;$randomCountry\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;$randomCountryCode\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;$randomPhoneNumber\u0026#34;, \u0026#34;fax\u0026#34;: \u0026#34;$randomPhoneNumber\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;$randomEmail\u0026#34; } } } The postman syntax includes opening and closing brackets. They were excluded above because Jekyll, the engine that powers this blog interprets them as empty strings, so it will not render them on the page.\nI am providing all attributes here since there are no validation or rules yet. Note the use of postman\u0026rsquo;s dynamic variable syntax. Postman uses faker.js under the hood to generate this random data. Do forgive me for using random country code for the state property. Didn\u0026rsquo;t feel like making a helper function that generates a random state.\n.NET has a copy of faker called Bogus, an excellent library to use in your Unit/Integration test whenever you need to generate data. You could even it use it to seed a test database.\nWhen I execute the POSTMAN request I get a 201 Created as the response code with the newly created user on the response body. You can execute the test yourself by pulling the chinook repository down and importing the test into POSTMAN. All tests are located under the test folder.\nGreat, so the API now supports creating new customers. All is great in the world, well, almost all. See what we have here is the most basic example, it is simple and easy, starting with the fact that in this sample app there are no validation or business rules, but what really complicates thing is having resources like customers, that have related resources.\nThe customer resource we just created did not have any relationships, meaning the customer being created did not have any invoices. There might be instances where a new customer has one or many invoices, to support this type of request the API should be enhanced to support adding new invoices, the link between the new customer and the new invoice can be established by including the relationship on the HTTP request body for the customer resource or vice versa. The response body of such request may look like the following JSON document.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;john\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;smith\u0026#34;, \u0026#34;company\u0026#34;: \u0026#34;Auth0\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;1234 Sesame Street\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;El Dorado\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;NY\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;United States of America\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;33543\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;999-871-0900\u0026#34;, \u0026#34;fax\u0026#34;: \u0026#34;345-987-7890\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;ssmith@auth0.com\u0026#34; } }, \u0026#34;relationships\u0026#34;: { \u0026#34;invoices\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;invoices\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;9\u0026#34; } } } } Another option would be to support sideposting, that is being able to create multiple resources of different types in a single request so that the client application doesn\u0026rsquo;t have to send multiple POST request for every new resource it needs to link. This is where JSON:API extensions for atomic operations come into play. It establishes a contract on how the client should tructure a request that contains multiple resource and how the server should handle these type of request.\nThe release of JSON:API v1.1 should simplify side posting due to the introduction of lid. An lid is an id generated on the client, this id is then used to link resources on a client document. Remember the great thing about JSON:API is that it is a wire protocol for incrementally fetching and updating a graph over HTTP. The keywords here being updating a graph, by combining lid with resource linkage a client can create a JSON:API documents with a complex hierarchy, for example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 POST /customer HTTP/2.0 Content-Type: application/vnd.api+json\u0026#34; Accept: application/vnd.api+json { \u0026#34;data\u0026#34;: { \u0026#34;lid\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;john\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;smith\u0026#34;, \u0026#34;company\u0026#34;: \u0026#34;Auth0\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;1234 Sesame Street\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;El Dorado\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;NY\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;United States of America\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;33543\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;999-871-0900\u0026#34;, \u0026#34;fax\u0026#34;: \u0026#34;345-987-7890\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;ssmith@auth0.com\u0026#34; }, \u0026#34;relationships\u0026#34;: { \u0026#34;invoices\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;invoices\u0026#34;, \u0026#34;lid\u0026#34;: \u0026#34;9\u0026#34; } } }, \u0026#34;included\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;invoices\u0026#34;, \u0026#34;lid\u0026#34;: \u0026#34;9\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;invoiceDate\u0026#34;: \u0026#34;MjAwOS0wMS0wMSAwMDowMDowMA==\u0026#34;, \u0026#34;billingAddress\u0026#34;: \u0026#34;Theodor-Heuss-Straße 34\u0026#34;, \u0026#34;billingCity\u0026#34;: \u0026#34;Stuttgart\u0026#34;, \u0026#34;billingState\u0026#34;: null, \u0026#34;billingCountry\u0026#34;: \u0026#34;Germany\u0026#34;, \u0026#34;billingPostalCode\u0026#34;: \u0026#34;70174\u0026#34;, \u0026#34;total\u0026#34;: \u0026#34;MS45OA==\u0026#34; } }] } } In the example above, the included resource, invoices, could also have a relationship member that links to another resource on the included array, and that resource could also have a relationship member that links to another resource on the included array and so on. When the server receives this JSON:API document, it can create all those resources on the database and establish relatioships as defined on the client document.\n","permalink":"http://localhost:1313/post/2021/json-api-creating-new-resources/","summary":"\u003cp\u003eSo far in my JSON:API series I\u0026rsquo;ve covered the \u003ca href=\"https://www.yunier.dev/2020-09-14-Adding-Home-Resource/\"\u003ehome resource\u003c/a\u003e, adding \u003ca href=\"https://www.yunier.dev/2020-10-30-Adding-Customer-Resource/\"\u003eyour own resource\u003c/a\u003e, adding an \u003ca href=\"https://www.yunier.dev/2020-10-19-Exception-Handling-Middleware/\"\u003eexception handling middleware\u003c/a\u003e and how to \u003ca href=\"https://www.yunier.dev/2020-12-06-Exposing-Relationships/\"\u003eexpose relationship\u003c/a\u003e between resources. For the today\u0026rsquo;s post, I would like to cover creating resources. I will update the chinook project by allowing \u003ca href=\"https://datatracker.ietf.org/doc/html/rfc2616/#section-9.5\"\u003ePOST\u003c/a\u003e request on the customers collections to add new customers.\u003c/p\u003e\n\u003cp\u003eTo get started, the customer controller needs to have a method that will accept the incoming POST request. I\u0026rsquo;ve decided to call the method \u003cstrong\u003eCreateCustomerResource\u003c/strong\u003e, the method will accept a \u003ca href=\"https://jsonapi.org/format/#document-structure\"\u003eJSON:API document\u003c/a\u003e from the request body. The full method signature is defined below.\u003c/p\u003e","title":"JSON:API - Creating New Resources"},{"content":"In the world of front end development there is no better tool than Lighthouse. Lighthouse is an open-source, automated tool for improving the quality of web pages. You can run it against any web page, public or requiring authentication. It has audits for performance, accessibility, progressive web apps, SEO and more.\nThe only problem with lighthouse, at least from my experience, is that it is not used until after the app has been deployed. I haven\u0026rsquo;t been involved in any project that utilizes lighthouse upfront, certainly not on the ci/cd pipelines. Which can be done using lighthouse-ci. There is also another way to get lighthouse running on your ci/cd pipeline, it involves executing lighthouse while you are running your unit test regardless of the unit test engine, be that Jest or Mocha. However, these tools lack the ability to invoke a web browser, after all, lighthouse can only be run against an actual website.\nThis is where Playwright comes into play. For those that don\u0026rsquo;t know, playwright is the new kid on the block, it is a tool that enables end to end testing. Playwright is able to invoke a headless browser session, could be chrome, firefox, or webkit, then using that headless session we can run lighthouse thus giving us the ability to run lighthouse on a unit test. The idea came from this blog post by applitools where they combine lighthouse with cypress and Pa11y to do performance testing.\nTo get started I am going to start a new project using the following npm command.\n1 npm init Followed by this npm install command to get all dependencies installed.\n1 npm install --save-dev jest playwright lighthouse typescript ts-jest ts-node @types/jest @types/lighthouse @babel/preset-typescript Here is the package.json file generated so far using the above npm commands.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \u0026#34;name\u0026#34;: \u0026#34;lighthouse-ci-playwright\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A sample project on how to use lighthouse along with playwright\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;jest\u0026#34; }, \u0026#34;repository\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;git\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://github.com/circleupx/lighthouse-ci-example-with-playwright\u0026#34; }, \u0026#34;keywords\u0026#34;: [ \u0026#34;playwright\u0026#34;, \u0026#34;lighthouse\u0026#34; ], \u0026#34;author\u0026#34;: \u0026#34;CircleUpX\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@babel/preset-typescript\u0026#34;: \u0026#34;^7.13.0\u0026#34;, \u0026#34;@types/jest\u0026#34;: \u0026#34;^26.0.23\u0026#34;, \u0026#34;jest\u0026#34;: \u0026#34;^27.0.4\u0026#34;, \u0026#34;jest-junit\u0026#34;: \u0026#34;^12.1.0\u0026#34;, \u0026#34;lighthouse\u0026#34;: \u0026#34;^8.0.0\u0026#34;, \u0026#34;playwright\u0026#34;: \u0026#34;^1.11.1\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;ts-jest\u0026#34;: \u0026#34;^27.0.3\u0026#34;, \u0026#34;ts-node\u0026#34;: \u0026#34;^10.0.0\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^4.3.2\u0026#34; } } Next, I\u0026rsquo;ll configure jest using the following jest.config.ts file.\n1 2 3 4 5 6 7 8 9 10 11 12 export default { preset: \u0026#39;ts-jest\u0026#39;, testEnvironment: \u0026#39;node\u0026#39;, testMatch: [\u0026#39;\u0026lt;rootDir\u0026gt;/**/test/*.ts\u0026#39;], testPathIgnorePatterns: [\u0026#39;/node_modules/\u0026#39;], coverageDirectory: \u0026#39;./coverage\u0026#39;, coveragePathIgnorePatterns: [\u0026#39;node_modules\u0026#39;, \u0026#39;src/database\u0026#39;, \u0026#39;src/test\u0026#39;, \u0026#39;src/types\u0026#39;], reporters: [\u0026#39;default\u0026#39;, \u0026#39;jest-junit\u0026#39;], globals: { \u0026#39;ts-jest\u0026#39;: { diagnostics: false } }, transform: {}, testTimeout : 20000 }; and I will use the following setting to configure babel. This is required to get jest to play nicely with typescript, see using typescript for more information.\n1 2 3 export const presets = [ \u0026#39;@babel/preset-typescript\u0026#39;, ]; Everything has been configured, I am ready to write my first test. I\u0026rsquo;ll add a new \u0026rsquo;test\u0026rsquo; folder to host all the unit test file. I am going to use https://shop.polymer-project.org/ as my test site, this is full feature e-commerce Progressive Web App demo site. The test will fire up playwright, it will visit the demo site, then lighthouse will be invoked, it will perform an analysis on the site\u0026rsquo;s performance. Lastly, I will assert that the site\u0026rsquo;s performance is greater than or equal to 80.\nI\u0026rsquo;ll add a new file under the test folder, I will name it lighthouse.ts. The first thing I need to do is to import the required dependencies.\n1 2 import { Browser, chromium, Page } from \u0026#39;playwright\u0026#39;; import lh = require(\u0026#39;lighthouse\u0026#39;); Next, I\u0026rsquo;ll configure beforeAll, beforeEach, afterAll and afterEach. This methods are responsible for constructing and tearing down playwright.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 let browser: Browser; beforeAll(async () =\u0026gt; { browser = await chromium.launch({ headless: false, args: [`--remote-debugging-port=8041`] }); }); afterAll(async () =\u0026gt; { await browser.close(); }); let page: Page; beforeEach(async () =\u0026gt; { page = await browser.newPage(); }); afterEach(async () =\u0026gt; { await page.close(); }); Do notice the args parameter passed the launch method, it includes a debugging port, make a note of the port number, you will need it later on. The flag \u0026ndash;remote-debugging-port will allow the lighthouse instance of chrome to connect to the playwright instance of chrome. Feels like a hack, but this works. The code below represents the unit test.\n1 2 3 4 5 6 7 8 describe(\u0026#39;Load shop.polymer-project.org home page\u0026#39;, function () { it(\u0026#39;should have a page performance greater than or equal to 90\u0026#39;, async () =\u0026gt; { const options = { logLevel: \u0026#39;info\u0026#39;, output: \u0026#39;html\u0026#39;, onlyCategories: [\u0026#39;performance\u0026#39;], port: 8041 }; const runnerResult = await lh(\u0026#39;https://shop.polymer-project.org/\u0026#39;, options); console.log(\u0026#39;Performance score was\u0026#39;, runnerResult.lhr.categories.performance.score * 100); expect(runnerResult.lhr.categories.performance.score * 100).toBeGreaterThanOrEqual(80); }); }); As you can see the test itself is rather simple. Some lighthouse options are set, notice the port number is the same port number used on the remote flag. The only category that will be tested will be performance. The log level and output type are set. By the way, the output could be set to json. Here is the entire test file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import { Browser, chromium, Page } from \u0026#39;playwright\u0026#39;; import lh = require(\u0026#39;lighthouse\u0026#39;); let browser: Browser; beforeAll(async () =\u0026gt; { browser = await chromium.launch({ headless: false, args: [`--remote-debugging-port=8041`] }); }); afterAll(async () =\u0026gt; { await browser.close(); }); let page: Page; beforeEach(async () =\u0026gt; { page = await browser.newPage(); }); afterEach(async () =\u0026gt; { await page.close(); }); describe(\u0026#39;Load shop.polymer-project.org home page\u0026#39;, function () { it(\u0026#39;should have a page performance greater than or equal to 80\u0026#39;, async () =\u0026gt; { const options = { logLevel: \u0026#39;info\u0026#39;, output: \u0026#39;html\u0026#39;, onlyCategories: [\u0026#39;performance\u0026#39;], port: 8041 }; const runnerResult = await lh(\u0026#39;https://shop.polymer-project.org/\u0026#39;, options); console.log(\u0026#39;Performance score was\u0026#39;, runnerResult.lhr.categories.performance.score * 100); expect(runnerResult.lhr.categories.performance.score * 100).toBeGreaterThanOrEqual(80); }); }); To run it, use the following npm command.\n1 npm test It will yield the following result.\n1 2 3 4 5 6 7 8 9 PASS test/lighthouse.ts (9.791 s) Load shop.polymer-project.org home page √ should have a page performance greater than or equal to 80 (8010 ms) Test Suites: 1 passed, 1 total Tests: 1 passed, 1 total Snapshots: 0 total Time: 9.925 s, estimated 11 s Ran all test suites. Awesome. My idea works, but I wouldn\u0026rsquo;t recommend using it. Too complicated, it is easier to install and run the lighthouse-cli tool, it basically does everything I just documented on this post, all you have to do is provide it with a URL.\nHope you enjoyed this post.\nCredits:\nUsing lighthouse programmatically lighthouse-jest-example [BUG] Can\u0026rsquo;t connect playwright to lighthouse. Getting 426 error ","permalink":"http://localhost:1313/post/2021/running-lighthouse-in-cicd-pipeline/","summary":"\u003cp\u003eIn the world of front end development there is no better tool than \u003ca href=\"https://developers.google.com/web/tools/lighthouse/\"\u003eLighthouse\u003c/a\u003e. Lighthouse is an open-source, automated tool for improving the quality of web pages. You can run it against any web page, public or requiring authentication. It has audits for performance, accessibility, progressive web apps, SEO and more.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://developers.google.com/web/tools/lighthouse/images/report.png\" alt=\"Report generated by lighthouse\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003eThe only problem with lighthouse, at least from my experience, is that it is not used until after the app has been deployed. I haven\u0026rsquo;t been involved in any project that utilizes lighthouse upfront, certainly not on the ci/cd pipelines. Which can be done using \u003ca href=\"https://github.com/GoogleChrome/lighthouse-ci\"\u003elighthouse-ci\u003c/a\u003e. There is also another way to get lighthouse running on your ci/cd pipeline, it involves executing lighthouse while you are running your unit test regardless of the unit test engine, be that \u003ca href=\"https://github.com/facebook/jest\"\u003eJest\u003c/a\u003e or \u003ca href=\"https://github.com/mochajs/mocha\"\u003eMocha\u003c/a\u003e. However, these tools lack the ability to invoke a web browser, after all, lighthouse can only be run against an actual website.\u003c/p\u003e","title":"Running Lighthouse On CI/CD Pipeline"},{"content":"I\u0026rsquo;m seeing many API developers, specially those that come from a REST background, struggle with GraphQL simply because they are introducing protocol concepts into their GraphQL documents.\ngive it a REST... pic.twitter.com/sUxqL4ACdj\n\u0026mdash; I Am Devloper (@iamdevloper) November 13, 2020 GraphQL is not bound to any network protocol, it is most often implemented on top of the HTTP protocol and it only uses the most basic features of HTTP. That is because GraphQL treats HTTP as a dum pipe. Introducing protocol concept such as a 404 Not Found status code into your GraphQL documents will only cause you development pain.\nNow, I know what you are thinking, \u0026ldquo;but I don\u0026rsquo;t want to reinvent the wheel\u0026rdquo;, normally you would be correct, after all the http spec takes care of many things that developers often take for granted, things like authorization and caching, but in the case of GraphQL ignoring the HTTP spec is the price of admission to creating a good GraphQL API.\nCredits:\nA few things to think about before blindly dumping REST for GraphQL. Serving over HTTP Modeling Errors in GraphQL ","permalink":"http://localhost:1313/post/2021/graphql-is-protocol-agnostic/","summary":"\u003cp\u003eI\u0026rsquo;m seeing many API developers, specially those that come from a REST background, struggle with GraphQL simply because they are introducing protocol concepts into their GraphQL documents.\u003c/p\u003e\n\u003cblockquote class=\"twitter-tweet\"\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003egive it a REST... \u003ca href=\"https://t.co/sUxqL4ACdj\"\u003epic.twitter.com/sUxqL4ACdj\u003c/a\u003e\u003c/p\u003e\u0026mdash; I Am Devloper (@iamdevloper) \u003ca href=\"https://twitter.com/iamdevloper/status/1327190006520221696?ref_src=twsrc%5Etfw\"\u003eNovember 13, 2020\u003c/a\u003e\u003c/blockquote\u003e \u003cscript async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"\u003e\u003c/script\u003e\r\n\u003cp\u003eGraphQL is not bound to any network protocol, it is most often implemented on top of the HTTP protocol and it only uses the most basic features of HTTP. That is because GraphQL treats HTTP as a dum pipe. Introducing protocol concept such as a 404 Not Found status code into your GraphQL documents will only cause you development pain.\u003c/p\u003e","title":"GraphQL Is Protocol Agnostic"},{"content":"One of the many benefits of working with JSON:API and GraphQL is having a standardize way to communicate failures to a client. If you are not working with a spec like JSON:API or GraphQL, then you are in the hands of the developer that built the API and every developers implements error handling differently.\nAlmost every HTTP API that I\u0026#39;ve consumed implements errors differently. Can we just agree to use Problem Details and be done with it?\n\u0026mdash; Derek Comartin (@codeopinion) April 11, 2021 RFC 7807, better known as \u0026ldquo;Problem Details for HTTP APIs\u0026rdquo; was created to standardize the way errors are handled in web APIs. Even though the RFC was published in 2016, it is not well-known, in fact, I myself did not know about it until 2019. To help spread the knowledge and usefulness of using \u0026ldquo;Problem Details\u0026rdquo; I would like to demonstrate how you can utilize it in .NET.\nProblem details is currently going through a revision. You can make contributions here.\n.NET already comes with a problem details class. No need to import extra packages. The problem details class has the following properties.\nDetail: A human-readable explanation specific to this occurrence of the problem. Extensions: Gets the IDictionary\u0026lt;TKey,TValue\u0026gt; for extension members. Problem type definitions MAY extend the problem details object with additional members. Extension members appear in the same namespace as other members of a problem type. Instance: A URI reference that identifies the specific occurrence of the problem.It may or may not yield further information if dereferenced. Status: The HTTP status code(RFC7231, Section 6) generated by the origin server for this occurrence of the problem. Title: A short, human-readable summary of the problem type.It SHOULD NOT change from occurrence to occurrence of the problem, except for purposes of localization(e.g., using proactive content negotiation; seeRFC7231, Section 3.4). Type: A URI reference RFC3986 that identifies the problem type. This specification encourages that, when dereferenced, it provide human-readable documentation for the problem type (e.g., using HTML W3C.REC-html5-20141028). When this member is not present, its value is assumed to be \u0026ldquo;about:blank\u0026rdquo;. To demonstrate how to use RFC 7807 on .NET I will create a new dotnet web api project by running the following dotnet command.\n1 dotnet new webapi -n ProblemDetailsExample Within this new web api I will create a new exception handling middleware. The purpose of this middleware is to act as a mapper between an exception and a problem details json document. Basically, when an exception is caught by the middleware, it will extract metadata from the excepton to produce a Problem Details object.\nMicrosoft has some really good documentation on how to handle errors in .NET Web APIs, see Handle errors in ASP.NET Core web APIs.\nHere is the middleware class definition without any mapping logic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public class ExceptionHandlingMiddleware { private readonly RequestDelegate _next; private readonly ILogger\u0026lt;ExceptionHandlingMiddleware\u0026gt; _logger; public ExceptionHandlingMiddleware(RequestDelegate next, ILogger\u0026lt;ExceptionHandlingMiddleware\u0026gt; logger) { _next = next; _logger = logger; } public async Task Invoke(HttpContext httpContext) { try { await _next(httpContext); } catch (Exception exception) { await HandleException(httpContext, exception); } } private Task HandleException(HttpContext httpContext, Exception exception) { } } When you create a new .NET web api it comes with a weather forecast controller, the controller returns an in-memoery list of weather forecast. I will use this controller to test my middleware, instead of returning a list of weather forecast, the GET() method will be changed to throw a new not implemented exception.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class WeatherForecastController : ControllerBase { private readonly ILogger\u0026lt;WeatherForecastController\u0026gt; _logger; public WeatherForecastController(ILogger\u0026lt;WeatherForecastController\u0026gt; logger) { _logger = logger; } [HttpGet] public IEnumerable\u0026lt;WeatherForecast\u0026gt; Get() { throw new NotImplementedException(\u0026#34;This method has not been implemented.\u0026#34;); } } To handle the GET() method throwing an exception I will need to modify the HandleException method within the middleware. The code below is the my first attempt at handling the exception.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 private Task HandleException(HttpContext httpContext, Exception exception) { var itle = exception.GetType().Name; var detail = exception.Message; var instance = httpContext.Request.GetDisplayUrl(); var problemDetails = new ProblemDetails { Title = problemDetailsTitle, Detail = problemDetailsDetail, Status = 500, Type = \u0026#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\u0026#34;, Instance = instance }; httpContext.Response.StatusCode = 500; httpContext.Response.ContentType = \u0026#34;application/problem+json\u0026#34;; return httpContext.Response.WriteAsJsonAsync(problemDetails); } As you can see the method extract information from the exception object. The exception name is used as the title, the exception message is used as the detail and the current request uri is used as the instance. Since this is just sample project, the type field will simply point to the MDN docs that corresponds to the HTTP status returned by the middleware. In your project, the type property should point to some documentation that provides addtional details.\nThe extension methods WriteAsJsonAsync and GetDisplayUrl are part of Microsoft.AspNetCore.Http.Extensions. The method WriteAsJsonAsync is only available in .NET 5 and above.\nWhen an HTTP GET request is sent to /weatherforecast the not implemented exception is handled by the middleware, producing the following HTTP response.\n1 2 3 4 5 6 7 { \u0026#34;type\u0026#34;: \u0026#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;NotImplementedException\u0026#34;, \u0026#34;status\u0026#34;: 500, \u0026#34;detail\u0026#34;: \u0026#34;This method has not been implemented.\u0026#34;, \u0026#34;instance\u0026#34;: \u0026#34;https://localhost:44336/WeatherForecast\u0026#34; } Great, I\u0026rsquo;m pretty happy with my first implementation, later on I\u0026rsquo;ll extend my implementation by introducing my own problem details type. For now I do want to make the title more human readable, to accomplish that I will rely upon Humanizer. Humanizer will tansform \u0026ldquo;NotImplemnetedException\u0026rdquo; into \u0026ldquo;Not Implemented Exception\u0026rdquo;. To install humanizer, run the following command.\n1 Install-Package Humanizer.Core -Version 2.8.26 With humanizer now installed, I\u0026rsquo;ll modify the HandleException method again.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 private Task HandleException(HttpContext httpContext, Exception exception) { var title = exception.GetType().Name.Humanize(LetterCasing.Title); var detail = exception.Message; var instance = httpContext.Request.GetDisplayUrl(); var problemDetails = new ProblemDetails { Title = title, Detail = detail, Status = 500, Type = \u0026#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\u0026#34;, Instance = instance }; httpContext.Response.StatusCode = 500; httpContext.Response.ContentType = ProblemDetailsContentType; return httpContext.Response.WriteAsJsonAsync(problemDetails); } Running the same HTTP request yields the following result.\n1 2 3 4 5 6 7 { \u0026#34;type\u0026#34;: \u0026#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Not Implemented Exception\u0026#34;, \u0026#34;status\u0026#34;: 500, \u0026#34;detail\u0026#34;: \u0026#34;This method has not been implemented.\u0026#34;, \u0026#34;instance\u0026#34;: \u0026#34;https://localhost:44336/WeatherForecast\u0026#34; } Much better.\nI now want to extend the problem details class. I want to include additional types, and move the logic that extracts data from the exception into another class. This specialize problem details class can accept an expcetion as a parameter, the class would then create a problem details document out of the exception context. The idea is very similar to how the ValidationProblemDetails class works.\nThe class will be named ExceptionProblemDetails, for now it will have three constructors, the default constructor, one constructor that take an exception and a constructor that takes an exception and an httpcontext.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class ExceptionProblemDetails : ProblemDetails { public ExceptionProblemDetails() { } public ExceptionProblemDetails(Exception exception) { var title = exception.GetType().Name.Humanize(LetterCasing.Title); var message = exception.GetAllExceptions().Select(e =\u0026gt; e.Message); var detail = exception.Message; Title = title; Detail = detail; Errors = new Dictionary\u0026lt;string, IEnumerable\u0026lt;string\u0026gt;\u0026gt;(); Status = 500; Type = exception.HelpLink; } public ExceptionProblemDetails(Exception exception, HttpContext context) { var title = exception.GetType().Name.Humanize(LetterCasing.Title); var detail = exception.Message; var instance = context.Request.GetDisplayUrl(); Title = exception.GetType().Name; Detail = detail; Instance = instance; Errors = new Dictionary\u0026lt;string, IEnumerable\u0026lt;string\u0026gt;\u0026gt;() { { \u0026#34;friendlyErrorMessage\u0026#34;, new []{\u0026#34;Server encountered an error, please try again.\u0026#34;} } }; Status = context.Response.StatusCode; Type = exception.HelpLink; } public IDictionary\u0026lt;string, IEnumerable\u0026lt;string\u0026gt;\u0026gt; Errors { get; } } The first constructor doesn\u0026rsquo;t do anything, it assumes that the developer will fill all the property, this is an a good option for developer that prefer to their own style of metadata extraction. The second constructor just provides some good defaults i.e. 500 as the status code. The last constructor is where most of the exception metadata will be extracted. It simplifies the HandleException method.\n1 2 3 4 5 6 7 8 private static Task HandleException(HttpContext httpContext, Exception exception) { var problemDetails = new ExceptionProblemDetails(exception, httpContext); httpContext.Response.StatusCode = (int) problemDetails.Status; httpContext.Response.ContentType = ProblemDetailsContentType; return httpContext.Response.WriteAsJsonAsync(problemDetails); } Here is response produces by the middleware now.\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;errors\u0026#34;: { \u0026#34;friendlyErrorMessage\u0026#34;: [ \u0026#34;Server encountered an errorm, please try again.\u0026#34; ] }, \u0026#34;type\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;NotImplementedException\u0026#34;, \u0026#34;status\u0026#34;: 200, \u0026#34;detail\u0026#34;: \u0026#34;This method has not been implemented.\u0026#34;, \u0026#34;instance\u0026#34;: \u0026#34;https://localhost:44336/WeatherForecast\u0026#34; } Perfect, though you should note that in your implementation, you should return a more detail error message, not just \u0026ldquo;Server encountered an error, please try again.\u0026rdquo;. I did that here in order to keep the example simple, it is up to your on how to implement the friendly error message, be that having a switch statment that handle each exception, perhaps having the content live in another API and then fetching it when the exception occurs, or you may try implementing your own exception class that provides all the required metadata.\n","permalink":"http://localhost:1313/post/2021/problem-details-for-http-apis/","summary":"\u003cp\u003eOne of the many benefits of working with \u003ca href=\"https://jsonapi.org/\"\u003eJSON:API\u003c/a\u003e and \u003ca href=\"https://graphql.org/\"\u003eGraphQL\u003c/a\u003e is having a standardize way to communicate failures to a client. If you are not working with a spec like JSON:API or GraphQL, then you are in the hands of the developer that built the API and every developers implements error handling differently.\u003c/p\u003e\n\u003cblockquote class=\"twitter-tweet\"\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eAlmost every HTTP API that I\u0026#39;ve consumed implements errors differently. Can we just agree to use Problem Details and be done with it?\u003c/p\u003e","title":"Problem Details for HTTP APIs"},{"content":"I don\u0026rsquo;t believe I\u0026rsquo;ve mention this before here, but I am a huge fan of Hey. By far the best email service I have ever used. What makes Hey even cooler is the team behind Hey sharing they engineering approach to different problems. Be that through various tweets or blog post like Scaling the hottest app in tech on AWS and Kubernetes which outline how they use k8s. Recently, they shared how to tackle ay11 under hey accessibility is a lot of work. One thing that stood out was to me was their usage of axe-core. Axe-core is an accessibility engine for automated Web UI testing. Which reminded me of playwright, so I started to wonder if the two could be combined, turns out they can be. Let\u0026rsquo;s explore how to do that.\nI\u0026rsquo;m going to reuse the project I created in my last playwright post, it already has few test and has been configure to use other packages like Mocha and Chai. To get started, axe-core needs to be installed, you can use the following command.\n1 npm install axe-core With axe now installed I can start building some test. I\u0026rsquo;ll add new file under the test folder, I\u0026rsquo;ll call it a11y.ts. I\u0026rsquo;ll start by importing the required packages need for our test.\n1 2 3 4 5 6 import { Browser, Page } from \u0026#39;playwright/types/types\u0026#39;; import { chromium } from \u0026#39;playwright\u0026#39;; import { describe } from \u0026#39;mocha\u0026#39;; import * as fs from \u0026#39;fs\u0026#39;; import { AxePlugin, AxeResults } from \u0026#39;axe-core\u0026#39;; import { expect } from \u0026#39;chai\u0026#39;; Next, I\u0026rsquo;ll need to configure playwright\u0026rsquo;s browser and page object to run before and after each test. That can be done like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 let browser: Browser; before(async () =\u0026gt; { browser = await chromium.launch(); }); after(async () =\u0026gt; { await browser.close(); }); let page: Page; beforeEach(async () =\u0026gt; { page = await browser.newPage(); }); afterEach(async () =\u0026gt; { await page.close(); }); Base on the axe docs, I need to include axe.min.js in any fixture or test system. That file can be loaded from the node_modules folder using node\u0026rsquo;s own file system module.\n1 2 // Loads axe.min.js const file: string = fs.readFileSync(require.resolve(\u0026#39;axe-core/axe.min.js\u0026#39;), \u0026#39;utf8\u0026#39;); The tricky part comes next, you see, you need to understand a very important aspect of playwright. That is that playwright has two execution contexts, one for playwright and one for the browser. These two execution context will never share state with each other, it is not possible to access a window or document directly in playwright\u0026rsquo;s exection context. So, how can we get axe.min.js, which was loaded under playwright context injected into the browser context. The answer can be found under the evaluate function of playwright\u0026rsquo;s page object. You see, the evaluate function can run a JavaScript function in the context of the web page and bring results back to the playwright environment. It can also take something that exist on the playwright environment and send it to the browser context.\nFor example, if you wanted to get the value of a document\u0026rsquo;s href that is running under the browser context loaded into a playwright context, you would do so like this.\n1 2 // After evaluation, href will have the same value as document.location.href const href = await page.evaluate(() =\u0026gt; document.location.href); For more information see the playwright docs on execution context.\nThe evaluate method can take a string or a function as an optional argument. I can use file system to read axe.min.js into memory as a string, then pass that as an argument on the evaluate method. This is how I can get axe.min.js to be included.\n1 2 const file: string = fs.readFileSync(require.resolve(\u0026#39;axe-core/axe.min.js\u0026#39;), \u0026#39;utf8\u0026#39;); await page.evaluate((minifiedAxe: string) =\u0026gt; window.eval(minifiedAxe), file); Now comes the next trick, and that is that I need to run axe under the context of the browser, see #1638. That means we have to use the evaluate method again. The problem here is that when the test are executed, axe will run under the playwright context, and will not be available under the browser context. The solution to this problem is to extend the window interface to include axe, see How to declare a new property on the Window object with Typescript for more details.\nI\u0026rsquo;ll use the following code to include axe under the window interface.\n1 2 3 4 5 declare global { interface Window { axe: AxePlugin } } Putting everything we have talked about so far together yields the following test.\n1 2 3 4 5 6 7 8 9 10 11 describe(\u0026#39;loading google.com successfully\u0026#39;, function () { context(\u0026#39;accesibility evaluation\u0026#39;, function () { it(\u0026#39;should pass accessibility test\u0026#39;, async () =\u0026gt; { await page.goto(\u0026#39;https://www.google.com\u0026#39;); const file: string = fs.readFileSync(require.resolve(\u0026#39;axe-core/axe.min.js\u0026#39;), \u0026#39;utf8\u0026#39;); await page.evaluate((minifiedAxe: string) =\u0026gt; window.eval(minifiedAxe), file); const evaluationResult: AxeResults = await page.evaluate(() =\u0026gt; window.axe.run(window.document)) expect(evaluationResult.violations).to.be.empty; }); }); }); Let\u0026rsquo;s review what the test above is doing.\nPlaywright\u0026rsquo;s page object is used to nagivate to google.com. Node\u0026rsquo;s file system is used to load axe.min.js. axe.min.js is injected onto the browser context using evaluate method. axe.run is executed by providing it with document context. Assert that no a11y violations were found on google.com. The a11y.ts file should now look like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import { Browser, Page } from \u0026#39;playwright/types/types\u0026#39;; import { chromium } from \u0026#39;playwright\u0026#39;; import { describe } from \u0026#39;mocha\u0026#39;; import * as fs from \u0026#39;fs\u0026#39;; import { AxePlugin, AxeResults } from \u0026#39;axe-core\u0026#39;; import { expect } from \u0026#39;chai\u0026#39;; declare global { interface Window { axe: AxePlugin } } let browser: Browser; before(async () =\u0026gt; { browser = await chromium.launch(); }); after(async () =\u0026gt; { await browser.close(); }); let page: Page; beforeEach(async () =\u0026gt; { page = await browser.newPage(); }); afterEach(async () =\u0026gt; { await page.close(); }); describe(\u0026#39;loading google.com successfully\u0026#39;, function () { context(\u0026#39;accesibility evaluation\u0026#39;, function () { it(\u0026#39;should pass accessibility test\u0026#39;, async () =\u0026gt; { await page.goto(\u0026#39;https://www.google.com\u0026#39;); const file: string = fs.readFileSync(require.resolve(\u0026#39;axe-core/axe.min.js\u0026#39;), \u0026#39;utf8\u0026#39;); await page.evaluate((minifiedAxe: string) =\u0026gt; window.eval(minifiedAxe), file); const evaluationResult: AxeResults = await page.evaluate(() =\u0026gt; window.axe.run(window.document)) expect(evaluationResult.violations).to.be.empty; }); }); }); As always, to execute the a11y test, plus the test that already existed on the project run \u0026rsquo;npm test\u0026rsquo;. If you run that command you will notice the following output.\n1 2 3 4 5 6 $ npm test \u0026gt; e2eplaywright@1.0.0 test C:\\Users\\Yunier\\Documents\\playwright-demo \u0026gt; mocha -r ts-node/register \u0026#39;test/*.ts\u0026#39; --recursive --reporter mocha-junit-reporter --timeout 60000 --exit npm ERR! Test failed. See above for more details. The test failed, I can use console.log to spit out the violation. Running the test again yields the following result.s\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;id\u0026#34;: \u0026#34;aria-required-attr\u0026#34;, \u0026#34;impact\u0026#34;: \u0026#34;critical\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;cat.aria\u0026#34;, \u0026#34;wcag2a\u0026#34;, \u0026#34;wcag412\u0026#34;], \u0026#34;description\u0026#34;: \u0026#34;Ensures elements with ARIA roles have all required ARIA attributes\u0026#34;, \u0026#34;help\u0026#34;: \u0026#34;Required ARIA attributes must be provided\u0026#34;, \u0026#34;helpUrl\u0026#34;: \u0026#34;https://dequeuniversity.com/rules/axe/4.1/aria-required-attr?application=axeAPI\u0026#34;, \u0026#34;nodes\u0026#34;: [ { \u0026#34;any\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;aria-required-attr\u0026#34;, \u0026#34;data\u0026#34;: [\u0026#34;aria-expanded\u0026#34;], \u0026#34;relatedNodes\u0026#34;: [], \u0026#34;impact\u0026#34;: \u0026#34;critical\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Required ARIA attribute not present: aria-expanded\u0026#34; } ], \u0026#34;all\u0026#34;: [], \u0026#34;none\u0026#34;: [], \u0026#34;impact\u0026#34;: \u0026#34;critical\u0026#34;, \u0026#34;html\u0026#34;: \u0026#34;\u0026lt;input class=\\\u0026#34;gLFyf gsfi\\\u0026#34; jsaction=\\\u0026#34;paste:puy29d;\\\u0026#34; maxlength=\\\u0026#34;2048\\\u0026#34; name=\\\u0026#34;q\\\u0026#34; type=\\\u0026#34;text\\\u0026#34; aria-autocomplete=\\\u0026#34;both\\\u0026#34; aria-haspopup=\\\u0026#34;false\\\u0026#34; autocapitalize=\\\u0026#34;off\\\u0026#34; autocomplete=\\\u0026#34;off\\\u0026#34; autocorrect=\\\u0026#34;off\\\u0026#34; autofocus=\\\u0026#34;\\\u0026#34; role=\\\u0026#34;combobox\\\u0026#34; spellcheck=\\\u0026#34;false\\\u0026#34; title=\\\u0026#34;Search\\\u0026#34; value=\\\u0026#34;\\\u0026#34; aria-label=\\\u0026#34;Search\\\u0026#34; data-ved=\\\u0026#34;0ahUKEwiV8IqSubHvAhXHjFkKHaP-DIQQ39UDCAY\\\u0026#34;\u0026gt;\u0026#34;,\u0026#34;target\u0026#34;: [\u0026#34;.gLFyf\u0026#34;], \u0026#34;failureSummary\u0026#34;: \u0026#34;Fix any of the following:\\n Required ARIA attribute not present: aria-expanded\u0026#34; } ] } Awesome. I can now do a11y testing while using playwright. You should know, that the example above is the most basic a11y test you can create. Axe is a very powerful library that offers many configurations. You can take the code above and expand it by passing different configurations to the run method. Or you can take a different approach, that is to leverage an existing library that does most of the heavy lifting for you, a library like axe-playwright.\nCredits:\nHey, accessibility is a lot of work! Accessibility Testing[Question] #1638 Writing unit tests in TypeScript How to declare a new property on the Window object with Typescript ","permalink":"http://localhost:1313/post/2021/accessibility-testing-with-playwright/","summary":"\u003cp\u003eI don\u0026rsquo;t believe I\u0026rsquo;ve mention this before here, but I am a huge fan of \u003ca href=\"https://hey.com/\"\u003eHey\u003c/a\u003e. By far the \u003cstrong\u003ebest\u003c/strong\u003e email service I have ever used. What makes Hey even cooler is the team behind Hey sharing they engineering approach to different problems. Be that through various \u003ca href=\"https://twitter.com/dhh/status/1275901955995385856\"\u003etweets\u003c/a\u003e or blog post like \u003ca href=\"https://acloudguru.com/blog/engineering/scaling-the-hottest-app-in-tech-on-aws-and-kubernetes\"\u003eScaling the hottest app in tech on AWS and Kubernetes\u003c/a\u003e which outline how they use k8s. Recently, they shared how to tackle ay11 under \u003ca href=\"https://world.hey.com/michael/hey-accessibility-is-a-lot-of-work-785ec5cf\"\u003ehey accessibility is a lot of work\u003c/a\u003e. One thing that stood out was to me was their usage of \u003ca href=\"https://github.com/dequelabs/axe-core\"\u003eaxe-core\u003c/a\u003e. Axe-core is an accessibility engine for automated Web UI testing. Which reminded me of \u003ca href=\"https://playwright.dev/\"\u003eplaywright\u003c/a\u003e, so I started to wonder if the two could be combined, turns out they can be. Let\u0026rsquo;s explore how to do that.\u003c/p\u003e","title":"Accessibility Testing in Playwright"},{"content":"A few weeks ago I was looking for an end-to-end testing framework. An alternative to Selenium, and all the other end-to-end frameworks. I came across a project called Playwright. Playwright is a new end-to-end framewrok created and maintained by Microsoft, it allows you to test web applications on different browsers. Some the major feature it provides are as follows.\nPlaywright has full API coverage for all modern browsers, including Google Chrome and Microsoft Edge (with Chromium), Apple Safari (with WebKit) and Mozilla Firefox. Supports multiple languages like Node.js, Python, c# and Java. First-party Docker image and GitHub Actions to deploy tests to your preferred CI/CD provider. Use device emulation to test your responsive web apps in mobile web browsers. Provides APIs to monitor and modify network traffic, both HTTP and HTTPS. Any requests that page does, including XHRs and fetch requests, can be tracked, modified and handled. While those feature are all great and useful, they don\u0026rsquo;t measure up to what I consider to be the best feature of Playwright. That is being able to create and execute test as easily as unit tests. You can also leverage tools like qa wolf and headless-recorder, these tools record any action you take on the browser, those actions are then converted into Playwright scripts.\nFor today\u0026rsquo;s post, I am going to build a small playwright project, I will use this project to demonstrate how to create some basic tests using playwright. To follow along you will need to have Node.js installed, version 10.17 or greater.\nOpen your favorite cli tool, windows terminal in my case. Run the following npm command to install playwright\n1 npm i -D playwright With playwright now installed, create a new file on the same directory you opened your terminal. Call the file google.ts, copy and paste the following code.\n1 2 3 4 5 6 7 8 9 const {chromium} = require(\u0026#39;playwright\u0026#39;); (async () =\u0026gt; { const browser = await chromium.launch(); const page = await browser.newPage(); await page.goto(\u0026#39;https://www.google.com\u0026#39;); await page.screenshot({path: \u0026#39;google.png\u0026#39;}); await browser.close(); })(); The first line of code imports the playwright library, then an asynchronous context is created, this is where playwright is executed. The second line of code launches an instace of chrome, from that instance a new page is created, a go to url is provided so that it can be visited, once the page is loaded, a screenshot is saved to the directory playwright is running under. Finally, the browser instance is disposed.\nTo execute the code, run the following command on the cli.\n1 node google.ts This is the most basic example on how to use playwright. Time to take things to the next level. Clean the working directory, delete all files. Create a new node.js project by running the following command.\n1 npm init Provided the required information such as project name, author, description and so on. A package.json fille will be created with the information you provided.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;name\u0026#34;: \u0026#34;e2eplaywright\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Sample playwright repository\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;mocha\u0026#34; }, \u0026#34;keywords\u0026#34;: [ \u0026#34;playwright\u0026#34; ], \u0026#34;author\u0026#34;: \u0026#34;yunier\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34; } Time to establish the folder structure. Whenever I work with a Node.js library, I like to follow the project structure outlined in this stack overflow post.\nFow now I will only add the test folder using the following command.\n1 mkdir test It is time to install a few additional node packages. Since I wiped the directory clean, I will need to reinstall playwright, I will also need a testing library, Playwright supports Mocha, Jest and Ava. For this repo I will use Mocha. I will also need an assertion library, you can use whichever library you want on your project, chaijs is my preferred framework. I work a lot with postman for API testing, postman uses chaijs to do assertions so I\u0026rsquo;m most familiar with chaijs and I know the syntax by heart. Other packages I will need are ts-node, typescript and mocha-junit-reporter.\nRun the following command to install the required packages\n1 npm install playwright ts-node typescript eslint mocha chai @types/chai @types/faker @types/mocha mocha-junit-reporter Your package.json should look somewhat like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \u0026#34;name\u0026#34;: \u0026#34;e2eplaywright\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Sample playwright repository\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;mocha\u0026#34; }, \u0026#34;keywords\u0026#34;: [ \u0026#34;playwright\u0026#34; ], \u0026#34;author\u0026#34;: \u0026#34;yunier\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;@types/chai\u0026#34;: \u0026#34;^4.2.15\u0026#34;, \u0026#34;@types/faker\u0026#34;: \u0026#34;^5.1.7\u0026#34;, \u0026#34;@types/mocha\u0026#34;: \u0026#34;^8.2.1\u0026#34;, \u0026#34;chai\u0026#34;: \u0026#34;^4.3.0\u0026#34;, \u0026#34;eslint\u0026#34;: \u0026#34;^7.21.0\u0026#34;, \u0026#34;mocha\u0026#34;: \u0026#34;^8.3.0\u0026#34;, \u0026#34;mocha-junit-reporter\u0026#34;: \u0026#34;^2.0.0\u0026#34;, \u0026#34;playwright\u0026#34;: \u0026#34;^1.9.1\u0026#34;, \u0026#34;ts-node\u0026#34;: \u0026#34;^9.1.1\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^4.2.2\u0026#34; } } I do want to make a slight modification. I will repalce the scripts field with the following script commands to let npm know how to handle the command \u0026ldquo;npm test\u0026rdquo; and \u0026ldquo;npm lint\u0026rdquo;.\n1 2 3 4 5 6 { \u0026#34;scripts\u0026#34; : { \u0026#34;lint\u0026#34; : \u0026#34;eslint .\u0026#34;, \u0026#34;test\u0026#34; : \u0026#34;mocha -r ts-node/register \u0026#39;test/*.ts\u0026#39; --recursive --reporter mocha-junit-reporter --timeout 60000 --exit\u0026#34; } } Your package.json should now look like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;name\u0026#34;: \u0026#34;e2eplaywright\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Sample playwright repository\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;lint\u0026#34;: \u0026#34;eslint .\u0026#34;, \u0026#34;test\u0026#34;: \u0026#34;mocha -r ts-node/register \u0026#39;test/*.ts\u0026#39; --recursive --reporter mocha-junit-reporter --timeout 60000 --exit\u0026#34; }, \u0026#34;keywords\u0026#34;: [ \u0026#34;playwright\u0026#34; ], \u0026#34;author\u0026#34;: \u0026#34;yunier\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;@types/chai\u0026#34;: \u0026#34;^4.2.15\u0026#34;, \u0026#34;@types/faker\u0026#34;: \u0026#34;^5.1.7\u0026#34;, \u0026#34;@types/mocha\u0026#34;: \u0026#34;^8.2.1\u0026#34;, \u0026#34;chai\u0026#34;: \u0026#34;^4.3.0\u0026#34;, \u0026#34;eslint\u0026#34;: \u0026#34;^7.21.0\u0026#34;, \u0026#34;mocha\u0026#34;: \u0026#34;^8.3.0\u0026#34;, \u0026#34;mocha-junit-reporter\u0026#34;: \u0026#34;^2.0.0\u0026#34;, \u0026#34;playwright\u0026#34;: \u0026#34;^1.9.1\u0026#34;, \u0026#34;ts-node\u0026#34;: \u0026#34;^9.1.1\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^4.2.2\u0026#34; } } Perfect, our project is ready, I can start writing some test. I\u0026rsquo;m going to start small by adding two test, one that goes to google.com and confirms the page title is \u0026lsquo;Google\u0026rsquo; and another that assert that when google.com is loaded succesfully, a 200 OK Http status should be the response status code.\nAdd a new file under the test folder, call it google.ts and open it up on your favorite editor, I recommend vscode. Copy and paste the following code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import { Browser, Page } from \u0026#39;playwright/types/types\u0026#39;; import { expect } from \u0026#39;chai\u0026#39;; import { chromium } from \u0026#39;playwright\u0026#39;; import { describe } from \u0026#39;mocha\u0026#39;; let browser: Browser; before(async () =\u0026gt; { browser = await chromium.launch(); }); after(async () =\u0026gt; { await browser.close(); }); let page: Page; beforeEach(async () =\u0026gt; { page = await browser.newPage(); }); afterEach(async () =\u0026gt; { await page.close(); }); describe(\u0026#39;loading google.com successfully\u0026#39;, function () { context(\u0026#39;page title\u0026#39;, function () { it(\u0026#39;Should be equal to Google\u0026#39;, async () =\u0026gt; { await page.goto(\u0026#39;https://www.google.com\u0026#39;); const pageTitle = await page.title(); expect(pageTitle).to.equal(\u0026#39;Google\u0026#39;) }); }); }); describe(\u0026#39;loading google.com successfully\u0026#39;, function () { context(\u0026#39;response status code\u0026#39;, function () { it(\u0026#39;Should be 200 OK\u0026#39;, async () =\u0026gt; { const response = await page.goto(\u0026#39;https://www.google.com\u0026#39;); expect(response?.status()).to.equal(200); }); }); }); The code above imports the required node packages, then playwright\u0026rsquo;s browser and page objects are configured to run before and after each test, then finally at the end, you have the two test. Like I said before one test will assert the title of the page and the other test will aserrt the response code. To execute these tests run the following command.\n1 npm test The command should output the following result.\n1 2 \u0026gt; e2eplaywright@1.0.0 test C:\\Users\\Yunier\\Documents\\playwright \u0026gt; mocha -r ts-node/register \u0026#39;test/*.ts\u0026#39; --recursive --reporter mocha-junit-reporter --timeout 60000 --exit Playwright run headless by default, if you want to see it execute the tests on the browser, then set the headless property to false on the launch() method.\n1 2 3 4 let browser: Browser; before(async () =\u0026gt; { browser = await chromium.launch({headless: false}); }); For a complete list of available launch options, see the playwright api docs.\nAwesome. A foundation for building end-to-end test with playwright has been established. More test can now be added. If you are building tests that deal with data input, then I suggest installing faker.js, it is another node libary used to create fake data such as people, address, number, text and so on. Learn more about playwright by reading the docs.\n","permalink":"http://localhost:1313/post/2021/testing-webapps-with-playwright/","summary":"\u003cp\u003eA few weeks ago I was looking for an end-to-end testing framework. An alternative to \u003ca href=\"https://www.selenium.dev/\"\u003eSelenium\u003c/a\u003e, and all the other end-to-end frameworks. I came across a project called \u003ca href=\"https://playwright.dev/\"\u003ePlaywright\u003c/a\u003e. Playwright is a new end-to-end framewrok created and maintained by Microsoft, it allows you to test web applications on different browsers. Some the major feature it provides are as follows.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ePlaywright has full API coverage for all modern browsers, including Google Chrome and Microsoft Edge (with Chromium), Apple Safari (with WebKit) and Mozilla Firefox.\u003c/li\u003e\n\u003cli\u003eSupports multiple languages like Node.js, Python, c# and Java.\u003c/li\u003e\n\u003cli\u003eFirst-party Docker image and GitHub Actions to deploy tests to your preferred CI/CD provider.\u003c/li\u003e\n\u003cli\u003eUse device emulation to test your responsive web apps in mobile web browsers.\u003c/li\u003e\n\u003cli\u003eProvides APIs to monitor and modify network traffic, both HTTP and HTTPS. Any requests that page does, including XHRs and fetch requests, can be tracked, modified and handled.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWhile those feature are all great and useful, they don\u0026rsquo;t measure up to what I consider to be the best feature of Playwright. That is being able to create and execute test as easily as unit tests. You can also leverage tools like \u003ca href=\"https://www.qawolf.com/\"\u003eqa wolf\u003c/a\u003e and \u003ca href=\"https://github.com/checkly/headless-recorder\"\u003eheadless-recorder\u003c/a\u003e, these tools record any action you take on the browser, those actions are then converted into Playwright scripts.\u003c/p\u003e","title":"Testing Web Apps With Playwright"},{"content":"I am currently building a JSON:API driven API on .NET 5, the project is called Chinook after the Sqlite Chinook project. The API is mature enough for me to introduce filtering via the Filter query parameter used in JSON:API.\nI would like to support dynamic filtering, I want to avoid creating nested if-else/switch statements that check if a given input is part of the filter criteria, and if it is then it gets appended to a filtering clause. For example, take the following API request, it uses the OData filter syntax.\n1 https://api.example.com/users?filter=name eq \u0026#39;James\u0026#39; Some developers might choose to handle that request by implement filtering like this,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class UserDataHandler { public async Task\u0026lt;IEnumerable\u0026lt;Users\u0026gt;\u0026gt; Handle(GetUsersResourceCollectionCommand request, CancellationToken cancellationToken) { var uriQueryString = request.QueryString; var query = await _chinookDbContext.Users .ToListAsync(cancellationToken); if(uriQueryString.Contains(\u0026#34;name\u0026#34;)) { var parsedUri = Microsoft.AspNetCore.WebUtilities.QueryHelpers.ParseQuery(uriQueryString); var value = parsedUri[\u0026#34;name\u0026#34;]; query.where(u =\u0026gt;u.Where(u =\u0026gt; u.FirstName.Contains(value) || u.LastName.Contains(value))) } } } Each additional filter on the API is another if-else/switch statement. We should avoid this type of code, simply because .NET gives us the power to create better solutions through expressions trees. Expression trees represent code in a tree-like data structure, where each node is an expression, for example, a method call or a binary operation such as x \u0026lt; y. You can compile and run code represented by expression trees. This enables dynamic modification of executable code, the execution of LINQ queries in various databases, and the creation of dynamic queries.\nThe first step in creating an expression tree is being able to parse a sequence of characters into a stream of tokens, this process is better known as lexical analysis or a lexer. Sometimes it is also refer to as a tokenizer. The tokens in this process are nothing more than a key value pair, in our example above, \u0026lsquo;James\u0026rsquo; would be a token of type string, with a corresponding value of James.\nThe question now is, how to parse strings in C#, something like 2 * 3 + 8. For that, I will create a parser based on a regex parser created by Jack Vanlightly. The source code to his parser can be found on his GitHub page.\nTo build this parser I will have to define the tokens it supports. For our expression, 2 * 3 + 8 I will only need four token types, an integer, summation, multiplication and a sequence terminator type. The following Enum can be used to represent those tokens.\n1 2 3 4 5 6 7 public enum TokenType { Sum = 0, Number = 1, Multiplication = 2, SequenceTerminator = 4 } Now all I need to do is combine this enum with the tokenizer class created by Jack Vanlightly and I end up with the following code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class Program { static void Main(string[] args) { var testExpression = \u0026#34;2*3+8\u0026#34;; var precedenceBasedRegexTokenizer = new PrecedenceBasedRegexTokenizer(); var tokens = precedenceBasedRegexTokenizer.Tokenize(testExpression); foreach (var token in tokens) { Console.WriteLine($\u0026#34;Token type {token.TokenType} has value {token.Value}\u0026#34;); } } } public enum TokenType { Sum = 0, Number = 1, Multiplication = 2, SequenceTerminator = 4 } public class TokenDefinition { private Regex _regex; private readonly TokenType _returnsToken; private readonly int _precedence; public TokenDefinition(TokenType returnsToken, string regexPattern, int precedence) { _regex = new Regex(regexPattern, RegexOptions.IgnoreCase | RegexOptions.Compiled); _returnsToken = returnsToken; _precedence = precedence; } public IEnumerable\u0026lt;TokenMatch\u0026gt; FindMatches(string inputString) { var matches = _regex.Matches(inputString); for (int i = 0; i \u0026lt; matches.Count; i++) { yield return new TokenMatch() { StartIndex = matches[i].Index, EndIndex = matches[i].Index + matches[i].Length, TokenType = _returnsToken, Value = matches[i].Value, Precedence = _precedence }; } } } public class TokenMatch { public TokenType TokenType { get; set; } public string Value { get; set; } public int StartIndex { get; set; } public int EndIndex { get; set; } public int Precedence { get; set; } } public class Token { public Token(TokenType tokenType) { TokenType = tokenType; Value = string.Empty; } public Token(TokenType tokenType, string value) { TokenType = tokenType; Value = value; } public TokenType TokenType { get; set; } public string Value { get; set; } public Token Clone() { return new Token(TokenType, Value); } } public class PrecedenceBasedRegexTokenizer { private List\u0026lt;TokenDefinition\u0026gt; _tokenDefinitions; public PrecedenceBasedRegexTokenizer() { _tokenDefinitions = new List\u0026lt;TokenDefinition\u0026gt; { new TokenDefinition(TokenType.Multiplication, @\u0026#34;[*]\u0026#34;, 1), new TokenDefinition(TokenType.Sum, @\u0026#34;[+]\u0026#34;, 1), new TokenDefinition(TokenType.Number, \u0026#34;\\\\d+\u0026#34;, 2) }; } public IEnumerable\u0026lt;Token\u0026gt; Tokenize(string lqlText) { var tokenMatches = FindTokenMatches(lqlText); var groupedByIndex = tokenMatches.GroupBy(x =\u0026gt; x.StartIndex) .OrderBy(x =\u0026gt; x.Key) .ToList(); TokenMatch lastMatch = null; for (int i = 0; i \u0026lt; groupedByIndex.Count; i++) { var bestMatch = groupedByIndex[i].OrderBy(x =\u0026gt; x.Precedence).First(); if (lastMatch != null \u0026amp;\u0026amp; bestMatch.StartIndex \u0026lt; lastMatch.EndIndex) continue; yield return new Token(bestMatch.TokenType, bestMatch.Value); lastMatch = bestMatch; } yield return new Token(TokenType.SequenceTerminator); } private List\u0026lt;TokenMatch\u0026gt; FindTokenMatches(string lqlText) { var tokenMatches = new List\u0026lt;TokenMatch\u0026gt;(); foreach (var tokenDefinition in _tokenDefinitions) tokenMatches.AddRange(tokenDefinition.FindMatches(lqlText).ToList()); return tokenMatches; } } Running the code above yields the following result.\n1 2 3 4 5 6 Token type Number has value 2 Token type Multiplication has value * Token type Number has value 3 Token type Sum has value + Token type Number has value 8 Token type SequenceTerminator has value Great! I am able to parse an input string into tokens, the time has come to build an expression tree from these token. If you break down the expression 2 * 3 + 8 into a tree it would like this.\nThe above structure is what we are aiming to build. Once the expression tree has been created I can compile it and execute it. If everything is done correctly, we should get 14 as our result. It is time to use perhaps one of the least used design patterns, yet one of the most powerful patterns, I\u0026rsquo;m talking about the Visitor Design pattern. The visitor pattern or visitation pattern will be used to visit each node on our tree, the visitor will traverse the tree, in the end it will output our compile expression which can then be invoked to get our our result.\nThe visitation pattern is heavily used in frameworks like EF Core. This video by Shay Rojansky explains how EF Core uses expression tree to generate SQL statements.\nThe first step in implementing the visitor pattern is to define our Visitor interface and what is often called the \u0026ldquo;element\u0026rdquo; interface. The Element interface declares a method for “accepting” visitors. This method should have one parameter declared with the type of the visitor interface.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // The Visitor interface declares a set of visiting methods that can take concrete elements of an object structure as // arguments. public interface IExpressionTreeVisitor { public IExpression LeftChildNode { get; set; } public IExpression RightChildNode { get; set; } public IExpression ParentNode { get; set; } void VisitLeafNode(Number integer); void VisitParentNode(Multiplication addition); void VisitParentNode(Addition multiplication); } // The accepting interface, declares a method for accepting visitors. interface IExpression { void Accept(IExpressionTreeVisitor visitor); } Additionally, I need to create a class that represents each the nodes on the tree.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 public class Number : IExpression { public double Value { get; } public Number(double value) { Value = value; } public void Accept(IExpressionTreeVisitor visitor) { visitor.VisitLeafNode(this); if (visitor.ParentNode == null) { return; } visitor.ParentNode.Accept(visitor); } } public class Addition : IExpression { public Addition() { } public void Accept(IExpressionTreeVisitor visitor) { visitor.VisitParentNode(this); } } public class Multiplication : IExpression { public Multiplication() { } public void Accept(IExpressionTreeVisitor visitor) { visitor.VisitParentNode(this); } } Finally, here is the implementation of the IExpressionTreeVisitor interface.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 public class ExpressionTreeVisitor : IExpressionTreeVisitor { public IExpression LeftChildNode { get; set; } public IExpression RightChildNode { get ; set ; } public IExpression ParentNode { get; set; } private Expression _leftChildExpressionValue; private Expression _rightChildExpressionValue; public void VisitLeafNode(Number number) { if (_leftChildExpressionValue == null) { LeftChildNode = number; _leftChildExpressionValue = Expression.Constant(number.Value); } else { RightChildNode = number; _rightChildExpressionValue = Expression.Constant(number.Value); } } public void VisitParentNode(Multiplication multiplication) { if (ParentNode == null) { ParentNode = multiplication; return; } var expression = Expression.Multiply(_leftChildExpressionValue, _rightChildExpressionValue); ParentNode = null; _leftChildExpressionValue = expression; } public void VisitParentNode(Addition addition) { if (ParentNode == null) { ParentNode = addition; return; } var expression = Expression.Add(_leftChildExpressionValue, _rightChildExpressionValue); ParentNode = null; _leftChildExpressionValue = expression; } public Expression\u0026lt;Func\u0026lt;double\u0026gt;\u0026gt; GetCompletedExpression() { return Expression.Lambda\u0026lt;Func\u0026lt;double\u0026gt;\u0026gt;(_leftChildExpressionValue); } } and here is the complete Program.cs class utilizing the expression visitor class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 static void Main(string[] args) { var testExpression = \u0026#34;2*3+8\u0026#34;; var precedenceBasedRegexTokenizer = new PrecedenceBasedRegexTokenizer(); var tokens = precedenceBasedRegexTokenizer.Tokenize(testExpression); var listOfExpressions = new List\u0026lt;IExpression\u0026gt;(); foreach (var token in tokens) { Console.WriteLine($\u0026#34;Token type {token.TokenType} has value {token.Value}\u0026#34;); switch (token.TokenType) { case TokenType.Addition: var sumNode = new Addition(); listOfExpressions.Add(sumNode); break; case TokenType.NumberLiteral: var numberNode = new Number(Convert.ToDouble(token.Value)); listOfExpressions.Add(numberNode); break; case TokenType.Multiplication: var multiplicationNode = new Multiplication(); listOfExpressions.Add(multiplicationNode); break; case TokenType.SequenceTerminator: break; default: break; } } var visitor = new ExpressionTreeVisitor(); foreach (var item in listOfExpressions) { item.Accept(visitor); } var completedExpression = visitor.GetCompletedExpression(); var result = completedExpression.Compile(); Console.WriteLine($\u0026#34;Expression {testExpression} is equal to {result()}\u0026#34;); } If I run the program I get the following console output.\n1 2 3 4 5 6 7 Token type NumberLiteral has value 2 Token type Multiplication has value * Token type NumberLiteral has value 3 Token type Addition has value + Token type NumberLiteral has value 8 Token type SequenceTerminator has no value Expression 2*3+8 is equal to 14 Awesome, 14 was the final result obtained after executing the expression tree. I now have a foundation for creating dynamic filtering on JSON:API. By the way, a lot of the work done here can be bypass by using some of the awesome projects found in the .NET community. When it comes to parsing you have two great options, one is superpower and another one is pidgin. Personally, I use superpower, it is what serilog uses to do parsing, plus I am a huge fan of any work done by Nicholas Blumhardt. For my JSON:API project I will use Pidgin, I want to understand how Pidgin works, and understand all of its capabilities and how it differs from Superpower.\nThe complete code above can be found here.\nCredits:\nhttps://ruslanspivak.com/lsbasi-part1/ https://codereview.stackexchange.com/questions/108001/implementation-of-the-visitor-pattern https://jack-vanlightly.com/blog/2016/2/24/a-more-efficient-regex-tokenizer https://www.youtube.com/watch?v=t4dYy6P3JuA https://www.youtube.com/watch?v=klHyc9HQnNQ https://thesharperdev.com/c-design-patterns-the-visitor-pattern/ https://refactoring.guru/design-patterns/visitor ","permalink":"http://localhost:1313/post/2021/parsing-in-csharp/","summary":"\u003cp\u003eI am currently building a \u003ca href=\"https://jsonapi.org/\"\u003eJSON:API\u003c/a\u003e driven API on \u003ca href=\"https://dotnet.microsoft.com/download/dotnet/5.0\"\u003e.NET 5\u003c/a\u003e, the project is called \u003ca href=\"https://github.com/circleupx/Chinook\"\u003eChinook\u003c/a\u003e after the Sqlite Chinook project. The API is mature enough for me to introduce \u003ca href=\"https://jsonapi.org/format/#fetching-filtering\"\u003efiltering\u003c/a\u003e via the \u003ca href=\"https://jsonapi.org/recommendations/#filtering\"\u003eFilter\u003c/a\u003e query parameter used in JSON:API.\u003c/p\u003e\n\u003cp\u003eI would like to support dynamic filtering, I want to avoid creating nested if-else/switch statements that check if a given input is part of the filter criteria, and if it is then it gets appended to a filtering clause. For example, take the following API request, it uses the \u003ca href=\"https://docs.oasis-open.org/odata/odata/v4.01/odata-v4.01-part2-url-conventions.html#_Toc31361038\"\u003eOData\u003c/a\u003e filter syntax.\u003c/p\u003e","title":"Parsing in C#"},{"content":"When I first started to learn about GraphQL I was somewhat surprise to learn that the GraphQL specification did not provide any guidance or spoke of any methods to handle asynchronous request. By asynchronous request, I mean request that cannot be completed within your normal request-response context.\nFor example, take an API that aggregates orders by combining various types of filters, the API may allow you to filter by only orders that are greater than $100.00, or orders placed in certain date range, or orders that have a particular product and so on. Depending on the amount of data and filters used, the query to get the data may take a couple of minutes, maybe even hours. The question now becomes how to best handle long-running request in GraphQL.\nIf the API were RESTful then one solution to this problem might be to allow the client to poll the API. The RESTful API may expose a \u0026ldquo;jobs\u0026rdquo; resource, remember anything in rest can be a resource, even concepts such as a long running job. The jobs resource would then accept a job request from the client, the API would respond to the client with a 202 Status code instead of a 200 OK. The API server would then begin processing the client\u0026rsquo;s request, and the client can then query the status of the job until it receives a 303 See Other HTTP respond from the API server with a location header indicating to the client where it can retrieve the output of the job.\nIn a GraphQL API there are two solutions to this problem. The first solution is the same solution used on our RESTful API example, the client submits a long running request to the API then polls the status of the request on a set interval. The trick with GraphQL is that GraphQL is a funnel API, every request in GraphQL goes through the same endpoint, hence the name funnel API, as GraphQL does not have the concept of using individual URIs to represent resources, therefore, a GraphQL API that whishes to support long-running request would need to expose an endpoint that handle long-running request.\nThe methodology described above is also mention on the book, Production Ready GraphQL by Marc-André Giroux. I highly recommend this book to anyone wanting to learn about GraphQL.\nThe second solution would be to use GraphQL subscriptions. GraphQL subscriptions are awesome, they can leverage the power of Websocket. The client application and the GraphQL API can switch protocols from HTTP to Websocket anytime there is need to process a long-running request, thanks to the full-duplex nature of Websocket, the GraphQL API can use the existing connection opened by the client to send back the output of the job request to the client.\nShopify\u0026rsquo;s Admin API is a good example of a GraphQL API that can query a large collection of data using a polling mechanism.\nIn GraphQL we have two options to help us deal with long-running request, the question now is when to use each option. At first, I was under the impression that Subscriptions should only be used from here on out, that polling was a dead mechanism, after all why would you want to create a specific endpoint that only deals with a certain type of request.\nGood q! I think subscriptions can be an alternative to running one big query if you want to stream data back, but it can\u0026#39;t necessarily replace the need to run a \u0026quot;bulk\u0026quot; query / really big query string that cant be executed within a request-response context.\n\u0026mdash; Marc-André Giroux (@__xuorig__) December 27, 2020 As noted by Marc-André Giroux on the tweet above, it all depends on the request-response context. If our use case is one where the client may not get a response for a couple of minutes, maybe even an hour, then the best solution is to go with polling, after all no client would want to keep a WebSocket connection open to the server for that long. On the other hand, if our use case involves request-respond context that only last a few seconds, then I recommend using a GraphQL Subscription.\n","permalink":"http://localhost:1313/post/2021/asynchronous-request-in-graphql/","summary":"\u003cp\u003eWhen I first started to learn about GraphQL I was somewhat surprise to learn that the \u003ca href=\"https://spec.graphql.org/June2018/\"\u003eGraphQL specification\u003c/a\u003e did not provide any guidance or spoke of any methods to handle asynchronous request. By asynchronous request, I mean request that cannot be completed within your normal request-response context.\u003c/p\u003e\n\u003cp\u003eFor example, take an API that aggregates orders by combining various types of filters, the API may allow you to filter by only orders that are greater than $100.00, or orders placed in certain date range, or orders that have a particular product and so on. Depending on the amount of data and filters used, the query to get the data may take a couple of minutes, maybe even hours. The question now becomes how to best handle long-running request in GraphQL.\u003c/p\u003e","title":"Asynchronous Request In GraphQL"},{"content":"My Chinook JSON:API project is now in a good enough state that I feel comfortable hosting it on a live server. Here is the base url, https://chinook-jsonapi.herokuapp.com/, I highly recommend using some kind of JSON viewer if you want to interact with the API. If you are on a Chromium base browser then I recommend using JSON Viewer.\nRemember, the API does not support filtering, pagination, sorting or include resolvers and it only supports READ operations. I\u0026rsquo;m hoping to add filtering soon but I first want to dedicate a blog post or two on building dynamic LINQ queries using expression trees. That should be fun!\n","permalink":"http://localhost:1313/post/2021/chinook-project-hosted-on-heroku/","summary":"\u003cp\u003eMy \u003ca href=\"https://github.com/circleupx/Chinook\"\u003eChinook JSON:API project\u003c/a\u003e is now in a good enough state that I feel comfortable hosting it on a live server. Here is the base url, \u003ca href=\"https://chinook-jsonapi.herokuapp.com/\"\u003ehttps://chinook-jsonapi.herokuapp.com/\u003c/a\u003e, I highly recommend using some kind of JSON viewer if you want to interact with the API. If you are on a Chromium base browser then I recommend using \u003ca href=\"https://chrome.google.com/webstore/detail/json-viewer/gbmdgpbipfallnflgajpaliibnhdgobh\"\u003eJSON Viewer\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eRemember, the API does not support filtering, pagination, sorting or include resolvers and it only supports READ operations. I\u0026rsquo;m hoping to add filtering soon but I first want to dedicate a blog post or two on building dynamic LINQ queries using expression trees. That should be fun!\u003c/p\u003e","title":"Chinook Project Hosted On Heroku"},{"content":".NET Core 2.2 introduce a small feature known as Query Tags. It allows you to annotate queries generated by EF Core. This is super useful for debugging purposes, after all one of the main complains you often hear about EntityFramework is the over completed SQL statements it generates.\nI am currently working on a project called Chinook, it demonstrates how to build a JSON:API on .NET Core. The project uses EF Core to query a SQLite database. Here is an example of one of the LINQ queries used to get a collection of users.\n1 2 3 4 public async Task\u0026lt;IEnumerable\u0026lt;Customer\u0026gt;\u0026gt; Handle(GetCustomerResourceCollectionCommand request, CancellationToken cancellationToken) { return await _chinookDbContext.Customers.ToListAsync(cancellationToken); } Here is the SQL query generated by EF Core.\n1 2 SELECT \u0026#34;c\u0026#34;.\u0026#34;CustomerId\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Address\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;City\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Company\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Country\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Email\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Fax\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;FirstName\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;LastName\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Phone\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;PostalCode\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;State\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;SupportRepId\u0026#34; FROM \u0026#34;customers\u0026#34; AS \u0026#34;c\u0026#34; We can tag the original LINQ query with a message using the TagWith method.\n1 2 3 4 5 public async Task\u0026lt;IEnumerable\u0026lt;Customer\u0026gt;\u0026gt; Handle(GetCustomerResourceCollectionCommand request, CancellationToken cancellationToken) { var tagMessage = $\u0026#34;Calling from {nameof(GetCustomerResourceCollectionHandler)}\u0026#34;; return await _chinookDbContext.Customers.TagWith(tagMessage).ToListAsync(cancellationToken); } and here is the SQL query generated by this LINQ query, as you can see it included our tag message.\n1 2 3 -- Calling from GetCustomerResourceCollectionHandler SELECT \u0026#34;c\u0026#34;.\u0026#34;CustomerId\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Address\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;City\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Company\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Country\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Email\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Fax\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;FirstName\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;LastName\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Phone\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;PostalCode\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;State\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;SupportRepId\u0026#34; FROM \u0026#34;customers\u0026#34; AS \u0026#34;c\u0026#34; We can improve this code by recording the exact line number that generated the query. That can be accomplished by creating an extension method of IQueryable, like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public static class IQueryableTaggingExtensions { public static IQueryable\u0026lt;T\u0026gt; TagWithSource\u0026lt;T\u0026gt;(this IQueryable\u0026lt;T\u0026gt; queryable, [CallerLineNumber] int lineNumber = 0, [CallerFilePath] string filePath = \u0026#34;\u0026#34;, [CallerMemberName] string memberName = \u0026#34;\u0026#34;) { return queryable.TagWith($\u0026#34;{memberName} - {filePath}:{lineNumber}\u0026#34;); } public static IQueryable\u0026lt;T\u0026gt; TagWithSource\u0026lt;T\u0026gt;(this IQueryable\u0026lt;T\u0026gt; queryable, string tag, [CallerLineNumber] int lineNumber = 0, [CallerFilePath] string filePath = \u0026#34;\u0026#34;, [CallerMemberName] string memberName = \u0026#34;\u0026#34;) { return queryable.TagWith($\u0026#34;{tag}{Environment.NewLine}{memberName} - {filePath}:{lineNumber}\u0026#34;); } } I copied this IQueryableTaggingExtensions class from Phil Scott. The original code exist in this Github repo, he also wrote up a post on Query Tags.\nI\u0026rsquo;m going to update our LINQ query to use this new extension class.\n1 2 3 4 5 public async Task\u0026lt;IEnumerable\u0026lt;Customer\u0026gt;\u0026gt; Handle(GetCustomerResourceCollectionCommand request, CancellationToken cancellationToken) { var tagMessage = $\u0026#34;Calling from {nameof(GetCustomerResourceCollectionHandler)}\u0026#34;; return await _chinookDbContext.Customers.TagWithSource(tagMessage).ToListAsync(cancellationToken); } Now the generated SQL statement looks like this.\n1 2 3 4 -- Calling from GetCustomerResourceCollectionHandler -- Handle - C:\\Users\\Yunier\\source\\repos\\Chinook\\src\\Chinook.Infrastructure\\Handlers\\GetCustomerResourceCollectionHandler.cs:27 SELECT \u0026#34;c\u0026#34;.\u0026#34;CustomerId\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Address\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;City\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Company\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Country\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Email\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Fax\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;FirstName\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;LastName\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;Phone\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;PostalCode\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;State\u0026#34;, \u0026#34;c\u0026#34;.\u0026#34;SupportRepId\u0026#34; FROM \u0026#34;customers\u0026#34; AS \u0026#34;c\u0026#34; As you can see our original message, the name of method and line number were included as a tag on the generated SQL statement.\nCredits: Better Tagging of EF Core Queries\n","permalink":"http://localhost:1313/post/2020/tagging-ef-core-queries/","summary":"\u003cp\u003e\u003ca href=\"https://devblogs.microsoft.com/dotnet/announcing-entity-framework-core-2-2/#query-tags\"\u003e.NET Core 2.2\u003c/a\u003e introduce a small feature known as \u003cstrong\u003eQuery Tags\u003c/strong\u003e. It allows you to annotate queries generated by EF Core. This is super useful for debugging purposes, after all one of the main complains you often hear about EntityFramework is the over completed SQL statements it generates.\u003c/p\u003e\n\u003cp\u003eI am currently working on a project called \u003ca href=\"https://github.com/circleupx/Chinook\"\u003eChinook\u003c/a\u003e, it demonstrates how to build a JSON:API on .NET Core. The project uses EF Core to query a SQLite database. Here is an example of one of the LINQ queries used to get a collection of users.\u003c/p\u003e","title":"Tagging EF Core Queries"},{"content":"My previous post on JSON:API exposed customers as an API resource, since then, I have updated the project to expose all remaining resources, that includes Albums, Artists, Employees, Genres, Invoices, InvoiceItems, MediaTypes, Playlists, and Tracks. The time has come to expose the relationship that exist between these resource.\nFor this post, I will expose the one-to-many relationship that exist between artists and albums. To accomplish this task I will have to update the class ArtistServiceModelConfiguration by using the ToManyRelationship method exposed by JsonApiFramework in order to link one artist to many albums.\nHere is current class definition for ArtistServiceModelConfiguration.\n1 2 3 4 5 6 7 8 9 class ArtistServiceModelConfiguration : ResourceTypeBuilder\u0026lt;Artist\u0026gt; { public ArtistServiceModelConfiguration() { // Ignore ER Core Navigation Properties this.Attribute(a =\u0026gt; a.Albums) .Ignore(); } } Here is the class definition afterwards.\n1 2 3 4 5 6 7 8 9 10 11 class ArtistServiceModelConfiguration : ResourceTypeBuilder\u0026lt;Artist\u0026gt; { public ArtistServiceModelConfiguration() { // Ignore ER Core Navigation Properties this.Attribute(a =\u0026gt; a.Albums) .Ignore(); this.ToManyRelationship\u0026lt;Album\u0026gt;(AlbumResourceKeyWords.ToManyRelationShipKey); } } With this change the API will expose the to-many part of the relationship between artist and album. Time to expose the to-one part of the relationship, for that I will update the class AlbumServiceModelConfiguration.\nHere is the class definition for AlbumServiceModelConfiguration.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class AlbumServiceModelConfiguration : ResourceTypeBuilder\u0026lt;Album\u0026gt; { public AlbumServiceModelConfiguration() { // Ignore ER Core Navigation Properties this.Attribute(a =\u0026gt; a.Artist) .Ignore(); this.Attribute(a =\u0026gt; a.Tracks) .Ignore(); // Ignore Foreign Keys this.Attribute(a =\u0026gt; a.ArtistId) .Ignore(); } } Here is the class afterwards.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class AlbumServiceModelConfiguration : ResourceTypeBuilder\u0026lt;Album\u0026gt; { public AlbumServiceModelConfiguration() { // Ignore ER Core Navigation Properties this.Attribute(a =\u0026gt; a.Artist) .Ignore(); this.Attribute(a =\u0026gt; a.Tracks) .Ignore(); // Ignore Foreign Keys this.Attribute(a =\u0026gt; a.ArtistId) .Ignore(); this.ToOneRelationship\u0026lt;Artist\u0026gt;(ArtistResourceKeyWords.ToOneRelationshipKey); } } Now the API will be able to expose the one-to-many relationship between artist and albums. With this changes JsonApiFramework will know how to build relationship links between artists and albums. Time to use the new configuration, I\u0026rsquo;ll start by updating the GetResourceCollection and GetResource method that exist in the class AlbumResource and ArtistResource. The change will be be the same across all methods. I will update the fluent style API expose by JsonApiFramework by calling the AddRelationship method in the Relationships chain.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 using var chinookDocumentContext = new ChinookJsonApiDocumentContext(currentRequestUri); var document = chinookDocumentContext .NewDocument(currentRequestUri) .SetJsonApiVersion(JsonApiVersion.Version10) .Links() .AddSelfLink() .AddUpLink() .LinksEnd() .ResourceCollection(artistResourceCollection) .Relationships() .AddRelationship(AlbumResourceKeyWords.ToManyRelationShipKey, new[] { Keywords.Related }) .RelationshipsEnd() .Links() .AddSelfLink() .LinksEnd() .ResourceCollectionEnd() .WriteDocument(); Now if I run the Web API project and go to any given artist, the API will expose a related link to a collection of album.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/artists/1\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:44323/artists\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;artists\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;AC/DC\u0026#34; }, \u0026#34;relationships\u0026#34;: { \u0026#34;albums\u0026#34;: { \u0026#34;links\u0026#34;: { \u0026#34;related\u0026#34;: \u0026#34;https://localhost:44323/artists/1/albums\u0026#34; } } }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/artists/1\u0026#34; } } } and if I go to any given album, the API should expose a related link to a single artist.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/albums/1\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:44323/albums\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;albums\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;For Those About To Rock We Salute You\u0026#34; }, \u0026#34;relationships\u0026#34;: { \u0026#34;artist\u0026#34;: { \u0026#34;links\u0026#34;: { \u0026#34;related\u0026#34;: \u0026#34;https://localhost:44323/albums/1/artist\u0026#34; } } }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/albums/1\u0026#34; } } } Exposing the relationship alone is not enough, if you were to click any of those relationship links you would get a 404 error code, this is simply because we haven\u0026rsquo;t updated the controllers to support these new links. Let\u0026rsquo;s change that, time to update the Artist and Album controllers.\nI\u0026rsquo;ll start with the Artist controller. I\u0026rsquo;ll update it by adding the following method.\n1 2 3 4 5 6 [Route(ArtistRoutes.ArtistResourceToAlbumResourceCollection)] public async Task\u0026lt;IActionResult\u0026gt; GetArtistResourceToAlbumResourceCollection(int resourceId) { var document = await _artistResource.GetArtistResourceToAlbumResourceCollection(resourceId); return Ok(document); } GetArtistResourceToAlbumResourceCollection is a new method on the IArtistResource interface. Here is the updated interface definition.\n1 2 3 4 5 6 public interface IArtistResource { Task\u0026lt;Document\u0026gt; GetArtistResource(int resourceId); Task\u0026lt;Document\u0026gt; GetArtistResourceCollection(); Task\u0026lt;Document\u0026gt; GetArtistResourceToAlbumResourceCollection(int resourceId); } Time to update the ArtistResource class by adding the concrete implementation of the GetArtistResourceToAlbumResourceCollection method.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public async Task\u0026lt;Document\u0026gt; GetArtistResourceToAlbumResourceCollection(int resourceId) { var albumResourceCollection = await _mediator.Send(new GetArtistResourceToAlbumResourceCollectionCommand(resourceId)); var currentRequestUri = _httpContextAccessor.HttpContext.GetCurrentRequestUri(); using var chinookDocumentContext = new ChinookJsonApiDocumentContext(currentRequestUri); var document = chinookDocumentContext .NewDocument(currentRequestUri) .SetJsonApiVersion(JsonApiVersion.Version10) .Links() .AddSelfLink() .AddUpLink() .LinksEnd() .ResourceCollection(albumResourceCollection) .Links() .AddSelfLink() .LinksEnd() .ResourceCollectionEnd() .WriteDocument(); _logger.LogInformation(\u0026#34;Request for {URL} generated JSON:API document {doc}\u0026#34;, currentRequestUri, document); return document; } Perfect, the controller has been configured, now if I click on the related link to albums for artist with id 1 the API will respond with the following JSON:API document.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/artists/1/albums\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:44323/artists/1\u0026#34; }, \u0026#34;data\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;albums\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;For Those About To Rock We Salute You\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/albums/1\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;albums\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Let There Be Rock\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/albums/4\u0026#34; } } ] } Now I will need to make a similar change to the album controller.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 // new controller method [Route(AlbumRoutes.AlbumResourceToArtistResource)] public async Task\u0026lt;IActionResult\u0026gt; GetAlbumResourceToArtistResource(int resourceId) { var document = await _albumResource.GetAlbumResourceToArtistResource(resourceId); return Ok(document); } // updated IAlbumResource interface public interface IAlbumResource { Task\u0026lt;Document\u0026gt; GetAlbumResource(int resourceId); Task\u0026lt;Document\u0026gt; GetAlbumResourceCollection(); Task\u0026lt;Document\u0026gt; GetAlbumResourceToArtistResource(int resourceId); } // concrete GetAlbumResourceToArtistResource implementation public async Task\u0026lt;Document\u0026gt; GetAlbumResourceToArtistResource(int resourceId) { var artistResource = await _mediator.Send(new GetAlbumResourceToArtistResourceCommand(resourceId)); var currentRequestUri = _httpContextAccessor.HttpContext.GetCurrentRequestUri(); using var chinookDocumentContext = new ChinookJsonApiDocumentContext(currentRequestUri); var document = chinookDocumentContext .NewDocument(currentRequestUri) .SetJsonApiVersion(JsonApiVersion.Version10) .Links() .AddSelfLink() .AddUpLink() .LinksEnd() .Resource(artistResource) .Links() .AddSelfLink() .LinksEnd() .ResourceEnd() .WriteDocument(); _logger.LogInformation(\u0026#34;Request for {URL} generated JSON:API document {doc}\u0026#34;, currentRequestUri, document); return document; } Now if I click the related link to artist on an album resource I get the following JSON:API document as a response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/albums/4/artist\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:44323/albums/4\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;artists\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;AC/DC\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/artists/1\u0026#34; } } } All that remains to do now is to expose all relationships. Something that I will accomplish before our next post.\n","permalink":"http://localhost:1313/post/2020/json-api-exposing-relationships/","summary":"\u003cp\u003eMy \u003ca href=\"/post/2020/json-api-exposing-the-customer-resource/\"\u003eprevious\u003c/a\u003e post on JSON:API exposed customers as an API resource, since then, I have updated the project to expose all remaining resources, that includes  Albums, Artists, Employees, Genres, Invoices, InvoiceItems, MediaTypes, Playlists, and Tracks. The time has come to expose the relationship that exist between these resource.\u003c/p\u003e\n\u003cp\u003eFor this post, I will expose the one-to-many relationship that exist between artists and albums. To accomplish this task I will have to update the class ArtistServiceModelConfiguration by using the ToManyRelationship method exposed by JsonApiFramework in order to link one artist to many albums.\u003c/p\u003e","title":"JSON:API - Exposing Relationships"},{"content":"When the .NET Core team started to envision how the .NET Framework would look like as a modern web framework they set out to expand the testing capabilities of the framework. If you come from the world of .NET MVC 5 you probably know that one of the best ways to test an HTTP request in MVC 5 was to use Phil\u0026rsquo;s HttpSimulator.\nThat is no longer the case in .NET Core thanks to the power of the WebApplicationFactory class. This class creates a local instance of TestServer, TestServer creates a local kestrel web server. Since we are dealing with an actual web server, not a fake web server, there is no need to stub, fake, or mock anything. The HTTP request that are made to the local kestrel web server are legitimate HTTP request, this gives you the power to test your application\u0026rsquo;s functionality from visual studio, build server, or wherever you are executing your Unit Test as if the app where hosted on a live server.\nAnother neat feature of WebApplication factory is that it can create one or more instances of HttpClient, that means that not only does WebApplicationFactory allow us to test our application from the server side but it also allows you to test clients.\nImagine the following scenario, we want to test how our application handles conditional HTTP GET request, using the WebApplicationFactory class, you would instantiate an HttpClient that would send an HTTP GET request to the kestrel server. The server responds to the request with 200 OK, in the response headers, an ETag and Last-Modified are included. The HttpClient sends a second request to the kestrel server, in the request headers, If-None-Match and If-Modified-Since are included. The resource that was requested on the first GET request has not change, therefore, the server should return a 304 Not-Modify. If the resource has change sine the first request than a 200 OK should be return with the updated content and a new ETag. If the resource has not change but the ETag has expire then the server should return a 200 OK with no content (a conditional request) to signal the client that their cache version of the resource is still valid. All these scenarios can be part of an integration test suite in our project, all possible thanks to WebApplicationFactory. Having the power to test how a client and server interact with each other in single test is super useful and extremely powerful. It elevates our testing to a new levels, it gives us confidence that our application will work as expected once it has been deployed.\nFor today\u0026rsquo;s post I will show you how setup WebApplicationFactory to create integration tests. In a future post I will take the tests even further by leveraging SQLite as in-memory database, this will allow me to test the client, the API server and the database to ensure the data was persisted correctly. Please be aware that the database option may not work for you, it really depends on your use case. In .NET Core you can use the EF Core in-memory provider or an in-memory SQLite database if you are using SQLServer, MySQL or any other RDBMS. Personally, I recommend never using the in-memory provider. Jimmy Bogard explains why on his blog.\nTo demonstrate the how WebApplicationFactory works I will create a new XUnit Test project on my Chinook JSON:API project. The project will be called Chinook.Web.IntegrationTest and it will be placed under the integration folder.\nNow that we have a test project, I will create a CustomWebApplicationFactory class. This class will inherit from WebApplicationFactory. The purpose of the CustomWebApplicationFactory is to have a centralized location to add services, register middlewares, configure HttpClients, and override services registered on StartUp class.\nBefore I go on, I will need to install a few NuGet packages on the Chinook.Web.IntegrationTest project.\n1 dotnet add package Microsoft.AspNetCore.Mvc.Testing --version 5.0.1 This NuGet package is required to use the WebApplicationFactory.\n1 dotnet add package FluentAssertions --version 5.10.3 I\u0026rsquo;m a huge fan of fluent style interfaces, fluent assertions is a great NuGet package for assertions.\nNow that I have the required NuGet packages I will create the CustomApplicationFactory class.\nHere is the class definition for CustomWebApplicationFactory.\n1 2 3 4 5 6 7 8 public class CustomWebApplicationFactory : WebApplicationFactory\u0026lt;Program\u0026gt; { protected override IHostBuilder CreateHostBuilder() { var builder = base.CreateHostBuilder(); return builder; } } Are you surprise by how simple the class looks?\nWell, that is only because my Chinook project has SQLite already configured as an in-memory database by reading a SQLite database file from the project directory. I would imagine that for most of you this is not the case, you are probably using some database hosted on a remote server and you\u0026rsquo;ve registered this dependency as a service on the StartUp class, probably like this.\n1 2 3 4 5 6 7 8 9 public class StartUp { // rest of the code omitted for brevity public void ConfigureServices(IServiceCollection services) { var connectionString = \u0026#34;REMOTE SERVER CONNECTION STRING\u0026#34;; services.AddDbContext\u0026lt;YourDbContext\u0026gt;(options =\u0026gt; options.UseSqlServer(connectionString)); } } If you want to switch to SQLite as an in-memory database provider for your integration test, then you will need to override your DbContext service, you can do so by overriding ConfigureWebHost as demonstrated on the following code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class CustomWebApplicationFactory : WebApplicationFactory\u0026lt;Program\u0026gt; { protected override void ConfigureWebHost(IWebHostBuilder builder) { builder.ConfigureServices(ConfigureServices); } private void ConfigureServices(WebHostBuilderContext webHostBuilderContext, IServiceCollection serviceCollection) { var dbContextService = serviceCollection.SingleOrDefault(d =\u0026gt; d.ServiceType == typeof(DbContextOptions\u0026lt;ChinookDbContext\u0026gt;)); if(dbContextService != null) { // remove the DbContext that is registered on StartUp.cs serviceCollection.Remove(dbContextService); } // register the new DbContext, .NET Core dependency injection framework will now use the this instance. var sqliteInMemoryConnectionString = new SqliteConnection(\u0026#34;DataSource=:memory:\u0026#34;); serviceCollection.AddDbContext\u0026lt;ChinookDbContext\u0026gt;(contextOptions =\u0026gt; contextOptions.UseSqlite(sqliteInMemoryConnectionString)); } } Now an in-memory SQLite database will be use instead of whatever DbContext you\u0026rsquo;ve registered on StartUp.cs, by the way, the code above will create an empty SQLite database, if you need to seed the database then you will need to access the DbContext, see the following code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // rest of the code omitted for brevity private void ConfigureServices(WebHostBuilderContext webHostBuilderContext, IServiceCollection serviceCollection) { var dbContextService = serviceCollection.SingleOrDefault(d =\u0026gt; d.ServiceType == typeof(DbContextOptions\u0026lt;ChinookDbContext\u0026gt;)); if(dbContextService != null) { // remove the DbContext that is registered on StartUp.cs serviceCollection.Remove(dbContextService); } // register the new DbContext, .NET Core dependency injection framework will now use the in-memory SQLite instance instead of whatever configuration was used to register the DbContext on the StartUp class. var sqliteInMemoryConnectionString = new SqliteConnection(\u0026#34;DataSource=:memory:\u0026#34;); serviceCollection.AddDbContext\u0026lt;ChinookDbContext\u0026gt;(contextOptions =\u0026gt; contextOptions.UseSqlite(sqliteInMemoryConnectionString)); var builtServiceProvider = serviceCollection.BuildServiceProvider(); using var scopedProvider = builtServiceProvider.CreateScope(); var scopedServiceProvider = scopedProvider.ServiceProvider() // private field omitted for brevity _chinookDbContext = scopedServiceProvider.GetRequiredService\u0026lt;ChinookDbContext\u0026gt;(); // these two lines are important, they ensure the in-memory database is created now. _chinookDbContext.Database.OpenConnection(); _chinookDbContext.Database.EnsureCreated(); // database is now ready to be seeded through the DbContext. The data will be available in each of your integration test due to the scope of the DbContext. } Anyways, back to our integration test. Our CustomApplicationFactory is ready, time to use it on a test. I will start by creating a test that instantiates an instance of HttpClient, the client will then send an HTTP GET request to the Home resource of the API, the API should return a 200 OK.\nHere is the class definition for HomeControllerIntegrationTest.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class HomeControllerIntegrationTest : IClassFixture\u0026lt;CustomWebApplicationFactory\u0026gt; { private readonly CustomWebApplicationFactory _customWebApplicationFactory; public HomeControllerIntegrationTest(CustomWebApplicationFactory customWebApplicationFactory) { _customWebApplicationFactory = customWebApplicationFactory; } [Fact] public async Task GetHomeResource_HttpResponse_ShouldReturn200OK() { // Arrange using var httpClient = _customWebApplicationFactory.CreateClient(); var requestUri = httpClient.BaseAddress.AbsoluteUri; // Act var sut = await httpClient.GetAsync(requestUri); // Assert var responseCode = sut.StatusCode; responseCode.Should().BeEquivalentTo(HttpStatusCode.OK); } } Time to execute the test on Visual Studio.\nAs you can see the test passed when executed on visual studio. In less than half of a second, our integration test create a kestrel web server, it configured the server, it configure our API project, it made the API accessible through HTTP, it sent an HTTP request to the server, it received a 200 OK response and it validated the response. Awesome.\nSummary:\nIn this post I demonstrate how to build integration test using WebApplicationFactory. These test do not stub, mock or fake any data or http life cycle. The test are executed against a local kestrel web server, they run in memory and are very fast. These test allow you to validate your application\u0026rsquo;s functionality as if it were deployed to a live server.\n","permalink":"http://localhost:1313/post/2021/integration-testing-using-webapplicationfactory/","summary":"\u003cp\u003eWhen the .NET Core team started to envision how the .NET Framework would look like as a modern web framework they set out to expand the testing capabilities of the framework. If you come from the world of \u003ca href=\"https://docs.microsoft.com/en-us/aspnet/mvc/overview/getting-started/introduction/getting-started\"\u003e.NET MVC 5\u003c/a\u003e you probably know that one of the best ways to test an HTTP request in MVC 5 was to use \u003ca href=\"https://haacked.com/archive/2007/06/19/unit-tests-web-code-without-a-web-server-using-httpsimulator.aspx/\"\u003ePhil\u0026rsquo;s HttpSimulator\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThat is no longer the case in .NET Core thanks to the power of the \u003ca href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.mvc.testing.webapplicationfactory-1?view=aspnetcore-5.0\"\u003eWebApplicationFactory\u003c/a\u003e class. This class creates a local instance of \u003ca href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.testhost.testserver?view=aspnetcore-5.0\"\u003eTestServer\u003c/a\u003e, TestServer creates a local \u003ca href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel?view=aspnetcore-5.0\"\u003ekestrel\u003c/a\u003e web server. Since we are dealing with an actual web server, not a fake web server, there is no need to stub, fake, or mock anything. The HTTP request that are made to the local kestrel web server are legitimate HTTP request, this gives you the power to test your application\u0026rsquo;s functionality from visual studio, build server, or wherever you are executing your Unit Test as if the app where hosted on a live server.\u003c/p\u003e","title":"Integration Testing Using WebApplicationFactory"},{"content":"I want to step away from software for this post to talk about some hardware. Over the years I\u0026rsquo;ve owned a few routers, some have been really really good and some have been really bad. So far the best router I have owned is my current router, the UniFi Dream Machine. While I was mostly happy with my last router, the NighHawk AC1900, it did dropped the WiFi signal a lot, I think that at one point it was dropping the WiFi signal once a week. That prompted me to start looking for a new router. After doing some research, and seeing Troy Hunt presentation on bad IOT devices and NetworkChuck\u0026rsquo;s review on the Dream Machine, I was sold on the Dream Machine.\nI\u0026rsquo;ve owned the Dream Machine for over nine months, and while I love my Dream Machine, there is one tiny little thing that I hate about it. The Blue LED light, the one that appears on top of the cylinder, the blue light means the Dream Machine is on and that there is internet connectivity. It may not look like it from the photos or videos, but that light is super bright, and if you have Dream Machine in a bedroom then it becomes almost impossible to sleep at night. I\u0026rsquo;ve been searching for a way to turn this light off from the very first day that I received the Dream Machine. There is no option to turn off the light on the UniFi Dashboard or UniFi app. Luckily, someone else also hate how bright the LED light was and came up with a solution.\nThat someone else was ameyuuno, he create a tool that allows you to turn the LED light on and off base on a cron job. Which is just the perfect solution for anyone that wants to turn off the LED light, but for me, I was so tired of the LED light that I want it off permanently. I took a look at the project ameyunno created and I notice that all I needed to do was modify this shell script and then to run the script from a linux based machine like a Raspberry Pie or from Windows using WSL.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 #!/bin/sh set -e echo \u0026#34;Run LED light switch script [$(date)].\u0026#34; led_state=off username=\u0026#39;REPLACE_WITH_UNIFI_USERNAME\u0026#39; password=\u0026#39;REPLACE_WITH_UNIFI_PASSWORD\u0026#39; url=\u0026#39;REPLACE_WITH_UNIFI_URL:8443\u0026#39; echo \u0026#34;State is: \u0026#39;${led_state}\u0026#39;\u0026#34; echo \u0026#34;Username is: \u0026#39;${username}\u0026#39;\u0026#34; echo \u0026#34;Password is: \u0026#39;${password}\u0026#39;\u0026#34; echo \u0026#34;Url is: \u0026#39;${url}\u0026#39;\u0026#34; case \u0026#34;${led_state}\u0026#34; in on) state=true ;; off) state=false ;; *) echo \u0026#34;Unknown state argument value: \u0026#39;${led_state}\u0026#39;\u0026#34; exit 1 ;; esac username=${username} password=${password} baseUrl=${url} cookie=/tmp/unifi-cookie make_request() { payload=$1 url=$2 curl --tlsv1 --silent --cookie ${cookie} --cookie-jar ${cookie} --insecure --data \u0026#34;${payload}\u0026#34; \u0026#34;${url}\u0026#34; } extract_status() { cat | jq -r \u0026#39;.meta | if .rc == \u0026#34;ok\u0026#34; then .rc else .rc + \u0026#34;(\u0026#34; + .msg + \u0026#34;)\u0026#34; end | \u0026#34;Status: \u0026#34; + .\u0026#39; } print_status_and_set_return_code() { status=$(cat) echo ${status} if [ \u0026#34;${status}\u0026#34; != \u0026#34;Status: ok\u0026#34; ]; then echo \u0026#34;Return non-zero code because an error was occurred.\u0026#34; return 1 fi return 0 } echo \u0026#34;Login on UniFi.\u0026#34; make_request \u0026#34;{\\\u0026#34;username\\\u0026#34;: \\\u0026#34;${username}\\\u0026#34;, \\\u0026#34;password\\\u0026#34;: \\\u0026#34;${password}\\\u0026#34;}\u0026#34; \u0026#34;${baseUrl}/api/login\u0026#34; | extract_status | print_status_and_set_return_code echo \u0026#34;Change state of LED to ${led_state}.\u0026#34; make_request \u0026#34;{\\\u0026#34;led_enabled\\\u0026#34;: \\\u0026#34;${state}\\\u0026#34;}\u0026#34; \u0026#34;${baseUrl}/api/s/default/set/setting/mgmt/\u0026#34; | extract_status | print_status_and_set_return_code echo \u0026#34;Logout.\u0026#34; make_request \u0026#34;\u0026#34; \u0026#34;${baseUrl}/api/logout\u0026#34; | extract_status | print_status_and_set_return_code rm -f ${cookie} echo \u0026#34;Done.\u0026#34; Before running the script, make sure to update the variables username, password and url with your username, your password, and the URL you use to connect to the UniFi Dream Machine. Do not drop the port, 8443, you need it. Also, if you have 2FA authentication enabled, you should, you will need to disable it before running the script. You can turn 2FA on again after running the script.\nCredits: All credits go to ameyuuno. I recommend checking out his script if you are interested on having the LED light be on and off base on a schedule.\n","permalink":"http://localhost:1313/post/2020/dream-machine-turn-off-blue-led-light-on-dreammachine/","summary":"\u003cp\u003eI want to step away from software for this post to talk about some hardware. Over the years I\u0026rsquo;ve owned a few routers, some have been really really \u003ca href=\"https://www.amazon.com/R7000-100PAS-Nighthawk-Parental-Controls-Compatible/dp/B00F0DD0I6/\"\u003egood\u003c/a\u003e and some have been really \u003ca href=\"https://en.wikipedia.org/wiki/Linksys_WRT54G_series\"\u003ebad\u003c/a\u003e. So far the best router I have owned is my current router, the \u003ca href=\"https://www.amazon.com/Ubiquiti-UniFi-Dream-Machine-UDM-US/dp/B081QNJFPV/\"\u003eUniFi Dream Machine\u003c/a\u003e. While I was mostly happy with my last router, the NighHawk AC1900, it did dropped the WiFi signal a lot, I think that at one point it was dropping the WiFi signal once a week. That prompted me to start looking for a new router. After doing some research, and seeing \u003ca href=\"https://youtu.be/FRsRoaubPiY?t=1800\"\u003eTroy Hunt presentation on bad IOT devices\u003c/a\u003e and NetworkChuck\u0026rsquo;s \u003ca href=\"https://www.youtube.com/watch?v=BezoNUflqXo\"\u003ereview\u003c/a\u003e on the Dream Machine, I was sold on the Dream Machine.\u003c/p\u003e","title":"Dream Machine - Turn On/Off LED Light Switch"},{"content":"This will be my third blog post on JSON:API in .NET Core.\nI plant to add Customer as an API resource, but before we get too deep on the code, I would like to review the Chinook database project. To do that I\u0026rsquo;m going to import Chinook.db into DB Browser for SQLite to see all available entities.\nAs you can see we have quite a few entities, for this blog post I will concentrate on the customers entity. To accomplish adding customers as an API resource I will need to create a new service model that represents the customers entity in both JsonApiFramework and EF Core. I will scaffold the SQLite database using EF Core\u0026rsquo;s reverse engineering capabilities.\nFor that, I\u0026rsquo;ll open up a command line tool (windows terminal in my case) to execute the following commands.\n1 dotnet tool install --global dotnet-ef This installs EF Core as a global tool on the dotnet CLI. Verify your installation by running,\n1 dotnet ef Next, I\u0026rsquo;ll need to add the Microsoft.EntityFrameworkCore.Design NuGet package, the package will be install on the Chinook Core project.\n1 dotnet add Chinook.Core package Microsoft.EntityFrameworkCore.Design Additionally, I am going to need Microsoft.EntityFrameworkCore.Sqlite to work with the database since our database is a SQLite database. It will also be installed on the Chinook.Core project.\n1 dotnet add Chinook.Core package Microsoft.EntityFrameworkCore.Sqlite Now I can run the ef scaffold command to generate entity models from the chinook database file.\n1 dotnet ef dbcontext scaffold \u0026#34;DataSource=chinook.db\u0026#34; Microsoft.EntityFrameworkCore.Sqlite --project=Chinook.Core Back on Visual Studio, if I expand the Chinook.Core project I can see that the entity models were successfully created.\nHere is the EF Core database context generated by the scaffold tool.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 public partial class ChinookContext : DbContext { public chinookContext() { } public chinookContext(DbContextOptions\u0026lt;chinookContext\u0026gt; options): base(options) { } public virtual DbSet\u0026lt;Albums\u0026gt; Albums { get; set; } public virtual DbSet\u0026lt;Artists\u0026gt; Artists { get; set; } public virtual DbSet\u0026lt;Customers\u0026gt; Customers { get; set; } public virtual DbSet\u0026lt;Employees\u0026gt; Employees { get; set; } public virtual DbSet\u0026lt;Genres\u0026gt; Genres { get; set; } public virtual DbSet\u0026lt;InvoiceItems\u0026gt; InvoiceItems { get; set; } public virtual DbSet\u0026lt;Invoices\u0026gt; Invoices { get; set; } public virtual DbSet\u0026lt;MediaTypes\u0026gt; MediaTypes { get; set; } public virtual DbSet\u0026lt;PlaylistTrack\u0026gt; PlaylistTrack { get; set; } public virtual DbSet\u0026lt;Playlists\u0026gt; Playlists { get; set; } public virtual DbSet\u0026lt;Tracks\u0026gt; Tracks { get; set; } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) { if (!optionsBuilder.IsConfigured) { optionsBuilder.UseSqlite(\u0026#34;DataSource=chinook.db\u0026#34;); } } protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity\u0026lt;Albums\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; e.AlbumId); entity.ToTable(\u0026#34;albums\u0026#34;); entity.HasIndex(e =\u0026gt; e.ArtistId) .HasName(\u0026#34;IFK_AlbumArtistId\u0026#34;); entity.Property(e =\u0026gt; e.AlbumId).ValueGeneratedNever(); entity.Property(e =\u0026gt; e.Title) .IsRequired() .HasColumnType(\u0026#34;NVARCHAR(160)\u0026#34;); entity.HasOne(d =\u0026gt; d.Artist) .WithMany(p =\u0026gt; p.Albums) .HasForeignKey(d =\u0026gt; d.ArtistId) .OnDelete(DeleteBehavior.ClientSetNull); }); modelBuilder.Entity\u0026lt;Artists\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; e.ArtistId); entity.ToTable(\u0026#34;artists\u0026#34;); entity.Property(e =\u0026gt; e.ArtistId).ValueGeneratedNever(); entity.Property(e =\u0026gt; e.Name).HasColumnType(\u0026#34;NVARCHAR(120)\u0026#34;); }); modelBuilder.Entity\u0026lt;Customers\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; e.CustomerId); entity.ToTable(\u0026#34;customers\u0026#34;); entity.HasIndex(e =\u0026gt; e.SupportRepId) .HasName(\u0026#34;IFK_CustomerSupportRepId\u0026#34;); entity.Property(e =\u0026gt; e.CustomerId).ValueGeneratedNever(); entity.Property(e =\u0026gt; e.Address).HasColumnType(\u0026#34;NVARCHAR(70)\u0026#34;); entity.Property(e =\u0026gt; e.City).HasColumnType(\u0026#34;NVARCHAR(40)\u0026#34;); entity.Property(e =\u0026gt; e.Company).HasColumnType(\u0026#34;NVARCHAR(80)\u0026#34;); entity.Property(e =\u0026gt; e.Country).HasColumnType(\u0026#34;NVARCHAR(40)\u0026#34;); entity.Property(e =\u0026gt; e.Email) .IsRequired() .HasColumnType(\u0026#34;NVARCHAR(60)\u0026#34;); entity.Property(e =\u0026gt; e.Fax).HasColumnType(\u0026#34;NVARCHAR(24)\u0026#34;); entity.Property(e =\u0026gt; e.FirstName) .IsRequired() .HasColumnType(\u0026#34;NVARCHAR(40)\u0026#34;); entity.Property(e =\u0026gt; e.LastName) .IsRequired() .HasColumnType(\u0026#34;NVARCHAR(20)\u0026#34;); entity.Property(e =\u0026gt; e.Phone).HasColumnType(\u0026#34;NVARCHAR(24)\u0026#34;); entity.Property(e =\u0026gt; e.PostalCode).HasColumnType(\u0026#34;NVARCHAR(10)\u0026#34;); entity.Property(e =\u0026gt; e.State).HasColumnType(\u0026#34;NVARCHAR(40)\u0026#34;); entity.HasOne(d =\u0026gt; d.SupportRep) .WithMany(p =\u0026gt; p.Customers) .HasForeignKey(d =\u0026gt; d.SupportRepId); }); modelBuilder.Entity\u0026lt;Employees\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; e.EmployeeId); entity.ToTable(\u0026#34;employees\u0026#34;); entity.HasIndex(e =\u0026gt; e.ReportsTo) .HasName(\u0026#34;IFK_EmployeeReportsTo\u0026#34;); entity.Property(e =\u0026gt; e.EmployeeId).ValueGeneratedNever(); entity.Property(e =\u0026gt; e.Address).HasColumnType(\u0026#34;NVARCHAR(70)\u0026#34;); entity.Property(e =\u0026gt; e.BirthDate).HasColumnType(\u0026#34;DATETIME\u0026#34;); entity.Property(e =\u0026gt; e.City).HasColumnType(\u0026#34;NVARCHAR(40)\u0026#34;); entity.Property(e =\u0026gt; e.Country).HasColumnType(\u0026#34;NVARCHAR(40)\u0026#34;); entity.Property(e =\u0026gt; e.Email).HasColumnType(\u0026#34;NVARCHAR(60)\u0026#34;); entity.Property(e =\u0026gt; e.Fax).HasColumnType(\u0026#34;NVARCHAR(24)\u0026#34;); entity.Property(e =\u0026gt; e.FirstName) .IsRequired() .HasColumnType(\u0026#34;NVARCHAR(20)\u0026#34;); entity.Property(e =\u0026gt; e.HireDate).HasColumnType(\u0026#34;DATETIME\u0026#34;); entity.Property(e =\u0026gt; e.LastName) .IsRequired() .HasColumnType(\u0026#34;NVARCHAR(20)\u0026#34;); entity.Property(e =\u0026gt; e.Phone).HasColumnType(\u0026#34;NVARCHAR(24)\u0026#34;); entity.Property(e =\u0026gt; e.PostalCode).HasColumnType(\u0026#34;NVARCHAR(10)\u0026#34;); entity.Property(e =\u0026gt; e.State).HasColumnType(\u0026#34;NVARCHAR(40)\u0026#34;); entity.Property(e =\u0026gt; e.Title).HasColumnType(\u0026#34;NVARCHAR(30)\u0026#34;); entity.HasOne(d =\u0026gt; d.ReportsToNavigation) .WithMany(p =\u0026gt; p.InverseReportsToNavigation) .HasForeignKey(d =\u0026gt; d.ReportsTo); }); modelBuilder.Entity\u0026lt;Genres\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; e.GenreId); entity.ToTable(\u0026#34;genres\u0026#34;); entity.Property(e =\u0026gt; e.GenreId).ValueGeneratedNever(); entity.Property(e =\u0026gt; e.Name).HasColumnType(\u0026#34;NVARCHAR(120)\u0026#34;); }); modelBuilder.Entity\u0026lt;InvoiceItems\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; e.InvoiceLineId); entity.ToTable(\u0026#34;invoice_items\u0026#34;); entity.HasIndex(e =\u0026gt; e.InvoiceId) .HasName(\u0026#34;IFK_InvoiceLineInvoiceId\u0026#34;); entity.HasIndex(e =\u0026gt; e.TrackId) .HasName(\u0026#34;IFK_InvoiceLineTrackId\u0026#34;); entity.Property(e =\u0026gt; e.InvoiceLineId).ValueGeneratedNever(); entity.Property(e =\u0026gt; e.UnitPrice) .IsRequired() .HasColumnType(\u0026#34;NUMERIC(10,2)\u0026#34;); entity.HasOne(d =\u0026gt; d.Invoice) .WithMany(p =\u0026gt; p.InvoiceItems) .HasForeignKey(d =\u0026gt; d.InvoiceId) .OnDelete(DeleteBehavior.ClientSetNull); entity.HasOne(d =\u0026gt; d.Track) .WithMany(p =\u0026gt; p.InvoiceItems) .HasForeignKey(d =\u0026gt; d.TrackId) .OnDelete(DeleteBehavior.ClientSetNull); }); modelBuilder.Entity\u0026lt;Invoices\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; e.InvoiceId); entity.ToTable(\u0026#34;invoices\u0026#34;); entity.HasIndex(e =\u0026gt; e.CustomerId) .HasName(\u0026#34;IFK_InvoiceCustomerId\u0026#34;); entity.Property(e =\u0026gt; e.InvoiceId).ValueGeneratedNever(); entity.Property(e =\u0026gt; e.BillingAddress).HasColumnType(\u0026#34;NVARCHAR(70)\u0026#34;); entity.Property(e =\u0026gt; e.BillingCity).HasColumnType(\u0026#34;NVARCHAR(40)\u0026#34;); entity.Property(e =\u0026gt; e.BillingCountry).HasColumnType(\u0026#34;NVARCHAR(40)\u0026#34;); entity.Property(e =\u0026gt; e.BillingPostalCode).HasColumnType(\u0026#34;NVARCHAR(10)\u0026#34;); entity.Property(e =\u0026gt; e.BillingState).HasColumnType(\u0026#34;NVARCHAR(40)\u0026#34;); entity.Property(e =\u0026gt; e.InvoiceDate) .IsRequired() .HasColumnType(\u0026#34;DATETIME\u0026#34;); entity.Property(e =\u0026gt; e.Total) .IsRequired() .HasColumnType(\u0026#34;NUMERIC(10,2)\u0026#34;); entity.HasOne(d =\u0026gt; d.Customer) .WithMany(p =\u0026gt; p.Invoices) .HasForeignKey(d =\u0026gt; d.CustomerId) .OnDelete(DeleteBehavior.ClientSetNull); }); modelBuilder.Entity\u0026lt;MediaTypes\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; e.MediaTypeId); entity.ToTable(\u0026#34;media_types\u0026#34;); entity.Property(e =\u0026gt; e.MediaTypeId).ValueGeneratedNever(); entity.Property(e =\u0026gt; e.Name).HasColumnType(\u0026#34;NVARCHAR(120)\u0026#34;); }); modelBuilder.Entity\u0026lt;PlaylistTrack\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; new { e.PlaylistId, e.TrackId }); entity.ToTable(\u0026#34;playlist_track\u0026#34;); entity.HasIndex(e =\u0026gt; e.TrackId) .HasName(\u0026#34;IFK_PlaylistTrackTrackId\u0026#34;); entity.HasOne(d =\u0026gt; d.Playlist) .WithMany(p =\u0026gt; p.PlaylistTrack) .HasForeignKey(d =\u0026gt; d.PlaylistId) .OnDelete(DeleteBehavior.ClientSetNull); entity.HasOne(d =\u0026gt; d.Track) .WithMany(p =\u0026gt; p.PlaylistTrack) .HasForeignKey(d =\u0026gt; d.TrackId) .OnDelete(DeleteBehavior.ClientSetNull); }); modelBuilder.Entity\u0026lt;Playlists\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; e.PlaylistId); entity.ToTable(\u0026#34;playlists\u0026#34;); entity.Property(e =\u0026gt; e.PlaylistId).ValueGeneratedNever(); entity.Property(e =\u0026gt; e.Name).HasColumnType(\u0026#34;NVARCHAR(120)\u0026#34;); }); modelBuilder.Entity\u0026lt;Tracks\u0026gt;(entity =\u0026gt; { entity.HasKey(e =\u0026gt; e.TrackId); entity.ToTable(\u0026#34;tracks\u0026#34;); entity.HasIndex(e =\u0026gt; e.AlbumId) .HasName(\u0026#34;IFK_TrackAlbumId\u0026#34;); entity.HasIndex(e =\u0026gt; e.GenreId) .HasName(\u0026#34;IFK_TrackGenreId\u0026#34;); entity.HasIndex(e =\u0026gt; e.MediaTypeId) .HasName(\u0026#34;IFK_TrackMediaTypeId\u0026#34;); entity.Property(e =\u0026gt; e.TrackId).ValueGeneratedNever(); entity.Property(e =\u0026gt; e.Composer).HasColumnType(\u0026#34;NVARCHAR(220)\u0026#34;); entity.Property(e =\u0026gt; e.Name) .IsRequired() .HasColumnType(\u0026#34;NVARCHAR(200)\u0026#34;); entity.Property(e =\u0026gt; e.UnitPrice) .IsRequired() .HasColumnType(\u0026#34;NUMERIC(10,2)\u0026#34;); entity.HasOne(d =\u0026gt; d.Album) .WithMany(p =\u0026gt; p.Tracks) .HasForeignKey(d =\u0026gt; d.AlbumId); entity.HasOne(d =\u0026gt; d.Genre) .WithMany(p =\u0026gt; p.Tracks) .HasForeignKey(d =\u0026gt; d.GenreId); entity.HasOne(d =\u0026gt; d.MediaType) .WithMany(p =\u0026gt; p.Tracks) .HasForeignKey(d =\u0026gt; d.MediaTypeId) .OnDelete(DeleteBehavior.ClientSetNull); }); OnModelCreatingPartial(modelBuilder); } partial void OnModelCreatingPartial(ModelBuilder modelBuilder); On a future blog post, I will come back to clean this up a bit more, I prefer having an EntityConfiguration class per entity rather than having everything define in the DbContext, but for now this will do.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public partial class Customers { public Customers() { Invoices = new HashSet\u0026lt;Invoices\u0026gt;(); } public long CustomerId { get; set; } public string FirstName { get; set; } public string LastName { get; set; } public string Company { get; set; } public string Address { get; set; } public string City { get; set; } public string State { get; set; } public string Country { get; set; } public string PostalCode { get; set; } public string Phone { get; set; } public string Fax { get; set; } public string Email { get; set; } public long? SupportRepId { get; set; } public virtual Employees SupportRep { get; set; } public virtual ICollection\u0026lt;Invoices\u0026gt; Invoices { get; set; } } This is the Customer model class that was generated using the scaffold tool. Let me clean up the project a little by moving all models that were generated by the scaffold to be under the ServiceModel folder, the namespace will be updated accordingly.\nOur next task will be to configure JsonApiFramework to work with our new entities by adding a ResourceConfiguration class for our Customer service model.\n1 2 3 4 class CustomerServiceModelConfiguration : ResourceTypeBuilder\u0026lt;Customers\u0026gt; { } Simple, nothing complicated, next we need to update the HomeResource class. The home resource will now need to expose a link to the customers resource. That be accomplished by modifying our HomeResource class like this,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 public class HomeResource { private readonly IHttpContextAccessor httpContextAccessor; private readonly ILogger\u0026lt;HomeResource\u0026gt; logger; public HomeResource(IHttpContextAccessor httpContextAccessor, ILogger\u0026lt;HomeResource\u0026gt; logger) { this.httpContextAccessor = httpContextAccessor; this.logger = logger; } public Task\u0026lt;Document\u0026gt; GetHomeDocument() { var homeResource = new HomeServiceModel { Message = \u0026#34;Hello World\u0026#34; }; var currentRequestUri = httpContextAccessor.HttpContext.GetCurrentRequestUri(); var scheme = currentRequestUri.Scheme; var host = currentRequestUri.Host; var port = currentRequestUri.Port; var urlBuilderConfiguration = new UrlBuilderConfiguration(scheme, host, port); var customersResourceCollectionLink = CreateCustomerResourceCollectionLink(urlBuilderConfiguration); using var chinookDocumentContext = new ChinookDocumentContext(currentRequestUri); var document = chinookDocumentContext .NewDocument(currentRequestUri) .SetJsonApiVersion(JsonApiVersion.Version10) .Links() .AddSelfLink() .LinksEnd() .Resource(homeResource) .Links() .AddLink(CustomerResourceKeyWords.Self, customersResourceCollectionLink) .LinksEnd() .ResourceEnd() .WriteDocument(); return Task.FromResult(document); } private Link CreateCustomerResourceCollectionLink(UrlBuilderConfiguration urlBuilderConfiguration) { var customersResourceCollectionLink = UrlBuilder.Create(urlBuilderConfiguration) .Path(CustomerResourceKeyWords.Self) .Build(); return new Link(customersResourceCollectionLink); } } Let\u0026rsquo;s review the changes I just made, the first change made was to call GetCurrentRequestUri(), this is a new method that exist within the HttpContextExtensions class. Here is the class definition.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public static class HttpContextExtensions { public static Uri GetCurrentRequestUri(this HttpContext httpContext) { var currentRequest = httpContext.Request; var currentRequestUriBuilder = new UriBuilder { Scheme = currentRequest.Scheme, Host = currentRequest.Host.Host, Port = currentRequest.Host.Port.GetValueOrDefault(), Path = currentRequest.Path.Value }; var currentRequestUri = currentRequestUriBuilder.Uri; return currentRequestUri; } } The second change I made was to create a private method within the HomeResource class called CreateCustomerResourceCollectionLink, this method utilizes the UrlBuilder class from JsonApiFramework to build a JSON:API Link.\nNow if I run the project the home resource should expose a link to the customers API resource.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;id\u0026#34;: null, \u0026#34;attributes\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Chinook Sample JSON:API Project\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;customers\u0026#34;: \u0026#34;https://localhost:44323/customers\u0026#34; } } } If you click the link you will get HTTP 404 error as we haven\u0026rsquo;t added any controllers that can handle that HTTP request.\nLet\u0026rsquo;s change that.\nI\u0026rsquo;ll start by adding a new controller, CustomerController. This will handle routing for the customer resource, as well as any relationships exposed to other resources.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 [ApiController] public class CustomerController : ControllerBase { private readonly ILogger\u0026lt;CustomerController\u0026gt; _logger; private readonly ChinookContext _chinookContext; private readonly IHttpContextAccessor _httpContextAccessor; public CustomerController(ILogger\u0026lt;CustomerController\u0026gt; logger, ChinookContext chinookContext, IHttpContextAccessor httpContextAccessor) { _logger = logger; _chinookContext = chinookContext; _httpContextAccessor = httpContextAccessor; } [Route(CustomerResourceKeyWords.Self)] public async Task\u0026lt;IActionResult\u0026gt; GetCustomersResourceCollection() { var customerResourceCollection = await _chinookContext.Customers.ToListAsync(); var currentRequestUri = _httpContextAccessor.HttpContext.GetCurrentRequestUri(); using var chinookDocumentContext = new ChinookDocumentContext(); var document = chinookDocumentContext .NewDocument(currentRequestUri) .SetJsonApiVersion(JsonApiVersion.Version10) .Links() .AddUpLink() .AddSelfLink() .LinksEnd() .ResourceCollection(customerResourceCollection) .Links() .AddSelfLink() .LinksEnd() .ResourceCollectionEnd() .WriteDocument(); return Ok(document); } } Controller has been added.\nDon\u0026rsquo;t forget to register ChinookContext on the built-int dependency injection framework.\n1 2 3 4 5 6 7 8 public void ConfigureServices(IServiceCollection services) { services.AddTransient\u0026lt;HomeResource\u0026gt;(); services.AddDbContext\u0026lt;ChinookContext\u0026gt;(); // DbContext added as a dependency. services.AddHttpContextAccessor(); services.AddControllers() .AddNewtonsoftJson(); } and I know, I can hear you saying \u0026lsquo;Doesn\u0026rsquo;t accessing the DbContext directly on the controller violate the Clean Architecture project structure?\u0026rsquo;. It does, but for right now I am not concerned about that, I don\u0026rsquo;t want to spend too much time on the Data layer at this point. I want to keep it simple by accessing the DbContext directly. In a future blog post, I will return to clean this all up. For now I want to get the resource up and running.\nTime to run the project again. Clicking on the customer link on the home resource gets me the following JSON:API errors documents as the HTTP response.\nThe error did not make any sense at first, but then it clicked. We created a CustomerServiceModelConfiguration class so that JsonApiFramework understands how to work the CustomersServiceModel class but we never registered it on our ConfigurationFactory class. Let\u0026rsquo;s change that.\n1 2 3 4 5 6 7 8 9 10 11 public static IServiceModel CreateServiceModel() { var serviceModelBuilder = new ServiceModelBuilder(); serviceModelBuilder.Configurations.Add(new HomeServiceModelConfiguration()); serviceModelBuilder.Configurations.Add(new CustomerServiceModelConfiguration()); serviceModelBuilder.HomeResource\u0026lt;HomeServiceModel\u0026gt;(); var createConventions = CreateConventions(); var serviceModel = serviceModelBuilder.Create(createConventions); return serviceModel; } Here is the updated CreateServiceModel method in our ConfigurationFactory class. As you can see, the CustomerServiceModelConfiguration class is now registered. When I run the project now I get the following runtime exception,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \u0026#34;errors\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;1312\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;InternalServerError\u0026#34;, \u0026#34;code\u0026#34;: \u0026#34;0380bf13-8d15-4bf5-9367-170af0bb7d95\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;ServiceModelException\u0026#34;, \u0026#34;detail\u0026#34;: \u0026#34;ServiceModel has missing ResourceType [clrType=Customer] metadata. Ensure metadata is configured correctly for the respective domain/schema.\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;pointer\u0026#34;: null }, \u0026#34;links\u0026#34;: { \u0026#34;about\u0026#34;: { \u0026#34;href\u0026#34;: null } }, \u0026#34;meta\u0026#34;: { \u0026#34;targetSite\u0026#34;: \u0026#34;GetResourceType\u0026#34; } } ] } JsonApiFramework is not able to determine which property within the Customers class should be used as the JSON:API identifier. JsonApiFramework offers two solutions to this problem. We can use the built-in conventions offered by JsonApiFramework. This is similar to how EF and EF Core Fluent APIs work, if you have a class named Dog and that class has a public property named Id or DogId, JsonApiFramework understands that this property is the entity identifier so it automatically maps it as the JSON:API identifier on your JSON:API document. In our case, our class is named Customers, plural, and the entity identifier is the public property CustomerId, notice that word Customer in CustomerId is singular. This naming mismatched is due to the EF scaffold tool.\nSo, if I refactor my code by renaming the Customers (plural) class to Customer (singular) then the API returns the following JSON:API Document.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/customers\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:44323\u0026#34; }, \u0026#34;data\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;Luís\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Gonçalves\u0026#34;, \u0026#34;company\u0026#34;: \u0026#34;Embraer - Empresa Brasileira de Aeronáutica S.A.\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;Av. Brigadeiro Faria Lima, 2170\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;São José dos Campos\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;SP\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;Brazil\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12227-000\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;+55 (12) 3923-5555\u0026#34;, \u0026#34;fax\u0026#34;: \u0026#34;+55 (12) 3923-5566\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;luisg@embraer.com.br\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/customers/1\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;Leonie\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Köhler\u0026#34;, \u0026#34;company\u0026#34;: null, \u0026#34;address\u0026#34;: \u0026#34;Theodor-Heuss-Straße 34\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Stuttgart\u0026#34;, \u0026#34;state\u0026#34;: null, \u0026#34;country\u0026#34;: \u0026#34;Germany\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;70174\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;+49 0711 2842222\u0026#34;, \u0026#34;fax\u0026#34;: null, \u0026#34;email\u0026#34;: \u0026#34;leonekohler@surfeu.de\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/customers/2\u0026#34; } } ] } The second approach is to use configurations by configuring our CustomerServiceModelConfiguration class. I\u0026rsquo;ll undo my refactoring, the customers class is back to being plural and I\u0026rsquo;ll update the CustomerServiceModelConfiguration class.\n1 2 3 4 5 6 7 class CustomerServiceModelConfiguration : ResourceTypeBuilder\u0026lt;Customers\u0026gt; { public CustomerServiceModelConfiguration() { ResourceIdentity(nameof(Customers.CustomerId), typeof(long)); } } Simply call the method ResourceIdentity, use the nameof and typeof parameter to let JsonApiFramework know what the entity identifier is for the service model.\nIf I run the Web Api project again. I get the same JSON:API document as an HTTP response.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/customers\u0026#34;, \u0026#34;up\u0026#34;: \u0026#34;https://localhost:44323\u0026#34; }, \u0026#34;data\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;Luís\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Gonçalves\u0026#34;, \u0026#34;company\u0026#34;: \u0026#34;Embraer - Empresa Brasileira de Aeronáutica S.A.\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;Av. Brigadeiro Faria Lima, 2170\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;São José dos Campos\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;SP\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;Brazil\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12227-000\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;+55 (12) 3923-5555\u0026#34;, \u0026#34;fax\u0026#34;: \u0026#34;+55 (12) 3923-5566\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;luisg@embraer.com.br\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/customers/1\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;customers\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;firstName\u0026#34;: \u0026#34;Leonie\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Köhler\u0026#34;, \u0026#34;company\u0026#34;: null, \u0026#34;address\u0026#34;: \u0026#34;Theodor-Heuss-Straße 34\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Stuttgart\u0026#34;, \u0026#34;state\u0026#34;: null, \u0026#34;country\u0026#34;: \u0026#34;Germany\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;70174\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;+49 0711 2842222\u0026#34;, \u0026#34;fax\u0026#34;: null, \u0026#34;email\u0026#34;: \u0026#34;leonekohler@surfeu.de\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323/customers/2\u0026#34; } } ] } The API now successfully exposes Customer as an API resource. By the next blog post, I will add the remaining resources.\n","permalink":"http://localhost:1313/post/2020/json-api-exposing-the-customer-resource/","summary":"\u003cp\u003eThis will be my third blog post on \u003ca href=\"https://jsonapi.org/\"\u003eJSON:API\u003c/a\u003e in .NET Core.\u003c/p\u003e\n\u003cp\u003eI plant to add \u003cstrong\u003eCustomer\u003c/strong\u003e as an API resource, but before we get too deep on the code, I would like to review the \u003ca href=\"https://www.sqlitetutorial.net/sqlite-sample-database/\"\u003eChinook\u003c/a\u003e database project. To do that I\u0026rsquo;m going to import \u003ca href=\"https://cdn.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\"\u003eChinook.db\u003c/a\u003e into \u003ca href=\"https://sqlitebrowser.org/dl/\"\u003eDB Browser\u003c/a\u003e for SQLite to see all available entities.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/post/2020/json-api-exposing-the-customer-resource/chinook-database-entities.PNG\" alt=\"Database Entities\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003eAs you can see we have quite a few entities, for this blog post I will concentrate on the \u003cstrong\u003ecustomers\u003c/strong\u003e entity. To accomplish adding customers as an API resource I will need to create a new service model that represents the customers entity in both JsonApiFramework and EF Core. I will scaffold the SQLite database using \u003ca href=\"https://docs.microsoft.com/en-us/ef/core/managing-schemas/scaffolding?tabs=dotnet-core-cli\"\u003eEF Core\u0026rsquo;s reverse engineering capabilities\u003c/a\u003e.\u003c/p\u003e","title":"JSON:API - Exposing The Customer Resource"},{"content":"On my second post on JSON:API in .NET Core I wanted to create an exception handling middleware. This middleware would be responsible for catching all exceptions and for generating a JSON:API Errors Documents.\nI\u0026rsquo;ll start by adding a middleware folder on the Chinook.Web project, for now it will only have the exception handling middleware, but, eventually it will have additional middleware.\nFolder has been added, now I will add the middleware class to the project in here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class ExceptionHandlingMiddleware { private readonly RequestDelegate _next; private readonly ILogger\u0026lt;ExceptionHandlingMiddleware\u0026gt; _logger; public ExceptionHandlingMiddleware(RequestDelegate next, ILogger\u0026lt;ExceptionHandlingMiddleware\u0026gt; logger) { _next = next; _logger = logger; } public Task Invoke(HttpContext httpContext) { return _next(httpContext); } } The code above is the default middleware class generate by Visual Studio, pretty simple, nothing complex, on another blog post I will come back to this middleware to add more complex error handling, for now it returns an HTTP 500 status code for all errors with a JSON:API Error document as the response.\nOur first modification to the code will be to wrap the code in the Invoke method around a try/catch. Then to create a HandleException method to put all logic that deals with error handling. I need to build the Errors Document and I also need to transform .NET Core Exception objects into a JSON:API Errors Object. Additionally, I want to include inner exceptions on the Errors document. Ben Brandt, has an exception extension class that I use a lot, though I slightly modified it for my use case, still it is useful because we can extract each child exception and log them while they are being added to the Errors document.\nBack in the exception handling middleware class, I\u0026rsquo;ll use the exception extension class to get all exceptions and to transformer them into a JSON:API Error Objects. The code ends up looking like the following code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 public class ExceptionHandlingMiddleware { private readonly RequestDelegate _next; private readonly ILogger\u0026lt;ExceptionHandlingMiddleware\u0026gt; _logger; public ExceptionHandlingMiddleware(RequestDelegate next, ILogger\u0026lt;ExceptionHandlingMiddleware\u0026gt; logger) { _next = next; _logger = logger; } public async Task Invoke(HttpContext httpContext) { try { await _next(httpContext); } catch (Exception exception) { await HandleException(httpContext, exception); } } private async Task\u0026lt;Task\u0026gt; HandleException(HttpContext httpContext, Exception exceptionContext) { // Change http status code to 500. httpContext.Response.StatusCode = (int)HttpStatusCode.InternalServerError; var errorsDocument = new ErrorsDocument(); var exceptionCollection = exceptionContext.GetAllExceptions(); // Get parent exception and all child exceptions foreach (var exception in exceptionCollection) { // For pointers, see https://tools.ietf.org/html/rfc6901 var pointer = JsonConvert.SerializeObject(new { pointer = exception.Source }); var exceptionSource = JObject.Parse(pointer); var eventTitle = exception.GetType().Name; var eventId = RandomNumberGenerator.GetInt32(10000); // For demo purposes only. Your event ids should be tied to specific errors. var @event = new EventId(eventId, eventTitle); var linkDictionary = new Dictionary\u0026lt;string, Link\u0026gt; { { Keywords.About, new Link(exception.HelpLink) // Link to error documentation, this is a hypermedia driven api after all. } }; var targetSite = exception.TargetSite.Name; var meta = new JObject { [\u0026#34;targetSite\u0026#34;] = targetSite }; var errorException = new ErrorException( Error.CreateId(@event.Id.ToString()), HttpStatusCode.InternalServerError, Error.CreateNewId(), @event.Name, exception.Message, exceptionSource, new Links(linkDictionary), meta); errorsDocument.AddError(errorException); } var jsonResponse = await errorsDocument.ToJsonAsync(); return httpContext.Response.WriteAsync(jsonResponse); // Remember to always write to the response body asynchronously. } } We can test it by generating an exception, I can do that by modifying the GetHomeDocument method in the HomeResource class to throw a not implemented exception.\n1 2 3 4 public Task\u0026lt;Document\u0026gt; GetHomeDocument() { throw new NotImplementedException(); } All I need to do now is to register the exception handling middleware in the StartUp.cs class. The Configure method should now look like this.\n1 2 3 4 5 6 7 8 9 10 11 public void Configure(IApplicationBuilder app, IWebHostEnvironment env) { app.UseHttpsRedirection(); app.UseRouting(); app.UseAuthorization(); app.UseExceptionHandlingMiddleware(); // Register ExceptionHandling Middleware app.UseEndpoints(endpoints =\u0026gt; { endpoints.MapControllers(); }); } If I run the Chinook.Web project, I get the following JSON:API Errors Document.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \u0026#34;errors\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;3646\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;InternalServerError\u0026#34;, \u0026#34;code\u0026#34;: \u0026#34;94ee7495-d507-4819-ad8c-4ebaab08c8e4\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;NotImplementedException\u0026#34;, \u0026#34;detail\u0026#34;: \u0026#34;The method or operation is not implemented.\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;pointer\u0026#34;: \u0026#34;Chinook.Web\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;about\u0026#34;: { \u0026#34;href\u0026#34;: null } }, \u0026#34;meta\u0026#34;: { \u0026#34;targetSite\u0026#34;: \u0026#34;GetHomeDocument\u0026#34; } } ] } Now the API has a global exception handler that will transform all errors into JSON:API Errors document.\nThe code here is meant to illustrate how to generate JSON:API errors documents using JsonApiFramework. I would not use this code in a production environment as it is still missing important implementation such as having a proper event id for each error rather than just generating random number. Additionally, as I mentioned before, this doesn\u0026rsquo;t handle specific errors like validation, 404s, invalid query parameters etc. I will cover those later on this series.\n","permalink":"http://localhost:1313/post/2020/json-api-exception-handling-middleware/","summary":"\u003cp\u003eOn my second post on \u003ca href=\"https://jsonapi.org/\"\u003eJSON:API\u003c/a\u003e in .NET Core I wanted to create an exception handling \u003ca href=\"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/middleware/?view=aspnetcore-3.1\"\u003emiddleware\u003c/a\u003e. This middleware would be responsible for catching all exceptions and for generating a JSON:API \u003ca href=\"https://jsonapi.org/format/#document-top-level\"\u003eErrors Documents\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ll start by adding a middleware folder on the Chinook.Web project, for now it will only have the exception handling middleware, but, eventually it will have additional middleware.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/post/2020/json-api-exception-handling-middleware/middleware-folder.PNG\" alt=\"Middleware Folder\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003eFolder has been added, now I will add the middleware class to the project in here.\u003c/p\u003e","title":"JSON:API - Exception Handling Middleware"},{"content":"Are you using SQLite as an in-memory provider for EF Core on your Unit/Integration test? If you are, you may come across the following exception when creating the in-memory database.\nAs you can see from the exception, the error is \u0026ldquo;SQLite Error 1: \u0026rsquo;no such table vPet\u0026rsquo;\u0026rdquo; which is odd because vPet is defined as a SQL view on my DbContext, not a SQL table.\nHere is my PetsDbContext.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public class PetsDbContext : DbContext { // Rest of code omitted for brevity public DbSet\u0026lt;Pet\u0026gt; Pets { get; set; } public PetsDbContext() { } public PetsDbContext(DbContextOptions\u0026lt;PetsDbContext\u0026gt; options) : base(options) { } protected override void OnModelCreating(ModelBuilder modelBuilder) { // Rest of code omitted for brevity modelBuilder.ApplyConfiguration(new PetConfiguration()) } } and my entity model for Pet.\n1 2 3 4 5 public class Pet { public int Id { get; set; } public string Name { get; set; } } and finally, here is my entity type configuration class for Pet.\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class PetModelConfiguration : IEntityTypeConfiguration\u0026lt;Pet\u0026gt; { public void Configure(EntityTypeBuilder\u0026lt;Pet\u0026gt; builder) { builder.HasNoKey(); builder.ToView(\u0026#34;vPet\u0026#34;); builder.Property(p =\u0026gt; p.Id) .IsRequired() .HasMaxLength(50); } } Notice the usage of .ToView() on my entity configuration class, per the EF Core documentation, the method .ToView assumes that the database object vPet has been created outside of the execution EnsuredCreated or EnsureCreatedAsync.\n1 2 3 4 5 6 7 8 9 10 11 12 13 [Fact] public async Task Test_UsingSqliteInMemoryProvider() { var options = new DbContextOptionsBuilder\u0026lt;PetsDbContext\u0026gt;() .UseSqlite(\u0026#34;DataSource=:memory:\u0026#34;) .Options; using (var context = new PetsDbContext(options)) { // This fails to create vPet view. await context.Database.EnsureCreatedAsync(); } } In other words, any entity configuration that utilizes .ToView() on your DbContext will not generate a corresponding SQL view. This is why SQLite is throwing the error \u0026ldquo;SQLite Error 1: \u0026rsquo;no such table vPets\u0026rsquo;\u0026rdquo;. To get around this problem, you can write a script that generates the missing View in SQLite, for example.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Fact] public async Task Test_UsingSqliteInMemoryProvider() { var options = new DbContextOptionsBuilder\u0026lt;PetsDbContext\u0026gt;() .UseSqlite(\u0026#34;DataSource=:memory:\u0026#34;) .Options; using (var context = new PetsDbContext(options)) { // vPet will now be created context.Database.ExecuteSqlRaw(\u0026#34;CREATE VIEW vPets as p.Id, p.[Name] FROM Pet p\u0026#34;); await context.Database.EnsureCreatedAsync(); } } ","permalink":"http://localhost:1313/post/2020/sqlite-no-such-table-error/","summary":"\u003cp\u003eAre you using SQLite as an in-memory provider for EF Core on your Unit/Integration test? If you are, you may come across the following exception when creating the in-memory database.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/post/2020/sqlite-no-such-table-error/pet-exception.PNG\" alt=\"Pet Exception\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003eAs you can see from the exception, the error is \u003cstrong\u003e\u0026ldquo;SQLite Error 1: \u0026rsquo;no such table vPet\u0026rsquo;\u0026rdquo;\u003c/strong\u003e which is odd because vPet is defined as a SQL view on my DbContext, not a SQL table.\u003c/p\u003e\n\u003cp\u003eHere is my PetsDbContext.\u003c/p\u003e","title":"SQLite - No Such Table Error"},{"content":"This post will be my first entry into a multi-part series of post showing how I\u0026rsquo;ve built RESTful APIs using the JSON:API specification on .NET Core.\nI will start by creating a new .NET Core Web Api project, I am going to call this project Chinook, after the sqlite database that I will use for this project. Whenever I create a .NET Core project I like to follow the project structure outlined by Steve Smith in his Clean Architecture repository.\nNow that I have a project structure, I will import a a library that will help me build the API. The official JSON:API website contains a list of server implementations that can be use on .NET Core. Out of that list, JsonApiDotNetCore is by far the most popular and the one with the most community support, but for this project I will utilize JsonApiFramework. The reason for that is that I do not like using decorators on my classes, I like the idea of using a framework that is not directly reference on my domain.\nThe convention/configuration approach taken by JsonApiFramework means that your domain remains clean and independent of any JSON:API implementation details, it also means that the models can be used in other higher level frameworks like EF Core with no interference. JsonApiDotNetCore does plan to support a similar fluent style API, but for now JsonApiFrameWork will do the job.\nOK. Let\u0026rsquo;s get started by installing JsonApiFramework using the following dotnet command.\n1 2 dotnet add package JsonApiFramework.Server --version 2.4.0 Now that I have JsonApiFramework installed, I can add our first API resource, the Home resource. If you are not familiar with the concept of having a home resource, I suggest reading API Discovery Using JSON Home by API Evangelist. Before creating the Home resource, I will add a new folder, Resources, to the Chinook.Web project, in this folder I will add the HomeResource class.\nIt is now time to configure our Home resource using JsonApiFramework. I added a new folder named ServiceModels under Chinook.Core. In this folder I added new class, HomeServiceModel. Additionally, under the ServiceModels I added a Configuration folder, and within this folder I added a HomeConfiguration class. Our Chinook.Core project so far looks like this.\nOur HomeServiceModel class is rather simple, for now it will have a message property, the property will be used to display a \u0026ldquo;Hello World\u0026rdquo; message on our home document.\n1 2 3 4 5 6 7 namespace Chinook.Core.ServiceModels { public class Home { public string Message { get; set; } } } As for our configuration class, I will set the API path to be an empty string as the home resource will not expose relationships to other resources. Then I set the JSON:API type for this resource to be \u0026ldquo;home\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 using JsonApiFramework.ServiceModel.Configuration; namespace Chinook.Core.ServiceModels.Configurations { class HomeConfiguration : ResourceTypeBuilder\u0026lt;Home\u0026gt; { private const string JsonApiType = \u0026#34;home\u0026#34;; public HomeConfiguration() { Hypermedia() .SetApiCollectionPathSegment(string.Empty); ResourceIdentity() .SetApiType(JsonApiType); } } } Now that I have created the home configuration class I will need to configure JsonApiFramework to use it. I will start by adding a ConfigurationFactory class under the Chinook.Core project. In this class I tell JsonApiFramework how I want our API resources to be configured. For example, I want to the JSON:API attributes to use Camel Casing, I also want JsonApiFramework to pluralize the API types, and I also want JsonApiFramework to auto discovery properties in my models and to map them, if my model exposes an Id property, then I want JsonApiFramework to automatically map that to the JSON:API resource\u0026rsquo;s Id. These type of configurations can be achieved like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 using JsonApiFramework.Conventions; using JsonApiFramework.ServiceModel; using JsonApiFramework.ServiceModel.Configuration; namespace Chinook.Core.ServiceModels { public static class ConfigurationFactory { public static IConventions CreateConventions() { var conventionsBuilder = new ConventionsBuilder(); conventionsBuilder.ApiAttributeNamingConventions() .AddStandardMemberNamingConvention(); conventionsBuilder.ApiTypeNamingConventions() .AddPluralNamingConvention() .AddStandardMemberNamingConvention(); conventionsBuilder.ResourceTypeConventions() .AddPropertyDiscoveryConvention(); var conventions = conventionsBuilder.Create(); return conventions; } } } Something to note, the method AddStandardMemberNamingConvention() just refers to the fact that JSON:API recommends using camel casing. JsonApiFramework does support Pascal, Kebab or your own custom casing, though I have never explored that option.\nIt is now time to update our Home resource class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 namespace Chinook.Web.Resources { public class HomeResource { private readonly IHttpContextAccessor httpContextAccessor; private readonly ILogger\u0026lt;HomeResource\u0026gt; logger; public HomeResource(IHttpContextAccessor httpContextAccessor, ILogger\u0026lt;HomeResource\u0026gt; logger) { this.httpContextAccessor = httpContextAccessor; this.logger = logger; } public Task\u0026lt;Document\u0026gt; GetHomeDocument() { var homeResource = new HomeServiceModel { Message = \u0026#34;Hello World\u0026#34; }; var displayUri = httpContextAccessor.HttpContext.Request.GetDisplayUrl(); var currentRequestUri = new Uri(displayUri); using var chinookDocumentContext = new ChinookDocumentContext(currentRequestUri); var document = chinookDocumentContext .NewDocument(currentRequestUri) .SetJsonApiVersion(JsonApiVersion.Version10) .Links() .AddSelfLink() .LinksEnd() .Resource(homeResource) .ResourceEnd() .WriteDocument(); return Task.FromResult(document); } } } Now, the last item we need is the DocumentContext, this class is required by JsonApiFramework whenever we work with a JSON:API document.\nHere is the complete class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 namespace Chinook.Core { public class ChinookDocumentContext : DocumentContext { public ChinookDocumentContext(Uri currentRequestUri) { var urlBuilderConfiguration = CreateUrlBuilderConfiguration(currentRequestUri); UrlBuilderConfiguration = urlBuilderConfiguration; } public ChinookDocumentContext(Uri currentRequestUri, Document document) : base(document) { var urlBuilderConfiguration = CreateUrlBuilderConfiguration(currentRequestUri); UrlBuilderConfiguration = urlBuilderConfiguration; } protected override void OnConfiguring(IDocumentContextOptionsBuilder optionsBuilder) { var serviceModel = ConfigurationFactory.CreateServiceModel(); optionsBuilder.UseServiceModel(serviceModel); optionsBuilder.UseUrlBuilderConfiguration(UrlBuilderConfiguration); } private IUrlBuilderConfiguration UrlBuilderConfiguration { get; set; } private static UrlBuilderConfiguration CreateUrlBuilderConfiguration(Uri currentRequestUri) { var scheme = currentRequestUri.Scheme; var host = currentRequestUri.Host; var port = currentRequestUri.Port; var urlBuilderConfiguration = new UrlBuilderConfiguration(scheme, host, port); return urlBuilderConfiguration; } } } I now have all the required dependencies, I can run the web api project.\nThis is response I get.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;id\u0026#34;: null, \u0026#34;attributes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;message\u0026#34; } ], \u0026#34;relationships\u0026#34;: null, \u0026#34;links\u0026#34;: null, \u0026#34;meta\u0026#34;: null }, \u0026#34;included\u0026#34;: null, \u0026#34;jsonApiVersion\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;meta\u0026#34;: null }, \u0026#34;meta\u0026#34;: null, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: { \u0026#34;hRef\u0026#34;: \u0026#34;https://localhost:44323\u0026#34;, \u0026#34;meta\u0026#34;: null, \u0026#34;pathSegments\u0026#34;: [ ], \u0026#34;uri\u0026#34;: \u0026#34;https://localhost:44323\u0026#34; } } } I got a 200 OK HTTP response, but wait, something doesn\u0026rsquo;t look right. The serialization does not look correct, I know what the problem is and how it can be fixed. JsonApiFramework has a hard decency on JSON.NET and .NET Core 3 and above use System.Text.Json. We can correct our problem by installing the following NuGet package.\n1 dotnet add package Microsoft.AspNetCore.Mvc.NewtonsoftJson I need to update our StartUp.cs class so that NewtonsoftJson is used to serialize/deserialize JSON.\n1 2 3 4 5 6 7 8 9 10 11 12 13 namespace Chinook { public class Startup { public void ConfigureServices(IServiceCollection services) { //Omitted for brevity services.AddControllers() .AddNewtonsoftJson(); } } } All set, if I run the project again I should get home document with the correct json serialization settings.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;jsonapi\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;links\u0026#34;: { \u0026#34;self\u0026#34;: \u0026#34;https://localhost:44323\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;id\u0026#34;: null, \u0026#34;attributes\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Chinook Sample JSON:API Project\u0026#34; } } } Perfect, our home resource is now working as expected.\n","permalink":"http://localhost:1313/post/2020/json-api-creating-the-home-resource/","summary":"\u003cp\u003eThis post will be my first entry into a multi-part series of post showing how I\u0026rsquo;ve built RESTful APIs using the \u003ca href=\"https://jsonapi.org/\"\u003eJSON:API\u003c/a\u003e specification on .NET Core.\u003c/p\u003e\n\u003cp\u003eI will start by creating a new .NET Core Web Api project, I am going to call this project \u003cstrong\u003eChinook\u003c/strong\u003e, after the sqlite \u003ca href=\"https://www.sqlitetutorial.net/sqlite-sample-database/\"\u003edatabase\u003c/a\u003e that I will use for this project. Whenever I create a .NET Core project I like to follow the project structure outlined by Steve Smith in his \u003ca href=\"https://github.com/ardalis/CleanArchitecture\"\u003eClean Architecture\u003c/a\u003e repository.\u003c/p\u003e","title":"JSON:API - Creating The Home Resource"},{"content":"In my second post, I wanted to cover AcquireRequestState. In my four years as a developer I have encountered issues with AcquireRequestState twice. So, what in the world is AcquireRequestState.\nAcquireRequestState is part of the ASP.NET Life Cycle, this is an event raised by the HttpApplication, it keeps session state in sync. Though I suspect that most developers are familiar with this event for being a major performance pain in their .NET Framework application, as documented here, here, here, here and here.\nAs seen on the various link above, the problem is always the same. Someone notices HTTP request spending a large amount of time in AcquireRequestState. For example,\nthis transaction took ~95 seconds to complete, out of 95, 94 were spent inside AcquireRequestState. That is a ridiculous amount of time spend in code that you didn\u0026rsquo;t write. So what is the problem? Just what in the world is going on inside the AcquireRequestState.\nThe problem is that AcquireRequestState reads session variables in a queue rather than in parallel, this is done to maintain thread safety and to prevent one session from overwriting the work done in another thread. I recommend reading this awesome post by Jono to understand more.\nSolutions?\nAside from the obvious ones, like turning off session or setting session to read-only, which by the way, in my experience does not make much of a difference.\nWhen I first encountered AcquireRequestState, turning off session was not an option. The project I was working on at the time required session to be on, in fact without session the payment system would not work at all, ouch. So we had to look for alternative solution. We noticed red-gate had a post on writing your own custom Session provider, which made us wonder if anyone had created one. Turns out someone did. Here is the project, it is a custom Session provider with Redis and boy it turned out to be great project.\nWe immediately modified our .NET Framework project to utilize RedisSessionProvider and the benefits were noticed right away. I believe, we dropped our response time by about a second across all pages while lowering .NET CLR from about 200ms to about ~58ms.\nDuring my second encounter with AcquireRequestState, I did not implement a customer Session provider. It was not needed, the application I was working on was only storing some basic information to session, and those values were available through some other means. So we turned off Session completely, thus eliminating AcquireRequestState. Once again, I saw response time improvement across the entire application, our .NET CLR went from about 250ms to about ~30ms.\n","permalink":"http://localhost:1313/post/2020/the-problem-with-acquirerequeststate/","summary":"\u003cp\u003eIn my second post, I wanted to cover AcquireRequestState. In my four years as a developer I have encountered issues with AcquireRequestState twice. So, what in the world is AcquireRequestState.\u003c/p\u003e\n\u003cp\u003eAcquireRequestState is part of the ASP.NET \u003ca href=\"https://docs.microsoft.com/en-us/previous-versions/aspnet/bb470252(v=vs.100)\"\u003eLife Cycle\u003c/a\u003e, this is an event raised by the \u003ca href=\"https://docs.microsoft.com/en-us/dotnet/api/system.web.httpapplication?redirectedfrom=MSDN\u0026view=netframework-4.8\"\u003eHttpApplication\u003c/a\u003e, it keeps session state in sync. Though I suspect that most developers are familiar with this event for being a major performance pain in their .NET Framework application, as documented \u003ca href=\"https://stackoverflow.com/questions/30066925/long-delays-in-acquirerequeststate\"\u003ehere\u003c/a\u003e, \u003ca href=\"https://discuss.newrelic.com/t/acquirerequeststate-is-delaying-response-times-web-api/38229\"\u003ehere\u003c/a\u003e, \u003ca href=\"https://stackoverflow.com/questions/3629709/i-just-discovered-why-all-asp-net-websites-are-slow-and-i-am-trying-to-work-out\"\u003ehere\u003c/a\u003e, \u003ca href=\"https://stackoverflow.com/questions/8349033/storing-anything-in-asp-net-session-causes-500ms-delays\"\u003ehere\u003c/a\u003e and \u003ca href=\"https://stackoverflow.com/questions/35133150/newrelic-async-http-handler-and-acquirerequeststate\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"The Problem With AcquireRequestState"},{"content":"Serilog has a neat feature that allows you to configure sub-loggers. With this feature you can essentially have log specific instances running on your application.\nI recently had to configure a .NET Framework application to use two different sub-loggers and while I was able find many examples online on how to configure sub-loggers through AppSettings.json, I did not find any examples on how to configure them through AppSettings.config/App.config so I wanted to document that process on this post.\nFirst, we need to grab a few NuGet packages.\nWe need Serilog.\n1 dotnet add package Serilog We have to read application settings from AppSettings.config/App.config, for that, we use the following Serilog package.\n1 dotnet add package Serilog.Settings.AppSettings Our sample application will be configured to log to files, so we need to grab file sink package.\n1 dotnet add package Serilog.Sinks.File and the last NuGet package.\n1 dotnet add package Serilog.Filters.Expressions This NuGet package will allow us to write logs to different sub-loggers based on an expression defined on our configuration. Our log will also be enriched logs with these expressions.\nNow that we have all the required NuGet package we can move on to adding our log configuration. For our sample application, we are going to configure two sub-loggers, sub-logger red and sub-logger blue.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026lt;appSettings\u0026gt; \u0026lt;!-- Configure the red logger --\u0026gt; \u0026lt;add key=\u0026#34;red:serilog:minimum-level\u0026#34; value=\u0026#34;Verbose\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;red:serilog:using:File\u0026#34; value=\u0026#34;Serilog.Sinks.File\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;red:serilog:using:FilterExpressions\u0026#34; value=\u0026#34;Serilog.Filters.Expressions\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;red:serilog:write-to:File.path\u0026#34; value=\u0026#34;C:\\Log\\red-.log\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;red:serilog:write-to:File.rollingInterval\u0026#34; value=\u0026#34;Day\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;red:serilog:write-to:File.formatter\u0026#34; value=\u0026#34;Serilog.Formatting.Compact CompactJsonFormatter, Serilog.Formatting.Compact\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;red:serilog:write-to:File.fileSizeLimitBytes\u0026#34; value=\u0026#34;100\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;red:serilog:write-to:File.retainedFileCountLimit\u0026#34; value=\u0026#34;10\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;red:serilog:write-to:File.restrictToMinimumLevel\u0026#34; value=\u0026#34;Verbose\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;red:serilog:filter:ByIncludingOnly.expression\u0026#34; value=\u0026#34;isRed = true\u0026#34; /\u0026gt; \u0026lt;!-- Configure the blue logger --\u0026gt; \u0026lt;add key=\u0026#34;blue:serilog:minimum-level\u0026#34; value=\u0026#34;Verbose\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;blue:serilog:using:File\u0026#34; value=\u0026#34;Serilog.Sinks.File\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;blue:serilog:using:FilterExpressions\u0026#34; value=\u0026#34;Serilog.Filters.Expressions\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;blue:serilog:write-to:File.path\u0026#34; value=\u0026#34;C:\\Log\\blue-.log\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;blue:serilog:write-to:File.formatter\u0026#34; value=\u0026#34;Serilog.Formatting.Compact.CompactJsonFormatter, Serilog.Formatting.Compact\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;blue:serilog:write-to:File.rollingInterval\u0026#34; value=\u0026#34;Day\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;blue:serilog:write-to:File.fileSizeLimitBytes\u0026#34; value=\u0026#34;100\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;blue:serilog:write-to:File.retainedFileCountLimit\u0026#34; value=\u0026#34;10\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;blue:serilog:write-to:File.restrictToMinimumLevel\u0026#34; value=\u0026#34;Verbose\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;blue:serilog:filter:ByIcludingOnly.expression\u0026#34; value=\u0026#34;isBlue = true\u0026#34; /\u0026gt; \u0026lt;/appSettings\u0026gt; As you can see, the trick here is to put the name of the sub-logger in front of :serilog. Next, we need to have Serilog read the configuration we defined above.\n1 2 3 4 Log.Logger = new LoggerConfiguration() .WriteTo.Logger(ReadRedLoggerConfigurations) .WriteTo.Logger(ReadBlueLoggerConfigurations) .CreateLogger(); ReadRedLoggerConfigurations and ReadBlueLoggerConfigurations are two static methods used to configure each sub-logger.\n1 2 3 4 5 6 7 8 9 10 11 private static void ReadRedLoggerConfigurations(LoggerConfiguration loggerConfiguration) { const string redLogger = \u0026#34;red\u0026#34;; loggerConfiguration.ReadFrom.AppSettings(redLogger); } private static void ReadBlueLoggerConfigurations(LoggerConfiguration loggerConfiguration) { const string blueLogger = \u0026#34;blue\u0026#34;; loggerConfiguration.ReadFrom.AppSettings(blueLogger); } Here is the completed console application.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 namespace SubLogger { class Program { static void Main(string[] args) { try { Log.Logger = new LoggerConfiguration() .WriteTo.Logger(ReadRedLoggerConfigurations) .WriteTo.Logger(ReadBlueLoggerConfigurations) .CreateLogger(); //Log to sub-loggers var log = Log.Logger; log.ForContext(new PropertyEnricher(\u0026#34;isBlue\u0026#34;, true)).Information(\u0026#34;Logging to blue sub-logger.\u0026#34;); log.ForContext(new PropertyEnricher(\u0026#34;isRed\u0026#34;, true)).Information(\u0026#34;Logging to red sub-logger.\u0026#34;); } catch (Exception exception) { Log.Fatal(exception, \u0026#34;Build Error.\u0026#34;); } finally { Log.CloseAndFlush(); } } private static void ReadRedLoggerConfigurations(LoggerConfiguration loggerConfiguration) { const string redLogger = \u0026#34;red\u0026#34;; loggerConfiguration.ReadFrom.AppSettings(redLogger); } private static void ReadBlueLoggerConfigurations(LoggerConfiguration loggerConfiguration) { const string blueLogger = \u0026#34;blue\u0026#34;; loggerConfiguration.ReadFrom.AppSettings(blueLogger); } } } Now we can test our console application to confirm that the correct configurations are being applied to each sub -logger. If you run the application, then two log files should be generated on C:\\Log\nand if we open the red log file you should see the following log lines.\n1 2 3 4 5 { \u0026#34;@t\u0026#34;:\u0026#34;2020-09-01T00:52:06.1676551Z\u0026#34;, \u0026#34;@mt\u0026#34;:\u0026#34;Logging to red sub-logger.\u0026#34;, \u0026#34;isRed\u0026#34;:true } Congratulations, you have successfully configured two different Serilog sub-loggers through XML configuration.\n","permalink":"http://localhost:1313/post/2020/configure-serilog-sub-logger-from-appsettings/","summary":"\u003cp\u003eSerilog has a neat feature that allows you to configure sub-loggers. With this feature you can essentially have log specific instances running on your application.\u003c/p\u003e\n\u003cp\u003eI recently had to configure a .NET Framework application to use two different sub-loggers and while I was able find many examples online on how to configure sub-loggers through AppSettings.json, I did not find any examples on how to configure them through AppSettings.config/App.config so I wanted to document that process on this post.\u003c/p\u003e","title":"Configure Serilog Sub-Loggers Using XML App Settings"},{"content":"In no particular order, here are my recommended list of books.\nTerraform: Up and Running: Writing Infrastructure as Code Designing Data-Intensive Applications Production Ready GraphQL Designing Distributed Systems The DynamoDb Book RESTful Web Services Cookbook The Imposter\u0026rsquo;s Handbook Design Patterns for Cloud Native Applications Understanding Distributed Systems Practical Process Automation Database Internals: A Deep Dive into How Distributed Data Systems Work Modern Trade-Off Analyses for Distributed Architectures Managing Cloud Native Data on Kubernetes Kubernetes Patterns: Reusable Elements for Designing Cloud Native Applications Team Topologies: Organizing Business and Technology Teams for Fast Flow Foundations of Scalable Systems: Designing Distributed Architectures Building Event-Driven Microservices: Leveraging Organizational Data at Scale Fundamentals of Software Architecture: An Engineering Approach System Design Interview – An insider\u0026rsquo;s guide Making Sense of Stream Processing Patterns of Distributed Systems Concurrency in Go Fluent Python Hypermedia Systems Learning Go ","permalink":"http://localhost:1313/reading-list/","summary":"\u003cp\u003eIn no particular order, here are my recommended list of books.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/j0150iK\"\u003eTerraform: Up and Running: Writing Infrastructure as Code\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/4VLsmog\"\u003eDesigning Data-Intensive Applications\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://book.productionreadygraphql.com/s\"\u003eProduction Ready GraphQL\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/4omRFjY\"\u003eDesigning Distributed Systems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.dynamodbbook.com/\"\u003eThe DynamoDb Book\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/e6vjoVL\"\u003eRESTful Web Services Cookbook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.goodreads.com/book/show/31572054-the-imposter-s-handbook\"\u003eThe Imposter\u0026rsquo;s Handbook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/3iMA9s0\"\u003eDesign Patterns for Cloud Native Applications\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/29Q6ge6\"\u003eUnderstanding Distributed Systems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/1fWJ1hk\"\u003ePractical Process Automation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/gurauZX\"\u003eDatabase Internals: A Deep Dive into How Distributed Data Systems Work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/9xzwkX6\"\u003eModern Trade-Off Analyses for Distributed Architectures\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/ePSzo48\"\u003eManaging Cloud Native Data on Kubernetes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/iLZq4qf\"\u003eKubernetes Patterns: Reusable Elements for Designing Cloud Native Applications\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/1px40zH\"\u003eTeam Topologies: Organizing Business and Technology Teams for Fast Flow\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/hQgiHJe\"\u003eFoundations of Scalable Systems: Designing Distributed Architectures\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/e0F5ZmK\"\u003eBuilding Event-Driven Microservices: Leveraging Organizational Data at Scale\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/8rtIzw3\"\u003eFundamentals of Software Architecture: An Engineering Approach\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/81sn78R\"\u003eSystem Design Interview – An insider\u0026rsquo;s guide\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.oreilly.com/library/view/making-sense-of/9781492042563/\"\u003eMaking Sense of Stream Processing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/foXKroJ\"\u003ePatterns of Distributed Systems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/fFgUZ9C\"\u003eConcurrency in Go\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/d5EOxbE\"\u003eFluent Python\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/goM6rQa\"\u003eHypermedia Systems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://a.co/d/ix1U7xv\"\u003eLearning Go\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e","title":"My reading list."}]